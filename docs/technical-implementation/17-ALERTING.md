# 17. å‘Šè­¦ç³»çµ±è¨­è¨ˆ (Alerting System Design)

## æ–‡æª”è³‡è¨Š

| é …ç›® | å…§å®¹ |
|------|------|
| **æ–‡æª”ç‰ˆæœ¬** | 1.0.0 |
| **å‰µå»ºæ—¥æœŸ** | 2025-01-15 |
| **æœ€å¾Œæ›´æ–°** | 2025-01-15 |
| **ç‹€æ…‹** | Draft |
| **ä½œè€…** | AI Workflow Platform Team |
| **é—œè¯æ–‡æª”** | 15-MONITORING.md, 16-LOGGING.md |

---

## ç›®éŒ„

- [17.1 å‘Šè­¦æ¶æ§‹ç¸½è¦½](#171-å‘Šè­¦æ¶æ§‹ç¸½è¦½)
- [17.2 Alertmanager é…ç½®](#172-alertmanager-é…ç½®)
- [17.3 å‘Šè­¦è¦å‰‡å®šç¾©](#173-å‘Šè­¦è¦å‰‡å®šç¾©)
- [17.4 å‘Šè­¦è·¯ç”±ç­–ç•¥](#174-å‘Šè­¦è·¯ç”±ç­–ç•¥)
- [17.5 é€šçŸ¥æ¸ é“é…ç½®](#175-é€šçŸ¥æ¸ é“é…ç½®)
- [17.6 å‘Šè­¦éœé»˜å’ŒæŠ‘åˆ¶](#176-å‘Šè­¦éœé»˜å’ŒæŠ‘åˆ¶)
- [17.7 å‘Šè­¦æœ€ä½³å¯¦è¸](#177-å‘Šè­¦æœ€ä½³å¯¦è¸)

---

## 17.1 å‘Šè­¦æ¶æ§‹ç¸½è¦½

### 17.1.1 å‘Šè­¦æµç¨‹

```mermaid
graph LR
    Prometheus[Prometheus<br/>Metrics Collection] --> AlertRules[Alert Rules<br/>Evaluation]
    AlertRules --> Alertmanager[Alertmanager<br/>Alert Management]
    Alertmanager --> Grouping[Grouping]
    Alertmanager --> Inhibition[Inhibition]
    Alertmanager --> Silencing[Silencing]
    Grouping --> Routing[Routing]
    Inhibition --> Routing
    Silencing --> Routing
    Routing --> Email[Email]
    Routing --> Slack[Slack]
    Routing --> PagerDuty[PagerDuty]
    Routing --> Webhook[Webhook]
```

### 17.1.2 å‘Šè­¦ç´šåˆ¥

| ç´šåˆ¥ | åš´é‡ç¨‹åº¦ | éŸ¿æ‡‰æ™‚é–“ | é€šçŸ¥æ–¹å¼ | ç¤ºä¾‹ |
|------|----------|---------|---------|------|
| **Critical** | åš´é‡ | ç«‹å³ (5 åˆ†é˜å…§) | PagerDuty + Slack + Email | æœå‹™å®Œå…¨å®•æ©Ÿ |
| **High** | é«˜ | 15 åˆ†é˜å…§ | Slack + Email | éŒ¯èª¤ç‡ > 5% |
| **Warning** | è­¦å‘Š | 1 å°æ™‚å…§ | Email | CPU ä½¿ç”¨ç‡ > 80% |
| **Info** | ä¿¡æ¯ | ä¸éœ€è¦ç«‹å³éŸ¿æ‡‰ | Email (åŒ¯ç¸½) | éƒ¨ç½²å®Œæˆé€šçŸ¥ |

### 17.1.3 å‘Šè­¦åŸå‰‡

**âœ… å¥½çš„å‘Šè­¦:**
- âœ… å¯æ“ä½œæ€§: éœ€è¦äººå·¥å¹²é 
- âœ… æ˜ç¢ºæ€§: æ¸…æ¥šèªªæ˜å•é¡Œ
- âœ… åŠæ™‚æ€§: åœ¨å•é¡Œå½±éŸ¿ç”¨æˆ¶å‰è§¸ç™¼
- âœ… æº–ç¢ºæ€§: èª¤å ±ç‡ä½

**âŒ å£çš„å‘Šè­¦:**
- âŒ å™ªéŸ³: é »ç¹è§¸ç™¼ä½†ä¸éœ€è¦è™•ç†
- âŒ æ¨¡ç³Š: ç„¡æ³•ç¢ºå®šå•é¡Œä¾†æº
- âŒ æ»¯å¾Œ: ç”¨æˆ¶å·²ç¶“å—åˆ°å½±éŸ¿
- âŒ èª¤å ±: å¯¦éš›æ²’æœ‰å•é¡Œ

---

## 17.2 Alertmanager é…ç½®

### 17.2.1 éƒ¨ç½² Alertmanager

**k8s/monitoring/alertmanager/alertmanager.yaml:**

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: alertmanager-config
  namespace: monitoring
data:
  alertmanager.yml: |
    global:
      resolve_timeout: 5m
      slack_api_url: 'https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK'
      pagerduty_url: 'https://events.pagerduty.com/v2/enqueue'

    # æ¨¡æ¿è·¯å¾‘
    templates:
    - '/etc/alertmanager/templates/*.tmpl'

    # è·¯ç”±é…ç½®
    route:
      group_by: ['alertname', 'cluster', 'service']
      group_wait: 10s
      group_interval: 10s
      repeat_interval: 12h
      receiver: 'default'

      routes:
      # Critical å‘Šè­¦ -> PagerDuty + Slack
      - match:
          severity: critical
        receiver: 'pagerduty-critical'
        continue: true

      - match:
          severity: critical
        receiver: 'slack-critical'

      # High å‘Šè­¦ -> Slack + Email
      - match:
          severity: high
        receiver: 'slack-high'
        continue: true

      - match:
          severity: high
        receiver: 'email-ops'

      # Warning å‘Šè­¦ -> Email
      - match:
          severity: warning
        receiver: 'email-ops'

      # æ•¸æ“šåº«å‘Šè­¦ -> DBA åœ˜éšŠ
      - match:
          service: postgresql
        receiver: 'slack-dba'

    # æ¥æ”¶å™¨é…ç½®
    receivers:
    # é»˜èªæ¥æ”¶å™¨
    - name: 'default'
      email_configs:
      - to: 'ops@aiworkflow.com'
        from: 'alertmanager@aiworkflow.com'
        smarthost: 'smtp.gmail.com:587'
        auth_username: 'alertmanager@aiworkflow.com'
        auth_password: 'password'

    # PagerDuty - Critical
    - name: 'pagerduty-critical'
      pagerduty_configs:
      - service_key: 'YOUR_PAGERDUTY_SERVICE_KEY'
        severity: 'critical'
        description: '{{ .CommonAnnotations.summary }}'
        details:
          firing: '{{ .Alerts.Firing | len }}'
          resolved: '{{ .Alerts.Resolved | len }}'
          description: '{{ .CommonAnnotations.description }}'

    # Slack - Critical
    - name: 'slack-critical'
      slack_configs:
      - channel: '#alerts-critical'
        title: 'ğŸš¨ Critical Alert: {{ .CommonLabels.alertname }}'
        text: |
          *Summary:* {{ .CommonAnnotations.summary }}
          *Description:* {{ .CommonAnnotations.description }}
          *Severity:* {{ .CommonLabels.severity }}
          *Service:* {{ .CommonLabels.service }}
          *Firing Alerts:* {{ .Alerts.Firing | len }}
        send_resolved: true

    # Slack - High
    - name: 'slack-high'
      slack_configs:
      - channel: '#alerts-high'
        title: 'âš ï¸ High Alert: {{ .CommonLabels.alertname }}'
        text: '{{ .CommonAnnotations.summary }}'
        send_resolved: true

    # Slack - DBA
    - name: 'slack-dba'
      slack_configs:
      - channel: '#alerts-dba'
        title: 'ğŸ—„ï¸ Database Alert: {{ .CommonLabels.alertname }}'
        text: '{{ .CommonAnnotations.summary }}'

    # Email - Ops Team
    - name: 'email-ops'
      email_configs:
      - to: 'ops@aiworkflow.com'
        from: 'alertmanager@aiworkflow.com'
        smarthost: 'smtp.gmail.com:587'
        auth_username: 'alertmanager@aiworkflow.com'
        auth_password: 'password'
        headers:
          Subject: '[{{ .Status }}] {{ .CommonLabels.alertname }}'
        html: |
          <h2>{{ .CommonLabels.alertname }}</h2>
          <p><strong>Summary:</strong> {{ .CommonAnnotations.summary }}</p>
          <p><strong>Description:</strong> {{ .CommonAnnotations.description }}</p>
          <p><strong>Severity:</strong> {{ .CommonLabels.severity }}</p>
          <p><strong>Service:</strong> {{ .CommonLabels.service }}</p>
          <p><strong>Firing Alerts:</strong> {{ .Alerts.Firing | len }}</p>

    # æŠ‘åˆ¶è¦å‰‡
    inhibit_rules:
    # å¦‚æœé›†ç¾¤å®•æ©Ÿï¼ŒæŠ‘åˆ¶æ‰€æœ‰å…¶ä»–å‘Šè­¦
    - source_match:
        severity: 'critical'
        alertname: 'ClusterDown'
      target_match_re:
        severity: 'warning|high'
      equal: ['cluster']

    # å¦‚æœç¯€é»å®•æ©Ÿï¼ŒæŠ‘åˆ¶è©²ç¯€é»ä¸Šæ‰€æœ‰ Pod å‘Šè­¦
    - source_match:
        severity: 'critical'
        alertname: 'NodeDown'
      target_match:
        severity: 'warning'
      equal: ['instance']

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: alertmanager
  namespace: monitoring
spec:
  replicas: 1
  selector:
    matchLabels:
      app: alertmanager
  template:
    metadata:
      labels:
        app: alertmanager
    spec:
      containers:
      - name: alertmanager
        image: prom/alertmanager:latest
        args:
        - '--config.file=/etc/alertmanager/alertmanager.yml'
        - '--storage.path=/alertmanager'
        ports:
        - containerPort: 9093
        volumeMounts:
        - name: config
          mountPath: /etc/alertmanager
        - name: storage
          mountPath: /alertmanager
        - name: templates
          mountPath: /etc/alertmanager/templates
        resources:
          requests:
            cpu: 250m
            memory: 512Mi
          limits:
            cpu: 1000m
            memory: 2Gi
      volumes:
      - name: config
        configMap:
          name: alertmanager-config
      - name: storage
        emptyDir: {}
      - name: templates
        configMap:
          name: alertmanager-templates

---
apiVersion: v1
kind: Service
metadata:
  name: alertmanager
  namespace: monitoring
spec:
  type: ClusterIP
  ports:
  - port: 9093
    targetPort: 9093
  selector:
    app: alertmanager
```

### 17.2.2 å‘Šè­¦æ¨¡æ¿

**alertmanager-templates.yaml:**

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: alertmanager-templates
  namespace: monitoring
data:
  slack.tmpl: |
    {{ define "slack.default.title" }}
    [{{ .Status | toUpper }}{{ if eq .Status "firing" }}:{{ .Alerts.Firing | len }}{{ end }}] {{ .CommonLabels.alertname }}
    {{ end }}

    {{ define "slack.default.text" }}
    {{ range .Alerts }}
    *Alert:* {{ .Labels.alertname }} - `{{ .Labels.severity }}`
    *Description:* {{ .Annotations.description }}
    *Details:*
      {{ range .Labels.SortedPairs }} â€¢ *{{ .Name }}:* `{{ .Value }}`
      {{ end }}
    {{ end }}
    {{ end }}

  email.tmpl: |
    {{ define "email.default.subject" }}
    [{{ .Status | toUpper }}] {{ .CommonLabels.alertname }}
    {{ end }}

    {{ define "email.default.html" }}
    <!DOCTYPE html>
    <html>
    <head>
      <style>
        body { font-family: Arial, sans-serif; }
        .alert { padding: 20px; margin: 10px 0; border-radius: 5px; }
        .critical { background-color: #f8d7da; color: #721c24; }
        .high { background-color: #fff3cd; color: #856404; }
        .warning { background-color: #d1ecf1; color: #0c5460; }
      </style>
    </head>
    <body>
      <h2>{{ .CommonLabels.alertname }}</h2>
      <div class="alert {{ .CommonLabels.severity }}">
        <p><strong>Summary:</strong> {{ .CommonAnnotations.summary }}</p>
        <p><strong>Description:</strong> {{ .CommonAnnotations.description }}</p>
        <p><strong>Severity:</strong> {{ .CommonLabels.severity }}</p>
        <p><strong>Service:</strong> {{ .CommonLabels.service }}</p>
        <p><strong>Firing Alerts:</strong> {{ .Alerts.Firing | len }}</p>
      </div>

      <h3>Alert Details</h3>
      <table border="1" cellpadding="5" cellspacing="0">
        <tr>
          <th>Alert Name</th>
          <th>Status</th>
          <th>Started At</th>
          <th>Labels</th>
        </tr>
        {{ range .Alerts }}
        <tr>
          <td>{{ .Labels.alertname }}</td>
          <td>{{ .Status }}</td>
          <td>{{ .StartsAt }}</td>
          <td>{{ range .Labels.SortedPairs }}{{ .Name }}={{ .Value }}<br/>{{ end }}</td>
        </tr>
        {{ end }}
      </table>
    </body>
    </html>
    {{ end }}
```

---

## 17.3 å‘Šè­¦è¦å‰‡å®šç¾©

### 17.3.1 Prometheus å‘Šè­¦è¦å‰‡

**k8s/monitoring/prometheus/rules/application-rules.yaml:**

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-application-rules
  namespace: monitoring
data:
  application.rules.yml: |
    groups:
    - name: application_alerts
      interval: 30s
      rules:

      # æœå‹™å¯ç”¨æ€§å‘Šè­¦
      - alert: ServiceDown
        expr: up{job=~".*-service"} == 0
        for: 1m
        labels:
          severity: critical
          service: '{{ $labels.job }}'
        annotations:
          summary: "Service {{ $labels.job }} is down"
          description: "{{ $labels.job }} has been down for more than 1 minute. Instance: {{ $labels.instance }}"
          runbook_url: "https://wiki.aiworkflow.com/runbooks/service-down"

      # é«˜éŒ¯èª¤ç‡å‘Šè­¦
      - alert: HighErrorRate
        expr: |
          (
            sum(rate(http_requests_total{status=~"5.."}[5m])) by (job)
            /
            sum(rate(http_requests_total[5m])) by (job)
          ) > 0.05
        for: 5m
        labels:
          severity: high
          service: '{{ $labels.job }}'
        annotations:
          summary: "High error rate on {{ $labels.job }}"
          description: "{{ $labels.job }} has error rate {{ $value | humanizePercentage }} (threshold: 5%)"
          runbook_url: "https://wiki.aiworkflow.com/runbooks/high-error-rate"

      # é«˜å»¶é²å‘Šè­¦
      - alert: HighLatency
        expr: |
          histogram_quantile(0.95,
            sum(rate(http_request_duration_seconds_bucket[5m])) by (job, le)
          ) > 1
        for: 10m
        labels:
          severity: warning
          service: '{{ $labels.job }}'
        annotations:
          summary: "High latency on {{ $labels.job }}"
          description: "{{ $labels.job }} P95 latency is {{ $value }}s (threshold: 1s)"

      # Pod é‡å•Ÿé »ç¹
      - alert: PodRestartingTooOften
        expr: |
          rate(kube_pod_container_status_restarts_total{namespace="ai-workflow-prod"}[15m]) > 0
        for: 5m
        labels:
          severity: warning
          service: '{{ $labels.pod }}'
        annotations:
          summary: "Pod {{ $labels.pod }} is restarting too often"
          description: "Pod {{ $labels.pod }} has restarted {{ $value }} times in the last 15 minutes"

      # Pod CrashLoopBackOff
      - alert: PodCrashLoopBackOff
        expr: |
          kube_pod_container_status_waiting_reason{reason="CrashLoopBackOff", namespace="ai-workflow-prod"} == 1
        for: 5m
        labels:
          severity: critical
          service: '{{ $labels.pod }}'
        annotations:
          summary: "Pod {{ $labels.pod }} is in CrashLoopBackOff"
          description: "Pod {{ $labels.pod }} has been in CrashLoopBackOff for more than 5 minutes"

      # é«˜ CPU ä½¿ç”¨ç‡
      - alert: HighCPUUsage
        expr: |
          (
            sum(rate(container_cpu_usage_seconds_total{namespace="ai-workflow-prod"}[5m])) by (pod)
            /
            sum(container_spec_cpu_quota{namespace="ai-workflow-prod"} / container_spec_cpu_period{namespace="ai-workflow-prod"}) by (pod)
          ) > 0.8
        for: 10m
        labels:
          severity: warning
          service: '{{ $labels.pod }}'
        annotations:
          summary: "High CPU usage on {{ $labels.pod }}"
          description: "{{ $labels.pod }} CPU usage is {{ $value | humanizePercentage }} (threshold: 80%)"

      # é«˜å…§å­˜ä½¿ç”¨ç‡
      - alert: HighMemoryUsage
        expr: |
          (
            sum(container_memory_usage_bytes{namespace="ai-workflow-prod"}) by (pod)
            /
            sum(container_spec_memory_limit_bytes{namespace="ai-workflow-prod"}) by (pod)
          ) > 0.85
        for: 10m
        labels:
          severity: warning
          service: '{{ $labels.pod }}'
        annotations:
          summary: "High memory usage on {{ $labels.pod }}"
          description: "{{ $labels.pod }} memory usage is {{ $value | humanizePercentage }} (threshold: 85%)"

      # API è«‹æ±‚é‡ç•°å¸¸ä½
      - alert: LowRequestVolume
        expr: |
          sum(rate(http_requests_total{job="agent-service"}[5m])) < 1
        for: 10m
        labels:
          severity: warning
          service: 'agent-service'
        annotations:
          summary: "Low request volume on agent-service"
          description: "agent-service receiving {{ $value }} requests/sec (expected > 1 req/s)"
```

### 17.3.2 æ•¸æ“šåº«å‘Šè­¦è¦å‰‡

**k8s/monitoring/prometheus/rules/database-rules.yaml:**

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-database-rules
  namespace: monitoring
data:
  database.rules.yml: |
    groups:
    - name: database_alerts
      interval: 30s
      rules:

      # PostgreSQL å®•æ©Ÿ
      - alert: PostgreSQLDown
        expr: pg_up == 0
        for: 1m
        labels:
          severity: critical
          service: postgresql
        annotations:
          summary: "PostgreSQL is down"
          description: "PostgreSQL has been down for more than 1 minute"
          runbook_url: "https://wiki.aiworkflow.com/runbooks/postgresql-down"

      # æ•¸æ“šåº«é€£æ¥æ•¸éé«˜
      - alert: HighDatabaseConnections
        expr: |
          sum(pg_stat_database_numbackends) by (datname) > 80
        for: 5m
        labels:
          severity: warning
          service: postgresql
        annotations:
          summary: "High database connections on {{ $labels.datname }}"
          description: "Database {{ $labels.datname }} has {{ $value }} connections (threshold: 80)"

      # æ…¢æŸ¥è©¢æ•¸é‡éå¤š
      - alert: HighSlowQueries
        expr: |
          rate(pg_stat_statements_calls{query=~".*"}[5m]) > 10
        for: 10m
        labels:
          severity: warning
          service: postgresql
        annotations:
          summary: "High number of slow queries"
          description: "{{ $value }} slow queries per second (threshold: 10)"

      # æ•¸æ“šåº«å¤§å°æ¥è¿‘ä¸Šé™
      - alert: DatabaseSizeNearLimit
        expr: |
          (pg_database_size_bytes / 1099511627776) > 0.9
        for: 1h
        labels:
          severity: high
          service: postgresql
        annotations:
          summary: "Database {{ $labels.datname }} size near limit"
          description: "Database {{ $labels.datname }} is {{ $value | humanize }}TB (90% of 1TB limit)"

      # Redis å®•æ©Ÿ
      - alert: RedisDown
        expr: redis_up == 0
        for: 1m
        labels:
          severity: critical
          service: redis
        annotations:
          summary: "Redis is down"
          description: "Redis has been down for more than 1 minute"

      # Redis å…§å­˜ä½¿ç”¨ç‡é«˜
      - alert: HighRedisMemory
        expr: |
          (redis_memory_used_bytes / redis_memory_max_bytes) > 0.9
        for: 5m
        labels:
          severity: high
          service: redis
        annotations:
          summary: "High Redis memory usage"
          description: "Redis memory usage is {{ $value | humanizePercentage }} (threshold: 90%)"

      # Redis ç·©å­˜å‘½ä¸­ç‡ä½
      - alert: LowRedisCacheHitRate
        expr: |
          (
            rate(redis_keyspace_hits_total[5m])
            /
            (rate(redis_keyspace_hits_total[5m]) + rate(redis_keyspace_misses_total[5m]))
          ) < 0.8
        for: 10m
        labels:
          severity: warning
          service: redis
        annotations:
          summary: "Low Redis cache hit rate"
          description: "Redis cache hit rate is {{ $value | humanizePercentage }} (expected > 80%)"
```

### 17.3.3 æ¥­å‹™æŒ‡æ¨™å‘Šè­¦è¦å‰‡

**k8s/monitoring/prometheus/rules/business-rules.yaml:**

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-business-rules
  namespace: monitoring
data:
  business.rules.yml: |
    groups:
    - name: business_alerts
      interval: 1m
      rules:

      # Agent å‰µå»ºå¤±æ•—ç‡é«˜
      - alert: HighAgentCreationFailureRate
        expr: |
          (
            sum(rate(agent_creations_total{status="error"}[5m]))
            /
            sum(rate(agent_creations_total[5m]))
          ) > 0.1
        for: 10m
        labels:
          severity: high
          service: agent-service
        annotations:
          summary: "High agent creation failure rate"
          description: "Agent creation failure rate is {{ $value | humanizePercentage }} (threshold: 10%)"

      # åŸ·è¡Œå¤±æ•—ç‡é«˜
      - alert: HighExecutionFailureRate
        expr: |
          (
            sum(rate(executions_total{status="error"}[5m]))
            /
            sum(rate(executions_total[5m]))
          ) > 0.05
        for: 10m
        labels:
          severity: high
          service: agent-service
        annotations:
          summary: "High execution failure rate"
          description: "Execution failure rate is {{ $value | humanizePercentage }} (threshold: 5%)"

      # OpenAI API èª¿ç”¨å¤±æ•—ç‡é«˜
      - alert: HighOpenAIAPIFailureRate
        expr: |
          (
            sum(rate(openai_api_calls_total{status="error"}[5m]))
            /
            sum(rate(openai_api_calls_total[5m]))
          ) > 0.1
        for: 5m
        labels:
          severity: critical
          service: agent-service
        annotations:
          summary: "High OpenAI API failure rate"
          description: "OpenAI API failure rate is {{ $value | humanizePercentage }} (threshold: 10%)"

      # æ¯æ—¥æˆæœ¬ç•°å¸¸é«˜
      - alert: HighDailyCost
        expr: |
          sum(increase(cost_usd_total[24h])) > 1000
        labels:
          severity: high
          service: billing
        annotations:
          summary: "Daily cost exceeds threshold"
          description: "Daily cost is ${{ $value }} (threshold: $1000)"

      # OpenAI Token ä½¿ç”¨é‡ç•°å¸¸é«˜
      - alert: HighTokenUsage
        expr: |
          sum(increase(openai_tokens_total[1h])) > 1000000
        labels:
          severity: warning
          service: agent-service
        annotations:
          summary: "High OpenAI token usage"
          description: "OpenAI token usage is {{ $value }} tokens/hour (threshold: 1M)"
```

---

## 17.4 å‘Šè­¦è·¯ç”±ç­–ç•¥

### 17.4.1 è·¯ç”±é…ç½®

**è·¯ç”±æ±ºç­–æ¨¹:**

```yaml
route:
  receiver: 'default'
  group_by: ['alertname', 'cluster', 'service']
  group_wait: 10s        # ç­‰å¾…åŒçµ„å‘Šè­¦èšåˆ
  group_interval: 10s    # åŒçµ„å‘Šè­¦ç™¼é€é–“éš”
  repeat_interval: 12h   # é‡è¤‡å‘Šè­¦é–“éš”

  routes:
  # Critical ç´šåˆ¥ -> PagerDuty (24/7 on-call)
  - match:
      severity: critical
    receiver: 'pagerduty-critical'
    group_wait: 10s
    repeat_interval: 5m
    continue: true  # ç¹¼çºŒåŒ¹é…å¾ŒçºŒè·¯ç”±

  # Critical ç´šåˆ¥ -> Slack (å¯¦æ™‚é€šçŸ¥)
  - match:
      severity: critical
    receiver: 'slack-critical'

  # High ç´šåˆ¥ -> Slack + Email
  - match:
      severity: high
    receiver: 'slack-high'
    continue: true

  - match:
      severity: high
    receiver: 'email-ops'

  # Warning ç´šåˆ¥ -> Email (éå·¥ä½œæ™‚é–“éœé»˜)
  - match:
      severity: warning
    receiver: 'email-ops'
    active_time_intervals:
      - business-hours

  # æŒ‰æœå‹™è·¯ç”±
  - match:
      service: postgresql
    receiver: 'slack-dba'

  - match:
      service: redis
    receiver: 'slack-dba'

  # æŒ‰å‘½åç©ºé–“è·¯ç”±
  - match:
      namespace: ai-workflow-staging
    receiver: 'slack-staging'
```

### 17.4.2 æ™‚é–“çª—å£é…ç½®

**å·¥ä½œæ™‚é–“å®šç¾©:**

```yaml
time_intervals:
  - name: business-hours
    time_intervals:
    - times:
      - start_time: '09:00'
        end_time: '17:00'
      weekdays: ['monday:friday']
      location: 'America/New_York'

  - name: out-of-hours
    time_intervals:
    - times:
      - start_time: '17:00'
        end_time: '09:00'
      weekdays: ['monday:friday']
    - weekdays: ['saturday', 'sunday']
```

---

## 17.5 é€šçŸ¥æ¸ é“é…ç½®

### 17.5.1 Slack é€šçŸ¥

**Slack Webhook é…ç½®:**

```yaml
receivers:
- name: 'slack-critical'
  slack_configs:
  - api_url: 'https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK'
    channel: '#alerts-critical'
    username: 'Alertmanager'
    icon_emoji: ':fire:'
    title: 'ğŸš¨ {{ .CommonLabels.alertname }}'
    title_link: 'http://alertmanager:9093'
    text: |
      *Summary:* {{ .CommonAnnotations.summary }}
      *Description:* {{ .CommonAnnotations.description }}
      *Severity:* {{ .CommonLabels.severity }}
      *Service:* {{ .CommonLabels.service }}
      *Firing:* {{ .Alerts.Firing | len }} | *Resolved:* {{ .Alerts.Resolved | len }}
    actions:
    - type: button
      text: 'View in Prometheus'
      url: 'http://prometheus:9090/alerts'
    - type: button
      text: 'View in Grafana'
      url: 'http://grafana:3000/dashboards'
    - type: button
      text: 'Silence'
      url: 'http://alertmanager:9093/#/silences/new'
    send_resolved: true
```

### 17.5.2 Email é€šçŸ¥

**Email é…ç½®:**

```yaml
receivers:
- name: 'email-ops'
  email_configs:
  - to: 'ops@aiworkflow.com'
    from: 'alertmanager@aiworkflow.com'
    smarthost: 'smtp.gmail.com:587'
    auth_username: 'alertmanager@aiworkflow.com'
    auth_password: 'password'
    require_tls: true
    headers:
      Subject: '[{{ .Status }}] {{ .CommonLabels.severity | toUpper }}: {{ .CommonLabels.alertname }}'
    html: |
      {{ template "email.default.html" . }}
    send_resolved: true
```

### 17.5.3 PagerDuty é€šçŸ¥

**PagerDuty é…ç½®:**

```yaml
receivers:
- name: 'pagerduty-critical'
  pagerduty_configs:
  - service_key: 'YOUR_PAGERDUTY_SERVICE_KEY'
    severity: '{{ .CommonLabels.severity }}'
    description: '{{ .CommonAnnotations.summary }}'
    client: 'Alertmanager'
    client_url: 'http://alertmanager:9093'
    details:
      firing: '{{ .Alerts.Firing | len }}'
      resolved: '{{ .Alerts.Resolved | len }}'
      num_firing: '{{ .Alerts.Firing | len }}'
      num_resolved: '{{ .Alerts.Resolved | len }}'
      description: '{{ .CommonAnnotations.description }}'
      service: '{{ .CommonLabels.service }}'
```

### 17.5.4 Webhook é€šçŸ¥ (è‡ªå®šç¾©)

**Webhook é…ç½®:**

```yaml
receivers:
- name: 'webhook-custom'
  webhook_configs:
  - url: 'http://custom-webhook-service/alerts'
    send_resolved: true
    http_config:
      basic_auth:
        username: 'alertmanager'
        password: 'secret'
```

**Webhook Payload æ ¼å¼:**

```json
{
  "receiver": "webhook-custom",
  "status": "firing",
  "alerts": [
    {
      "status": "firing",
      "labels": {
        "alertname": "HighErrorRate",
        "severity": "high",
        "service": "agent-service"
      },
      "annotations": {
        "summary": "High error rate on agent-service",
        "description": "agent-service has error rate 7.5% (threshold: 5%)"
      },
      "startsAt": "2025-01-15T10:30:00Z",
      "endsAt": "0001-01-01T00:00:00Z",
      "generatorURL": "http://prometheus:9090/graph?g0.expr=...",
      "fingerprint": "abc123"
    }
  ],
  "groupLabels": {
    "alertname": "HighErrorRate"
  },
  "commonLabels": {
    "alertname": "HighErrorRate",
    "severity": "high",
    "service": "agent-service"
  },
  "commonAnnotations": {
    "summary": "High error rate on agent-service",
    "description": "agent-service has error rate 7.5% (threshold: 5%)"
  },
  "externalURL": "http://alertmanager:9093",
  "version": "4",
  "groupKey": "{}:{alertname=\"HighErrorRate\"}"
}
```

---

## 17.6 å‘Šè­¦éœé»˜å’ŒæŠ‘åˆ¶

### 17.6.1 å‘Šè­¦éœé»˜ (Silencing)

**æ‰‹å‹•éœé»˜ (é€šé UI):**

```bash
# è¨ªå• Alertmanager UI
http://alertmanager:9093/#/silences

# å‰µå»ºéœé»˜è¦å‰‡
- Matchers: alertname="HighCPUUsage", service="agent-service"
- Start: 2025-01-15 10:00:00
- End: 2025-01-15 12:00:00
- Creator: ops@aiworkflow.com
- Comment: "Planned maintenance window"
```

**é€šé API å‰µå»ºéœé»˜:**

```bash
curl -X POST http://alertmanager:9093/api/v2/silences \
  -H "Content-Type: application/json" \
  -d '{
    "matchers": [
      {
        "name": "alertname",
        "value": "HighCPUUsage",
        "isRegex": false
      },
      {
        "name": "service",
        "value": "agent-service",
        "isRegex": false
      }
    ],
    "startsAt": "2025-01-15T10:00:00Z",
    "endsAt": "2025-01-15T12:00:00Z",
    "createdBy": "ops@aiworkflow.com",
    "comment": "Planned maintenance window"
  }'
```

### 17.6.2 å‘Šè­¦æŠ‘åˆ¶ (Inhibition)

**æŠ‘åˆ¶è¦å‰‡é…ç½®:**

```yaml
inhibit_rules:
# å¦‚æœé›†ç¾¤å®•æ©Ÿï¼ŒæŠ‘åˆ¶æ‰€æœ‰å…¶ä»–å‘Šè­¦
- source_match:
    severity: 'critical'
    alertname: 'ClusterDown'
  target_match_re:
    severity: 'warning|high'
  equal: ['cluster']

# å¦‚æœç¯€é»å®•æ©Ÿï¼ŒæŠ‘åˆ¶è©²ç¯€é»ä¸Šæ‰€æœ‰ Pod å‘Šè­¦
- source_match:
    severity: 'critical'
    alertname: 'NodeDown'
  target_match:
    severity: 'warning'
  equal: ['instance']

# å¦‚æœæ•¸æ“šåº«å®•æ©Ÿï¼ŒæŠ‘åˆ¶æ•¸æ“šåº«é€£æ¥å‘Šè­¦
- source_match:
    alertname: 'PostgreSQLDown'
  target_match:
    alertname: 'HighDatabaseConnections'
  equal: ['instance']

# å¦‚æœæœå‹™å®•æ©Ÿï¼ŒæŠ‘åˆ¶é«˜éŒ¯èª¤ç‡å‘Šè­¦
- source_match:
    severity: 'critical'
    alertname: 'ServiceDown'
  target_match:
    severity: 'high'
    alertname: 'HighErrorRate'
  equal: ['service']
```

---

## 17.7 å‘Šè­¦æœ€ä½³å¯¦è¸

### 17.7.1 å‘Šè­¦è¨­è¨ˆåŸå‰‡

**1. å‘Šè­¦å¿…é ˆå¯æ“ä½œ:**
```yaml
# âœ… å¥½çš„å‘Šè­¦
- alert: HighErrorRate
  expr: (rate(http_requests_total{status=~"5.."}[5m]) / rate(http_requests_total[5m])) > 0.05
  annotations:
    summary: "High error rate"
    description: "Error rate is {{ $value | humanizePercentage }}"
    runbook_url: "https://wiki.aiworkflow.com/runbooks/high-error-rate"
    action: "Check logs: kubectl logs -l app=agent-service --tail=100 | grep ERROR"

# âŒ å£çš„å‘Šè­¦
- alert: Something Wrong
  expr: some_metric > 100
  annotations:
    description: "Something is wrong"  # ä¸æ˜ç¢ºï¼Œç„¡æ³•æ“ä½œ
```

**2. é¿å…å‘Šè­¦ç–²å‹:**
```yaml
# âœ… åˆç†çš„é–¾å€¼å’ŒæŒçºŒæ™‚é–“
- alert: HighCPUUsage
  expr: cpu_usage > 0.8
  for: 10m  # é¿å…çŸ­æš«æ³¢å‹•

# âŒ éæ–¼æ•æ„Ÿ
- alert: HighCPUUsage
  expr: cpu_usage > 0.5
  for: 1m  # æœƒç”¢ç”Ÿå¤§é‡å™ªéŸ³
```

**3. æä¾›ä¸Šä¸‹æ–‡ä¿¡æ¯:**
```yaml
annotations:
  summary: "High error rate on {{ $labels.service }}"
  description: |
    Service {{ $labels.service }} error rate is {{ $value | humanizePercentage }}
    Current RPS: {{ printf "rate(http_requests_total[5m])" | query | first | value }}
    Error count: {{ printf "sum(rate(http_requests_total{status=~\"5..\"}[5m]))" | query | first | value }}
  runbook_url: "https://wiki.aiworkflow.com/runbooks/{{ $labels.alertname }}"
  dashboard_url: "https://grafana.aiworkflow.com/d/service-dashboard?var-service={{ $labels.service }}"
```

### 17.7.2 å‘Šè­¦ Runbook

**Runbook ç¯„ä¾‹:**

```markdown
# Runbook: HighErrorRate

## æ¦‚è¿°
æœå‹™éŒ¯èª¤ç‡è¶…é 5% é–¾å€¼ã€‚

## å½±éŸ¿
ç”¨æˆ¶è«‹æ±‚å¤±æ•—ï¼Œå½±éŸ¿ç”¨æˆ¶é«”é©—ã€‚

## è¨ºæ–·æ­¥é©Ÿ

### 1. æŸ¥çœ‹éŒ¯èª¤æ—¥èªŒ
```bash
kubectl logs -l app=agent-service -n ai-workflow-prod --tail=100 | grep ERROR
```

### 2. æª¢æŸ¥æœ€è¿‘çš„éƒ¨ç½²
```bash
kubectl rollout history deployment/agent-service -n ai-workflow-prod
```

### 3. æŸ¥çœ‹ä¾è³´æœå‹™ç‹€æ…‹
```bash
# PostgreSQL
kubectl exec -it postgresql-0 -n ai-workflow-prod -- pg_isready

# Redis
kubectl exec -it redis-0 -n ai-workflow-prod -- redis-cli ping
```

### 4. æŸ¥çœ‹ Grafana Dashboard
http://grafana:3000/d/agent-service

## è§£æ±ºæ–¹æ¡ˆ

### è‡¨æ™‚æªæ–½
- å›æ»¾åˆ°ä¸Šä¸€å€‹ç©©å®šç‰ˆæœ¬:
  ```bash
  kubectl rollout undo deployment/agent-service -n ai-workflow-prod
  ```

### æ ¹æœ¬è§£æ±º
1. è­˜åˆ¥éŒ¯èª¤æ ¹å› 
2. ä¿®å¾©ä»£ç¢¼
3. æ¸¬è©¦ä¿®å¾©
4. éƒ¨ç½²æ–°ç‰ˆæœ¬

## å‡ç´šè·¯å¾‘
- å¦‚æœ 15 åˆ†é˜å…§ç„¡æ³•è§£æ±º â†’ é€šçŸ¥ Tech Lead
- å¦‚æœ 30 åˆ†é˜å…§ç„¡æ³•è§£æ±º â†’ å‡ç´šåˆ° CTO
```

### 17.7.3 å‘Šè­¦ Checklist

**âœ… å‘Šè­¦é…ç½®:**
- [ ] å‘Šè­¦è¦å‰‡å·²å®šç¾©
- [ ] é–¾å€¼ç¶“éæ¸¬è©¦å’Œèª¿å„ª
- [ ] æŒçºŒæ™‚é–“åˆç† (é¿å…èª¤å ±)
- [ ] å‘Šè­¦ç´šåˆ¥æ­£ç¢º (Critical/High/Warning)
- [ ] åŒ…å« Runbook é€£çµ

**âœ… é€šçŸ¥é…ç½®:**
- [ ] Slack é€šçŸ¥å·²é…ç½®
- [ ] Email é€šçŸ¥å·²é…ç½®
- [ ] PagerDuty å·²é…ç½® (Critical ç´šåˆ¥)
- [ ] å‘Šè­¦è·¯ç”±æ­£ç¢º
- [ ] é€šçŸ¥æ¨¡æ¿æ¸…æ™°æ˜“è®€

**âœ… é‹ç¶­æº–å‚™:**
- [ ] Runbook å·²æ’°å¯«
- [ ] åœ˜éšŠå·²åŸ¹è¨“
- [ ] å‘Šè­¦æ¸¬è©¦å·²å®Œæˆ
- [ ] On-call è¼ªå€¼å·²å®‰æ’
- [ ] å‡ç´šè·¯å¾‘å·²å®šç¾©

**âœ… æŒçºŒå„ªåŒ–:**
- [ ] å®šæœŸå¯©æŸ¥èª¤å ±å‘Šè­¦
- [ ] èª¿æ•´é–¾å€¼å’ŒæŒçºŒæ™‚é–“
- [ ] æ›´æ–° Runbook
- [ ] æ”¶é›†åœ˜éšŠåé¥‹

---

## ç¸½çµ

æœ¬æ–‡æª”æä¾›äº† AI Workflow Platform çš„å®Œæ•´å‘Šè­¦ç³»çµ±è¨­è¨ˆï¼Œæ¶µè“‹:

1. **å‘Šè­¦æ¶æ§‹**: Prometheus + Alertmanager + é€šçŸ¥æ¸ é“
2. **Alertmanager é…ç½®**: è·¯ç”±ã€æ¥æ”¶å™¨ã€æŠ‘åˆ¶è¦å‰‡
3. **å‘Šè­¦è¦å‰‡**: æ‡‰ç”¨/æ•¸æ“šåº«/æ¥­å‹™æŒ‡æ¨™å‘Šè­¦è¦å‰‡
4. **è·¯ç”±ç­–ç•¥**: æŒ‰ç´šåˆ¥ã€æœå‹™ã€æ™‚é–“çª—å£è·¯ç”±
5. **é€šçŸ¥æ¸ é“**: Slackã€Emailã€PagerDutyã€Webhook
6. **éœé»˜å’ŒæŠ‘åˆ¶**: æ‰‹å‹•éœé»˜ã€è‡ªå‹•æŠ‘åˆ¶è¦å‰‡
7. **æœ€ä½³å¯¦è¸**: å‘Šè­¦è¨­è¨ˆåŸå‰‡ã€Runbookã€Checklist

**é—œéµé…ç½®:**
- **å‘Šè­¦ç´šåˆ¥**: Critical (PagerDuty) > High (Slack+Email) > Warning (Email)
- **éŸ¿æ‡‰æ™‚é–“**: Critical (5åˆ†é˜) > High (15åˆ†é˜) > Warning (1å°æ™‚)
- **å‘Šè­¦åŸå‰‡**: å¯æ“ä½œã€æ˜ç¢ºã€åŠæ™‚ã€æº–ç¢º

**ç›¸é—œæ–‡æª”:**
- 15-MONITORING.md - ç›£æ§ç³»çµ±è¨­è¨ˆ
- 16-LOGGING.md - æ—¥èªŒç³»çµ±è¨­è¨ˆ

---

**ç‰ˆæœ¬æ­·å²:**

| ç‰ˆæœ¬ | æ—¥æœŸ | ä½œè€… | è®Šæ›´èªªæ˜ |
|------|------|------|----------|
| 1.0.0 | 2025-01-15 | AI Workflow Team | åˆå§‹ç‰ˆæœ¬ |
