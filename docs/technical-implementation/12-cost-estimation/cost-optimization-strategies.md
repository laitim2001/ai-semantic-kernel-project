# æˆæœ¬å„ªåŒ–ç­–ç•¥

## æ¦‚è¿°

### æ–‡æª”ç›®çš„
æœ¬æ–‡æª”æä¾› AI Agent å·¥ä½œæµå¹³å°çš„å…¨é¢æˆæœ¬å„ªåŒ–ç­–ç•¥,æ¶µè“‹:
- **Right-Sizing ç­–ç•¥** (è³‡æºé©é…èˆ‡èª¿æ•´)
- **é ç•™å¯¦ä¾‹å„ªåŒ–** (Reserved Instances æœ€ä½³å¯¦è¸)
- **è‡ªå‹•æ“´å±•ç­–ç•¥** (Auto-Scaling æˆæœ¬å„ªåŒ–)
- **å­˜å„²å„ªåŒ–** (ç”Ÿå‘½é€±æœŸç®¡ç†, åˆ†å±¤å­˜å„²)
- **ç¶²çµ¡æˆæœ¬å„ªåŒ–** (æµé‡ç®¡ç†, CDN ç­–ç•¥)
- **AI æœå‹™æˆæœ¬æ§åˆ¶** (Token å„ªåŒ–, æ¨¡å‹é¸æ“‡)
- **é–‹ç™¼æ¸¬è©¦ç’°å¢ƒå„ªåŒ–** (ç’°å¢ƒè‡ªå‹•åŒ–ç®¡ç†)
- **FinOps æœ€ä½³å¯¦è¸** (è²¡å‹™é‹ç‡Ÿç®¡ç†)

### å„ªåŒ–ç›®æ¨™

**æˆæœ¬ç¯€çœç›®æ¨™**:
- âœ… **çŸ­æœŸ (3å€‹æœˆ)**: ç¯€çœ 15-20% é›²ç«¯æˆæœ¬
- âœ… **ä¸­æœŸ (6å€‹æœˆ)**: ç¯€çœ 25-35% é›²ç«¯æˆæœ¬
- âœ… **é•·æœŸ (12å€‹æœˆ)**: ç¯€çœ 40-50% é›²ç«¯æˆæœ¬,å»ºç«‹æŒçºŒå„ªåŒ–æ–‡åŒ–

**æ•ˆç‡æŒ‡æ¨™**:
- è³‡æºä½¿ç”¨ç‡: **70-85%** (é¿å…éåº¦é…ç½®æˆ–ä¸è¶³)
- é ç•™å¯¦ä¾‹è¦†è“‹ç‡: **â‰¥ 70%** (ç©©å®šå·¥ä½œè² è¼‰)
- è‡ªå‹•æ“´å±•éŸ¿æ‡‰æ™‚é–“: **â‰¤ 60ç§’** (å¿«é€Ÿé©æ‡‰è² è¼‰è®ŠåŒ–)
- æˆæœ¬ç•°å¸¸æª¢æ¸¬: **â‰¤ 24å°æ™‚** (å¿«é€Ÿç™¼ç¾æˆæœ¬ç•°å¸¸)

---

## Right-Sizing ç­–ç•¥

### è³‡æºä½¿ç”¨ç‡ç›£æ§

#### CPU èˆ‡ Memory åˆ†æå·¥å…·

```python
# scripts/resource_utilization_analyzer.py
"""
è³‡æºä½¿ç”¨ç‡åˆ†æå™¨ - è­˜åˆ¥éåº¦é…ç½®å’Œä¸è¶³é…ç½®çš„è³‡æº
"""
import subprocess
import json
from datetime import datetime, timedelta
from typing import List, Dict
from dataclasses import dataclass

@dataclass
class ResourceMetrics:
    """è³‡æºæŒ‡æ¨™"""
    resource_name: str
    resource_type: str
    cpu_avg: float
    cpu_p95: float
    memory_avg: float
    memory_p95: float
    recommendation: str
    potential_savings: float

class ResourceUtilizationAnalyzer:
    """è³‡æºä½¿ç”¨ç‡åˆ†æå™¨"""

    def __init__(self, prometheus_url: str):
        self.prometheus_url = prometheus_url

    def query_prometheus(self, query: str, days: int = 7) -> Dict:
        """æŸ¥è©¢ Prometheus æŒ‡æ¨™"""
        end_time = datetime.utcnow()
        start_time = end_time - timedelta(days=days)

        # ä½¿ç”¨ Azure Monitor æˆ– Prometheus API æŸ¥è©¢
        # é€™è£¡ç°¡åŒ–ç¤ºä¾‹
        result = {
            "data": {
                "result": []
            }
        }
        return result

    def analyze_aks_nodes(self) -> List[ResourceMetrics]:
        """åˆ†æ AKS ç¯€é»ä½¿ç”¨ç‡"""
        # CPU ä½¿ç”¨ç‡æŸ¥è©¢ (éå» 7 å¤©å¹³å‡)
        cpu_query = """
        avg_over_time(
            (1 - rate(node_cpu_seconds_total{mode="idle"}[5m]))
            [7d:1h]
        ) * 100
        """

        # Memory ä½¿ç”¨ç‡æŸ¥è©¢
        memory_query = """
        avg_over_time(
            (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes))
            [7d:1h]
        ) * 100
        """

        recommendations = []

        # æ¨¡æ“¬åˆ†æçµæœ
        nodes = [
            {
                "name": "aks-system-pool-12345",
                "vm_size": "Standard_D4s_v5",
                "cpu_avg": 35.2,
                "cpu_p95": 58.1,
                "memory_avg": 42.8,
                "memory_p95": 65.3,
                "current_cost": 140.16
            },
            {
                "name": "aks-app-pool-67890",
                "vm_size": "Standard_D8s_v5",
                "cpu_avg": 78.5,
                "cpu_p95": 92.3,
                "memory_avg": 81.2,
                "memory_p95": 94.7,
                "current_cost": 280.32
            },
            {
                "name": "aks-ai-pool-11111",
                "vm_size": "Standard_D16s_v5",
                "cpu_avg": 25.8,
                "cpu_p95": 42.1,
                "memory_avg": 30.5,
                "memory_p95": 48.9,
                "current_cost": 560.64
            }
        ]

        for node in nodes:
            recommendation, savings = self._generate_recommendation(node)

            metrics = ResourceMetrics(
                resource_name=node["name"],
                resource_type=f"AKS Node ({node['vm_size']})",
                cpu_avg=node["cpu_avg"],
                cpu_p95=node["cpu_p95"],
                memory_avg=node["memory_avg"],
                memory_p95=node["memory_p95"],
                recommendation=recommendation,
                potential_savings=savings
            )
            recommendations.append(metrics)

        return recommendations

    def _generate_recommendation(self, node: Dict) -> tuple[str, float]:
        """ç”Ÿæˆå„ªåŒ–å»ºè­°"""
        cpu_avg = node["cpu_avg"]
        cpu_p95 = node["cpu_p95"]
        memory_avg = node["memory_avg"]
        memory_p95 = node["memory_p95"]
        current_cost = node["current_cost"]

        # éåº¦é…ç½®æª¢æ¸¬ (CPU < 50%, Memory < 60%)
        if cpu_p95 < 50 and memory_p95 < 60:
            # å»ºè­°é™ç´šåˆ°æ›´å°çš„ VM
            new_vm_map = {
                "Standard_D16s_v5": ("Standard_D8s_v5", 280.32, 50),
                "Standard_D8s_v5": ("Standard_D4s_v5", 140.16, 50),
                "Standard_D4s_v5": ("Standard_D2s_v5", 70.08, 50)
            }

            if node["vm_size"] in new_vm_map:
                new_vm, new_cost, savings_pct = new_vm_map[node["vm_size"]]
                savings = current_cost - new_cost
                return (
                    f"â¬‡ï¸ é™ç´šåˆ° {new_vm} (CPU/Memory ä½¿ç”¨ç‡åä½)",
                    savings
                )

        # é…ç½®ä¸è¶³æª¢æ¸¬ (CPU > 85% æˆ– Memory > 90%)
        elif cpu_p95 > 85 or memory_p95 > 90:
            # å»ºè­°å‡ç´šåˆ°æ›´å¤§çš„ VM
            new_vm_map = {
                "Standard_D2s_v5": ("Standard_D4s_v5", 140.16),
                "Standard_D4s_v5": ("Standard_D8s_v5", 280.32),
                "Standard_D8s_v5": ("Standard_D16s_v5", 560.64)
            }

            if node["vm_size"] in new_vm_map:
                new_vm, new_cost = new_vm_map[node["vm_size"]]
                additional_cost = new_cost - current_cost
                return (
                    f"â¬†ï¸ å‡ç´šåˆ° {new_vm} (CPU/Memory ä½¿ç”¨ç‡éé«˜)",
                    -additional_cost
                )

        # é…ç½®åˆé©
        return ("âœ… é…ç½®åˆé©,ç„¡éœ€èª¿æ•´", 0.0)

    def generate_report(self) -> str:
        """ç”Ÿæˆå„ªåŒ–å ±å‘Š"""
        recommendations = self.analyze_aks_nodes()

        total_savings = sum(r.potential_savings for r in recommendations)

        report = f"""
=== è³‡æºä½¿ç”¨ç‡åˆ†æå ±å‘Š ===
ç”Ÿæˆæ™‚é–“: {datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S UTC')}

ç¸½æ½›åœ¨ç¯€çœ: ${total_savings:,.2f}/æœˆ

è³‡æºåˆ†æ:
{"="*80}
"""

        for rec in recommendations:
            report += f"""
è³‡æº: {rec.resource_name}
é¡å‹: {rec.resource_type}
CPU ä½¿ç”¨ç‡: å¹³å‡ {rec.cpu_avg:.1f}%, P95 {rec.cpu_p95:.1f}%
Memory ä½¿ç”¨ç‡: å¹³å‡ {rec.memory_avg:.1f}%, P95 {rec.memory_p95:.1f}%
å»ºè­°: {rec.recommendation}
æ½›åœ¨ç¯€çœ: ${rec.potential_savings:,.2f}/æœˆ

"""

        return report


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    analyzer = ResourceUtilizationAnalyzer(prometheus_url="http://prometheus:9090")
    print(analyzer.generate_report())
```

**è¼¸å‡ºç¤ºä¾‹**:
```
=== è³‡æºä½¿ç”¨ç‡åˆ†æå ±å‘Š ===
ç”Ÿæˆæ™‚é–“: 2025-11-02 10:30:00 UTC

ç¸½æ½›åœ¨ç¯€çœ: $280.32/æœˆ

è³‡æºåˆ†æ:
================================================================================

è³‡æº: aks-system-pool-12345
é¡å‹: AKS Node (Standard_D4s_v5)
CPU ä½¿ç”¨ç‡: å¹³å‡ 35.2%, P95 58.1%
Memory ä½¿ç”¨ç‡: å¹³å‡ 42.8%, P95 65.3%
å»ºè­°: âœ… é…ç½®åˆé©,ç„¡éœ€èª¿æ•´
æ½›åœ¨ç¯€çœ: $0.00/æœˆ

è³‡æº: aks-app-pool-67890
é¡å‹: AKS Node (Standard_D8s_v5)
CPU ä½¿ç”¨ç‡: å¹³å‡ 78.5%, P95 92.3%
Memory ä½¿ç”¨ç‡: å¹³å‡ 81.2%, P95 94.7%
å»ºè­°: â¬†ï¸ å‡ç´šåˆ° Standard_D16s_v5 (CPU/Memory ä½¿ç”¨ç‡éé«˜)
æ½›åœ¨ç¯€çœ: $-280.32/æœˆ

è³‡æº: aks-ai-pool-11111
é¡å‹: AKS Node (Standard_D16s_v5)
CPU ä½¿ç”¨ç‡: å¹³å‡ 25.8%, P95 42.1%
Memory ä½¿ç”¨ç‡: å¹³å‡ 30.5%, P95 48.9%
å»ºè­°: â¬‡ï¸ é™ç´šåˆ° Standard_D8s_v5 (CPU/Memory ä½¿ç”¨ç‡åä½)
æ½›åœ¨ç¯€çœ: $280.32/æœˆ
```

### è³‡æºèª¿æ•´æµç¨‹

```bash
#!/bin/bash
# scripts/right-size-aks-node-pool.sh

set -e

NODE_POOL_NAME="$1"
NEW_VM_SIZE="$2"
CLUSTER_NAME="aiagent-aks-prod"
RESOURCE_GROUP="aiagent-prod-rg"

echo "=== AKS Node Pool Right-Sizing ==="
echo "Node Pool: $NODE_POOL_NAME"
echo "Current VM Size: (æŸ¥è©¢ä¸­...)"

# æŸ¥è©¢ç•¶å‰é…ç½®
CURRENT_CONFIG=$(az aks nodepool show \
  --resource-group "$RESOURCE_GROUP" \
  --cluster-name "$CLUSTER_NAME" \
  --name "$NODE_POOL_NAME" \
  --query '{vmSize:vmSize, count:count, minCount:minCount, maxCount:maxCount}' \
  -o json)

echo "ç•¶å‰é…ç½®: $CURRENT_CONFIG"

# ç¢ºèªæ“ä½œ
read -p "ç¢ºå®šè¦å°‡ Node Pool èª¿æ•´ç‚º $NEW_VM_SIZE å—? (yes/no): " CONFIRM

if [ "$CONFIRM" != "yes" ]; then
  echo "æ“ä½œå·²å–æ¶ˆ"
  exit 0
fi

# å‰µå»ºæ–°çš„ Node Pool (Blue-Green ç­–ç•¥)
NEW_NODE_POOL_NAME="${NODE_POOL_NAME}-new"

echo "Step 1: å‰µå»ºæ–°çš„ Node Pool ($NEW_NODE_POOL_NAME)..."
az aks nodepool add \
  --resource-group "$RESOURCE_GROUP" \
  --cluster-name "$CLUSTER_NAME" \
  --name "$NEW_NODE_POOL_NAME" \
  --node-vm-size "$NEW_VM_SIZE" \
  --node-count 1 \
  --enable-cluster-autoscaler \
  --min-count 1 \
  --max-count 5 \
  --mode User

echo "Step 2: ç­‰å¾…æ–° Node Pool å°±ç·’..."
sleep 60

# é©—è­‰æ–°ç¯€é»å¥åº·
kubectl wait --for=condition=Ready nodes -l agentpool="$NEW_NODE_POOL_NAME" --timeout=300s

echo "Step 3: å°‡å·¥ä½œè² è¼‰é·ç§»åˆ°æ–° Node Pool..."
# ä½¿ç”¨ node affinity æˆ– taint/toleration ä¾†é·ç§»

# Cordon èˆŠç¯€é» (é˜²æ­¢æ–° Pod èª¿åº¦)
kubectl cordon -l agentpool="$NODE_POOL_NAME"

# Drain èˆŠç¯€é» (é©…é€ç¾æœ‰ Pod)
kubectl drain -l agentpool="$NODE_POOL_NAME" \
  --ignore-daemonsets \
  --delete-emptydir-data \
  --force \
  --grace-period=300

echo "Step 4: åˆªé™¤èˆŠ Node Pool..."
az aks nodepool delete \
  --resource-group "$RESOURCE_GROUP" \
  --cluster-name "$CLUSTER_NAME" \
  --name "$NODE_POOL_NAME" \
  --no-wait

echo "Step 5: é‡å‘½åæ–° Node Pool (å¯é¸)..."
# Azure ä¸æ”¯æŒ Node Pool é‡å‘½å,ä¿ç•™ -new å¾Œç¶´

echo "âœ… Right-Sizing å®Œæˆ!"
echo "æ–° Node Pool: $NEW_NODE_POOL_NAME (VM Size: $NEW_VM_SIZE)"
```

---

## é ç•™å¯¦ä¾‹å„ªåŒ–

### Reserved Instances è³¼è²·ç­–ç•¥

#### é ç•™å¯¦ä¾‹è¦†è“‹ç‡åˆ†æ

```csharp
// src/FinOps/ReservedInstanceCoverageAnalyzer.cs
using System;
using System.Collections.Generic;
using System.Linq;

public sealed class ReservedInstanceCoverageAnalyzer
{
    public record ResourceUsage(
        string ResourceType,
        string VmSize,
        int AverageCount,
        int MinCount,
        int MaxCount,
        decimal HourlyPayAsYouGoRate,
        decimal HourlyReservedRate1Year,
        decimal HourlyReservedRate3Year
    );

    public record CoverageRecommendation(
        string ResourceType,
        string VmSize,
        int RecommendedReservedCount,
        int ReservationTerm,
        decimal MonthlySavings,
        decimal AnnualSavings,
        string Rationale
    );

    public List<CoverageRecommendation> AnalyzeCoverage(List<ResourceUsage> usages)
    {
        var recommendations = new List<CoverageRecommendation>();

        foreach (var usage in usages)
        {
            // ç­–ç•¥: ç‚ºæœ€ä½ä½¿ç”¨é‡ (MinCount) è³¼è²· 3å¹´é ç•™
            // ç‚ºç©©å®šä½¿ç”¨é‡ (MinCount åˆ° AvgCount) è³¼è²· 1å¹´é ç•™
            // å³°å€¼éƒ¨åˆ† (AvgCount åˆ° MaxCount) ä½¿ç”¨æŒ‰éœ€è¨ˆè²»

            if (usage.MinCount > 0)
            {
                // 3å¹´é ç•™å»ºè­°
                var savings3Year = CalculateSavings(
                    usage.MinCount,
                    usage.HourlyPayAsYouGoRate,
                    usage.HourlyReservedRate3Year,
                    term: 3
                );

                recommendations.Add(new CoverageRecommendation(
                    ResourceType: usage.ResourceType,
                    VmSize: usage.VmSize,
                    RecommendedReservedCount: usage.MinCount,
                    ReservationTerm: 3,
                    MonthlySavings: savings3Year.MonthlySavings,
                    AnnualSavings: savings3Year.AnnualSavings,
                    Rationale: $"åŸºç¤å®¹é‡ ({usage.MinCount} å¯¦ä¾‹) æŒçºŒé‹è¡Œ,é©åˆ 3å¹´é ç•™"
                ));
            }

            // 1å¹´é ç•™å»ºè­° (ç©©å®šå¢é•·éƒ¨åˆ†)
            var stableGrowth = usage.AverageCount - usage.MinCount;
            if (stableGrowth > 0)
            {
                var savings1Year = CalculateSavings(
                    stableGrowth,
                    usage.HourlyPayAsYouGoRate,
                    usage.HourlyReservedRate1Year,
                    term: 1
                );

                recommendations.Add(new CoverageRecommendation(
                    ResourceType: usage.ResourceType,
                    VmSize: usage.VmSize,
                    RecommendedReservedCount: stableGrowth,
                    ReservationTerm: 1,
                    MonthlySavings: savings1Year.MonthlySavings,
                    AnnualSavings: savings1Year.AnnualSavings,
                    Rationale: $"ç©©å®šå¢é•·å®¹é‡ ({stableGrowth} å¯¦ä¾‹),é©åˆ 1å¹´é ç•™"
                ));
            }

            // å³°å€¼éƒ¨åˆ†ä½¿ç”¨æŒ‰éœ€è¨ˆè²» (ç„¡å»ºè­°)
            var burstCapacity = usage.MaxCount - usage.AverageCount;
            if (burstCapacity > 0)
            {
                Console.WriteLine($"å³°å€¼å®¹é‡ {burstCapacity} å¯¦ä¾‹å»ºè­°ä½¿ç”¨æŒ‰éœ€è¨ˆè²»æˆ– Spot Instances");
            }
        }

        return recommendations;
    }

    private (decimal MonthlySavings, decimal AnnualSavings) CalculateSavings(
        int instanceCount,
        decimal payAsYouGoRate,
        decimal reservedRate,
        int term)
    {
        var hoursPerMonth = 730m;  // å¹³å‡æ¯æœˆå°æ™‚æ•¸
        var hoursPerYear = hoursPerMonth * 12;

        var monthlyPayAsYouGo = instanceCount * payAsYouGoRate * hoursPerMonth;
        var monthlyReserved = instanceCount * reservedRate * hoursPerMonth;
        var monthlySavings = monthlyPayAsYouGo - monthlyReserved;

        var annualSavings = monthlySavings * 12;

        return (monthlySavings, annualSavings);
    }

    public void GenerateReport(List<CoverageRecommendation> recommendations)
    {
        Console.WriteLine("=== é ç•™å¯¦ä¾‹è¦†è“‹ç‡åˆ†æå ±å‘Š ===\n");

        var totalMonthlySavings = recommendations.Sum(r => r.MonthlySavings);
        var totalAnnualSavings = recommendations.Sum(r => r.AnnualSavings);

        Console.WriteLine($"ç¸½æ½›åœ¨ç¯€çœ: ${totalMonthlySavings:N2}/æœˆ, ${totalAnnualSavings:N2}/å¹´\n");

        foreach (var rec in recommendations)
        {
            Console.WriteLine($"è³‡æº: {rec.ResourceType} ({rec.VmSize})");
            Console.WriteLine($"å»ºè­°é ç•™æ•¸é‡: {rec.RecommendedReservedCount} å¯¦ä¾‹");
            Console.WriteLine($"é ç•™æœŸé™: {rec.ReservationTerm} å¹´");
            Console.WriteLine($"æœˆåº¦ç¯€çœ: ${rec.MonthlySavings:N2}");
            Console.WriteLine($"å¹´åº¦ç¯€çœ: ${rec.AnnualSavings:N2}");
            Console.WriteLine($"ç†ç”±: {rec.Rationale}");
            Console.WriteLine();
        }
    }
}

// ä½¿ç”¨ç¤ºä¾‹
var analyzer = new ReservedInstanceCoverageAnalyzer();

var usages = new List<ReservedInstanceCoverageAnalyzer.ResourceUsage>
{
    new(
        ResourceType: "AKS Node",
        VmSize: "Standard_D8s_v5",
        AverageCount: 5,
        MinCount: 3,
        MaxCount: 10,
        HourlyPayAsYouGoRate: 0.384m,
        HourlyReservedRate1Year: 0.2496m,
        HourlyReservedRate3Year: 0.1784m
    ),
    new(
        ResourceType: "PostgreSQL Flexible Server",
        VmSize: "Standard_D4ds_v5",
        AverageCount: 2,
        MinCount: 2,
        MaxCount: 2,
        HourlyPayAsYouGoRate: 0.283m,
        HourlyReservedRate1Year: 0.184m,
        HourlyReservedRate3Year: 0.131m
    )
};

var recommendations = analyzer.AnalyzeCoverage(usages);
analyzer.GenerateReport(recommendations);
```

### é ç•™å¯¦ä¾‹è³¼è²·è…³æœ¬

```bash
#!/bin/bash
# scripts/purchase-reserved-instances.sh

set -e

echo "=== Azure Reserved Instance è³¼è²·å·¥å…· ==="

# é…ç½®åƒæ•¸
RESOURCE_TYPE="$1"  # VirtualMachines, SQLDatabases
VM_SIZE="$2"        # Standard_D8s_v5
QUANTITY="$3"       # 5
TERM="$4"           # 1 å¹´æˆ– 3 å¹´
REGION="eastus"

# é©—è­‰åƒæ•¸
if [ -z "$RESOURCE_TYPE" ] || [ -z "$VM_SIZE" ] || [ -z "$QUANTITY" ] || [ -z "$TERM" ]; then
  echo "ç”¨æ³•: $0 <RESOURCE_TYPE> <VM_SIZE> <QUANTITY> <TERM>"
  echo "ç¤ºä¾‹: $0 VirtualMachines Standard_D8s_v5 5 P1Y"
  exit 1
fi

# æŸ¥è©¢é ç•™å¯¦ä¾‹å®šåƒ¹
echo "Step 1: æŸ¥è©¢é ç•™å¯¦ä¾‹å®šåƒ¹..."
az reservations catalog show \
  --reserved-resource-type "$RESOURCE_TYPE" \
  --location "$REGION" \
  --query "[?name=='$VM_SIZE']" \
  -o table

# è¨ˆç®—æˆæœ¬
echo ""
echo "Step 2: è¨ˆç®—é ç•™å¯¦ä¾‹æˆæœ¬..."

# è³¼è²·é ç•™å¯¦ä¾‹ (éœ€è¦ Billing Admin æ¬Šé™)
echo ""
echo "Step 3: è³¼è²·é ç•™å¯¦ä¾‹..."
echo "âš ï¸  æ³¨æ„: æ­¤æ“ä½œéœ€è¦ Billing Administrator è§’è‰²"
read -p "ç¢ºå®šè¦è³¼è²· $QUANTITY å€‹ $VM_SIZE é ç•™å¯¦ä¾‹ (æœŸé™: $TERM) å—? (yes/no): " CONFIRM

if [ "$CONFIRM" != "yes" ]; then
  echo "è³¼è²·å·²å–æ¶ˆ"
  exit 0
fi

# ä½¿ç”¨ Azure Portal æˆ– PowerShell è³¼è²·
# az CLI ç›®å‰ä¸å®Œå…¨æ”¯æŒé ç•™å¯¦ä¾‹è³¼è²·,å»ºè­°ä½¿ç”¨ Portal

echo "è«‹è¨ªå• Azure Portal å®Œæˆè³¼è²·:"
echo "https://portal.azure.com/#blade/Microsoft_Azure_Reservations/CreateBlade"
echo ""
echo "è³¼è²·åƒæ•¸:"
echo "  è³‡æºé¡å‹: $RESOURCE_TYPE"
echo "  VM å¤§å°: $VM_SIZE"
echo "  æ•¸é‡: $QUANTITY"
echo "  æœŸé™: $TERM"
echo "  å€åŸŸ: $REGION"
```

---

## è‡ªå‹•æ“´å±•ç­–ç•¥

### Horizontal Pod Autoscaler (HPA) å„ªåŒ–

```yaml
# k8s/base/hpa-optimized.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: api-deployment-hpa
  namespace: aiagent-prod
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: api-deployment

  minReplicas: 3  # æœ€å°å‰¯æœ¬æ•¸ (ä¿è­‰é«˜å¯ç”¨)
  maxReplicas: 20 # æœ€å¤§å‰¯æœ¬æ•¸ (æˆæœ¬æ§åˆ¶ä¸Šé™)

  # å¤šæŒ‡æ¨™æ“´å±•ç­–ç•¥
  metrics:
    # CPU æŒ‡æ¨™
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 70  # 70% CPU è§¸ç™¼æ“´å±•

    # Memory æŒ‡æ¨™
    - type: Resource
      resource:
        name: memory
        target:
          type: Utilization
          averageUtilization: 80  # 80% Memory è§¸ç™¼æ“´å±•

    # è‡ªå®šç¾©æŒ‡æ¨™ - HTTP è«‹æ±‚é€Ÿç‡
    - type: Pods
      pods:
        metric:
          name: http_requests_per_second
        target:
          type: AverageValue
          averageValue: "1000"  # æ¯ç§’ 1000 è«‹æ±‚è§¸ç™¼æ“´å±•

  # æ“´å±•è¡Œç‚ºé…ç½® (é¿å…é »ç¹æ“´ç¸®)
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 60  # æ“´å±•å‰è§€å¯Ÿ 60 ç§’
      policies:
      - type: Percent
        value: 50  # æ¯æ¬¡æœ€å¤šå¢åŠ  50% Pod
        periodSeconds: 60
      - type: Pods
        value: 5   # æˆ–æ¯æ¬¡æœ€å¤šå¢åŠ  5 å€‹ Pod
        periodSeconds: 60
      selectPolicy: Max  # é¸æ“‡æœ€æ¿€é€²çš„ç­–ç•¥

    scaleDown:
      stabilizationWindowSeconds: 300  # ç¸®å®¹å‰è§€å¯Ÿ 5 åˆ†é˜ (é¿å…æŠ–å‹•)
      policies:
      - type: Percent
        value: 10  # æ¯æ¬¡æœ€å¤šæ¸›å°‘ 10% Pod
        periodSeconds: 60
      - type: Pods
        value: 2   # æˆ–æ¯æ¬¡æœ€å¤šæ¸›å°‘ 2 å€‹ Pod
        periodSeconds: 60
      selectPolicy: Min  # é¸æ“‡æœ€ä¿å®ˆçš„ç­–ç•¥
```

### Cluster Autoscaler æˆæœ¬å„ªåŒ–é…ç½®

```yaml
# k8s/cluster-autoscaler-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-autoscaler-config
  namespace: kube-system
data:
  config.yaml: |
    # æ“´å±•ç­–ç•¥
    expanderPriorities:
      # å„ªå…ˆä½¿ç”¨é ç•™å¯¦ä¾‹ç¯€é»æ± 
      - name: reserved-instance-pool
        priority: 100
      # å…¶æ¬¡ä½¿ç”¨æŒ‰éœ€ç¯€é»æ± 
      - name: on-demand-pool
        priority: 50
      # æœ€å¾Œä½¿ç”¨ Spot ç¯€é»æ±  (æœ€ä¾¿å®œä½†å¯èƒ½è¢«å›æ”¶)
      - name: spot-pool
        priority: 10

    # ç¸®å®¹é…ç½®
    scaleDownEnabled: true
    scaleDownUnneededTime: 10m  # ç¯€é»é–’ç½® 10 åˆ†é˜å¾Œç¸®å®¹
    scaleDownUtilizationThreshold: 0.5  # ç¯€é»ä½¿ç”¨ç‡ < 50% è¦–ç‚ºé–’ç½®

    # æˆæœ¬å„ªåŒ–
    balanceSimilarNodeGroups: true  # å¹³è¡¡ç›¸ä¼¼ç¯€é»çµ„
    skipNodesWithSystemPods: false  # å…è¨±ç¸®å®¹ç³»çµ± Pod ç¯€é»
    skipNodesWithLocalStorage: false

    # æ™‚é–“èª¿åº¦å„ªåŒ– (å·¥ä½œæ™‚é–“ vs. éå·¥ä½œæ™‚é–“)
    scheduling:
      - name: "business-hours"
        cron: "0 8 * * 1-5"  # é€±ä¸€åˆ°é€±äº” 8:00
        minNodes: 10
        maxNodes: 20
      - name: "off-hours"
        cron: "0 18 * * *"  # æ¯å¤© 18:00
        minNodes: 5
        maxNodes: 10
      - name: "weekend"
        cron: "0 0 * * 6,0"  # é€±æœ«
        minNodes: 3
        maxNodes: 8
```

### æ™‚é–“èª¿åº¦è‡ªå‹•ç¸®å®¹

```python
# scripts/scheduled_scaling.py
"""
æ™‚é–“èª¿åº¦è‡ªå‹•ç¸®å®¹ - æ ¹æ“šæ¥­å‹™æ™‚é–“è‡ªå‹•èª¿æ•´é›†ç¾¤è¦æ¨¡
"""
import os
from datetime import datetime, time
from azure.identity import DefaultAzureCredential
from azure.mgmt.containerservice import ContainerServiceClient

class ScheduledScalingManager:
    """æ™‚é–“èª¿åº¦ç¸®å®¹ç®¡ç†å™¨"""

    def __init__(self, subscription_id: str, resource_group: str, cluster_name: str):
        self.subscription_id = subscription_id
        self.resource_group = resource_group
        self.cluster_name = cluster_name

        credential = DefaultAzureCredential()
        self.client = ContainerServiceClient(credential, subscription_id)

    def get_current_schedule(self) -> dict:
        """ç²å–ç•¶å‰æ‡‰è©²çš„èª¿åº¦é…ç½®"""
        now = datetime.now()
        current_time = now.time()
        is_weekday = now.weekday() < 5  # 0-4 æ˜¯é€±ä¸€åˆ°é€±äº”

        # æ¥­å‹™æ™‚é–“ (é€±ä¸€åˆ°é€±äº” 8:00-18:00)
        if is_weekday and time(8, 0) <= current_time < time(18, 0):
            return {
                "period": "business-hours",
                "application_pool": {"min": 5, "max": 15},
                "ai_pool": {"min": 4, "max": 10}
            }

        # é€±æœ«
        elif not is_weekday:
            return {
                "period": "weekend",
                "application_pool": {"min": 2, "max": 8},
                "ai_pool": {"min": 1, "max": 5}
            }

        # éæ¥­å‹™æ™‚é–“ (å·¥ä½œæ—¥æ™šä¸Š)
        else:
            return {
                "period": "off-hours",
                "application_pool": {"min": 3, "max": 10},
                "ai_pool": {"min": 2, "max": 6}
            }

    def apply_scheduled_scaling(self):
        """æ‡‰ç”¨æ™‚é–“èª¿åº¦ç¸®å®¹"""
        schedule = self.get_current_schedule()
        print(f"ç•¶å‰æ™‚æ®µ: {schedule['period']}")

        # èª¿æ•´ Application Pool
        self._update_node_pool(
            "application-pool",
            min_count=schedule["application_pool"]["min"],
            max_count=schedule["application_pool"]["max"]
        )

        # èª¿æ•´ AI Pool
        self._update_node_pool(
            "ai-pool",
            min_count=schedule["ai_pool"]["min"],
            max_count=schedule["ai_pool"]["max"]
        )

        print(f"âœ… æ™‚é–“èª¿åº¦ç¸®å®¹å·²æ‡‰ç”¨")

    def _update_node_pool(self, node_pool_name: str, min_count: int, max_count: int):
        """æ›´æ–°ç¯€é»æ± é…ç½®"""
        print(f"æ›´æ–° {node_pool_name}: min={min_count}, max={max_count}")

        # ç²å–ç•¶å‰ç¯€é»æ± é…ç½®
        agent_pool = self.client.agent_pools.get(
            self.resource_group,
            self.cluster_name,
            node_pool_name
        )

        # åƒ…æ›´æ–° autoscaler é…ç½®
        agent_pool.min_count = min_count
        agent_pool.max_count = max_count

        # æ‡‰ç”¨æ›´æ–°
        self.client.agent_pools.begin_create_or_update(
            self.resource_group,
            self.cluster_name,
            node_pool_name,
            agent_pool
        ).result()

# ä½¿ç”¨ç¤ºä¾‹ (é€šé Kubernetes CronJob æ¯å°æ™‚åŸ·è¡Œä¸€æ¬¡)
if __name__ == "__main__":
    manager = ScheduledScalingManager(
        subscription_id=os.getenv("AZURE_SUBSCRIPTION_ID"),
        resource_group="aiagent-prod-rg",
        cluster_name="aiagent-aks-prod"
    )
    manager.apply_scheduled_scaling()
```

**éƒ¨ç½²ç‚º Kubernetes CronJob**:

```yaml
# k8s/scheduled-scaling-cronjob.yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: scheduled-scaling
  namespace: kube-system
spec:
  schedule: "0 * * * *"  # æ¯å°æ™‚åŸ·è¡Œä¸€æ¬¡
  jobTemplate:
    spec:
      template:
        spec:
          serviceAccountName: cluster-autoscaler
          containers:
          - name: scheduled-scaling
            image: aiagent-acr.azurecr.io/scheduled-scaling:latest
            env:
            - name: AZURE_SUBSCRIPTION_ID
              valueFrom:
                secretKeyRef:
                  name: azure-credentials
                  key: subscription-id
            - name: AZURE_CLIENT_ID
              valueFrom:
                secretKeyRef:
                  name: azure-credentials
                  key: client-id
            - name: AZURE_CLIENT_SECRET
              valueFrom:
                secretKeyRef:
                  name: azure-credentials
                  key: client-secret
            - name: AZURE_TENANT_ID
              valueFrom:
                secretKeyRef:
                  name: azure-credentials
                  key: tenant-id
          restartPolicy: OnFailure
```

---

## å­˜å„²å„ªåŒ–

### Blob Storage ç”Ÿå‘½é€±æœŸç®¡ç†

```json
{
  "rules": [
    {
      "enabled": true,
      "name": "move-old-logs-to-cool",
      "type": "Lifecycle",
      "definition": {
        "filters": {
          "blobTypes": ["blockBlob"],
          "prefixMatch": ["logs/"]
        },
        "actions": {
          "baseBlob": {
            "tierToCool": {
              "daysAfterModificationGreaterThan": 30
            },
            "tierToArchive": {
              "daysAfterModificationGreaterThan": 90
            },
            "delete": {
              "daysAfterModificationGreaterThan": 365
            }
          }
        }
      }
    },
    {
      "enabled": true,
      "name": "move-old-backups-to-archive",
      "type": "Lifecycle",
      "definition": {
        "filters": {
          "blobTypes": ["blockBlob"],
          "prefixMatch": ["backups/postgresql/"]
        },
        "actions": {
          "baseBlob": {
            "tierToArchive": {
              "daysAfterModificationGreaterThan": 35
            },
            "delete": {
              "daysAfterModificationGreaterThan": 90
            }
          }
        }
      }
    },
    {
      "enabled": true,
      "name": "delete-temp-files",
      "type": "Lifecycle",
      "definition": {
        "filters": {
          "blobTypes": ["blockBlob"],
          "prefixMatch": ["temp/"]
        },
        "actions": {
          "baseBlob": {
            "delete": {
              "daysAfterModificationGreaterThan": 7
            }
          }
        }
      }
    }
  ]
}
```

**æ‡‰ç”¨ç”Ÿå‘½é€±æœŸç­–ç•¥**:

```bash
#!/bin/bash
# scripts/apply-blob-lifecycle-policy.sh

STORAGE_ACCOUNT="aiagentstorageprod"
RESOURCE_GROUP="aiagent-prod-rg"
POLICY_FILE="blob-lifecycle-policy.json"

echo "=== æ‡‰ç”¨ Blob Storage ç”Ÿå‘½é€±æœŸç­–ç•¥ ==="

# æ‡‰ç”¨ç­–ç•¥
az storage account management-policy create \
  --account-name "$STORAGE_ACCOUNT" \
  --resource-group "$RESOURCE_GROUP" \
  --policy "@$POLICY_FILE"

echo "âœ… ç”Ÿå‘½é€±æœŸç­–ç•¥å·²æ‡‰ç”¨"

# é©—è­‰ç­–ç•¥
az storage account management-policy show \
  --account-name "$STORAGE_ACCOUNT" \
  --resource-group "$RESOURCE_GROUP" \
  -o table
```

### PostgreSQL å‚™ä»½å„ªåŒ–

```yaml
postgresql_backup_optimization:
  strategies:
    - name: "å¢é‡å‚™ä»½å„ªåŒ–"
      description: "æ¸›å°‘å‚™ä»½å­˜å„²ç©ºé–“"
      implementation:
        - "ä½¿ç”¨ WAL æ­¸æª”å¯¦ç¾å¢é‡å‚™ä»½"
        - "åƒ…ä¿ç•™æœ€è¿‘ 7 å¤©çš„å®Œæ•´å‚™ä»½"
        - "èˆŠå‚™ä»½è½‰ç‚º Archive Tier"
      savings: "$50/æœˆ"

    - name: "å£“ç¸®å„ªåŒ–"
      description: "æé«˜å‚™ä»½å£“ç¸®ç‡"
      implementation:
        - "pg_dump --compress=9 (æœ€é«˜å£“ç¸®)"
        - "ä½¿ç”¨ zstd æ›¿ä»£ gzip (æ›´é«˜å£“ç¸®ç‡)"
      compression_ratio: "70% ç©ºé–“ç¯€çœ"
      savings: "$30/æœˆ"

    - name: "ä¿ç•™ç­–ç•¥å„ªåŒ–"
      description: "åˆç†çš„å‚™ä»½ä¿ç•™æœŸ"
      policy:
        daily_backups: "7 å¤©"
        weekly_backups: "4 é€±"
        monthly_backups: "12 å€‹æœˆ"
        yearly_backups: "3 å¹´ (Archive Tier)"
      savings: "$80/æœˆ"
```

---

## ç¶²çµ¡æˆæœ¬å„ªåŒ–

### CDN èˆ‡éœæ…‹è³‡æºå„ªåŒ–

```yaml
cdn_optimization:
  strategy: "ä½¿ç”¨ Azure CDN æ¸›å°‘ Blob Storage å‡ºç«™æµé‡"

  configuration:
    cdn_profile: "aiagent-cdn-standard"
    cdn_endpoint: "aiagent-static.azureedge.net"
    origin: "aiagentstorageprod.blob.core.windows.net"

  cost_comparison:
    without_cdn:
      blob_egress_gb: 1000  # 1 TB
      cost_per_gb: 0.087
      monthly_cost: "$87.00"

    with_cdn:
      cdn_egress_gb: 1000
      cdn_cost_per_gb: 0.081  # CDN å‡ºç«™ç¨ä¾¿å®œ
      blob_to_cdn_transfer: "å…è²» (åŒå€åŸŸ)"
      monthly_cost: "$81.00"
      cache_hit_rate: "85%"  # 85% è«‹æ±‚å‘½ä¸­ç·©å­˜
      effective_monthly_cost: "$12.15"  # åƒ… 15% æµé‡å›æº
      savings: "$74.85/æœˆ"

  implementation:
    - "é…ç½® CDN Profile å’Œ Endpoint"
    - "è¨­ç½®ç·©å­˜è¦å‰‡ (éœæ…‹è³‡æº 7 å¤© TTL)"
    - "å•Ÿç”¨å£“ç¸® (Gzip, Brotli)"
    - "é…ç½®è‡ªå®šç¾©åŸŸå"
```

**CDN é…ç½®è…³æœ¬**:

```bash
#!/bin/bash
# scripts/setup-cdn.sh

RESOURCE_GROUP="aiagent-prod-rg"
CDN_PROFILE="aiagent-cdn-standard"
CDN_ENDPOINT="aiagent-static"
STORAGE_ACCOUNT="aiagentstorageprod"

echo "=== é…ç½® Azure CDN ==="

# å‰µå»º CDN Profile
az cdn profile create \
  --resource-group "$RESOURCE_GROUP" \
  --name "$CDN_PROFILE" \
  --sku Standard_Microsoft

# å‰µå»º CDN Endpoint
az cdn endpoint create \
  --resource-group "$RESOURCE_GROUP" \
  --profile-name "$CDN_PROFILE" \
  --name "$CDN_ENDPOINT" \
  --origin "$STORAGE_ACCOUNT.blob.core.windows.net" \
  --origin-host-header "$STORAGE_ACCOUNT.blob.core.windows.net" \
  --enable-compression true \
  --content-types-to-compress \
    "text/plain" \
    "text/html" \
    "text/css" \
    "application/javascript" \
    "application/json"

# é…ç½®ç·©å­˜è¦å‰‡
az cdn endpoint rule add \
  --resource-group "$RESOURCE_GROUP" \
  --profile-name "$CDN_PROFILE" \
  --endpoint-name "$CDN_ENDPOINT" \
  --order 1 \
  --rule-name "CacheStaticAssets" \
  --match-variable UrlFileExtension \
  --operator Equal \
  --match-values "jpg" "png" "gif" "css" "js" \
  --action-name CacheExpiration \
  --cache-behavior Override \
  --cache-duration "7.00:00:00"  # 7 å¤©

echo "âœ… CDN é…ç½®å®Œæˆ"
echo "CDN Endpoint: https://$CDN_ENDPOINT.azureedge.net"
```

### è·¨å€åŸŸæµé‡å„ªåŒ–

```yaml
cross_region_optimization:
  problem: "è·¨å€åŸŸæ•¸æ“šå‚³è¼¸æˆæœ¬é«˜ ($0.02/GB)"

  solutions:
    - strategy: "ä½¿ç”¨ Azure Front Door æ™ºèƒ½è·¯ç”±"
      description: "è‡ªå‹•å°‡ç”¨æˆ¶è·¯ç”±åˆ°æœ€è¿‘çš„å€åŸŸ"
      cost_impact: "æ¸›å°‘ 60% è·¨å€åŸŸæµé‡"
      savings: "$200/æœˆ"

    - strategy: "å€åŸŸæ•¸æ“šæœ¬åœ°åŒ–"
      description: "åœ¨ç”¨æˆ¶æ‰€åœ¨å€åŸŸå­˜å„²å¸¸ç”¨æ•¸æ“š"
      implementation:
        - "ä½¿ç”¨ Geo-Replication è¤‡è£½ç†±æ•¸æ“š"
        - "å†·æ•¸æ“šé›†ä¸­å­˜å„²åœ¨ä¸»å€åŸŸ"
      savings: "$150/æœˆ"

    - strategy: "å£“ç¸®è·¨å€åŸŸå‚³è¼¸"
      description: "å£“ç¸®æ•¸æ“šå†å‚³è¼¸"
      compression_ratio: "70%"
      savings: "$100/æœˆ"
```

---

## AI æœå‹™æˆæœ¬æ§åˆ¶

### Token å„ªåŒ–ç­–ç•¥

```python
# src/AI/TokenOptimizer.py
"""
Azure OpenAI Token å„ªåŒ–å™¨
"""
import tiktoken
from typing import List, Dict
import hashlib
import redis

class TokenOptimizer:
    """Token ä½¿ç”¨å„ªåŒ–å™¨"""

    def __init__(self, redis_client: redis.Redis):
        self.redis_client = redis_client
        self.encoder = tiktoken.encoding_for_model("gpt-4")

    def count_tokens(self, text: str) -> int:
        """è¨ˆç®—æ–‡æœ¬çš„ token æ•¸é‡"""
        return len(self.encoder.encode(text))

    def optimize_prompt(self, prompt: str, max_tokens: int = 2000) -> str:
        """å„ªåŒ– prompt é•·åº¦"""
        current_tokens = self.count_tokens(prompt)

        if current_tokens <= max_tokens:
            return prompt

        # ç­–ç•¥ 1: ç§»é™¤ä¸å¿…è¦çš„ç©ºç™½
        optimized = " ".join(prompt.split())

        # ç­–ç•¥ 2: æˆªæ–·åˆ°æœ€å¤§é•·åº¦ (ä¿ç•™å‰å¾Œæ–‡)
        if self.count_tokens(optimized) > max_tokens:
            # ç°¡åŒ–æˆªæ–·ç­–ç•¥
            words = optimized.split()
            target_words = int(len(words) * (max_tokens / current_tokens))
            optimized = " ".join(words[:target_words])

        return optimized

    def get_cached_response(self, prompt: str) -> str | None:
        """å¾ç·©å­˜ç²å–éŸ¿æ‡‰"""
        cache_key = f"openai:response:{hashlib.sha256(prompt.encode()).hexdigest()}"
        cached = self.redis_client.get(cache_key)

        if cached:
            print(f"âœ… Cache hit (ç¯€çœ API èª¿ç”¨)")
            return cached.decode('utf-8')

        return None

    def cache_response(self, prompt: str, response: str, ttl: int = 86400):
        """ç·©å­˜éŸ¿æ‡‰ (24 å°æ™‚ TTL)"""
        cache_key = f"openai:response:{hashlib.sha256(prompt.encode()).hexdigest()}"
        self.redis_client.setex(cache_key, ttl, response)

    def batch_requests(self, prompts: List[str]) -> List[str]:
        """æ‰¹è™•ç†è«‹æ±‚ (æ¸›å°‘ API èª¿ç”¨æ¬¡æ•¸)"""
        # åˆä½µç›¸ä¼¼çš„ prompt
        batched_prompts = []
        responses = []

        # æª¢æŸ¥ç·©å­˜
        for prompt in prompts:
            cached = self.get_cached_response(prompt)
            if cached:
                responses.append(cached)
            else:
                batched_prompts.append(prompt)

        # æ‰¹é‡èª¿ç”¨ API
        if batched_prompts:
            # å¯¦éš›èª¿ç”¨ Azure OpenAI API
            # api_responses = self.call_openai_batch(batched_prompts)
            # responses.extend(api_responses)
            pass

        return responses

    def estimate_cost(self, input_tokens: int, output_tokens: int, model: str = "gpt-4") -> float:
        """ä¼°ç®— API èª¿ç”¨æˆæœ¬"""
        pricing = {
            "gpt-4": {"input": 0.03, "output": 0.06},  # per 1K tokens
            "gpt-3.5-turbo": {"input": 0.0015, "output": 0.002}
        }

        input_cost = (input_tokens / 1000) * pricing[model]["input"]
        output_cost = (output_tokens / 1000) * pricing[model]["output"]

        return input_cost + output_cost

# ä½¿ç”¨ç¤ºä¾‹
redis_client = redis.Redis(host='localhost', port=6379, db=0)
optimizer = TokenOptimizer(redis_client)

prompt = "åˆ†æä»¥ä¸‹å·¥ä½œæµä¸¦ç”Ÿæˆå„ªåŒ–å»ºè­°..."
optimized_prompt = optimizer.optimize_prompt(prompt, max_tokens=1500)

print(f"åŸå§‹ tokens: {optimizer.count_tokens(prompt)}")
print(f"å„ªåŒ–å¾Œ tokens: {optimizer.count_tokens(optimized_prompt)}")

# ä¼°ç®—æˆæœ¬
cost = optimizer.estimate_cost(input_tokens=1500, output_tokens=800, model="gpt-4")
print(f"ä¼°ç®—æˆæœ¬: ${cost:.4f}")
```

### æ¨¡å‹é¸æ“‡ç­–ç•¥

```yaml
model_selection_strategy:
  decision_tree:
    - task: "ç°¡å–®åˆ†é¡ä»»å‹™"
      recommended_model: "GPT-3.5 Turbo"
      cost_per_1k_tokens: "$0.002 (output)"
      use_cases:
        - "æ–‡æœ¬åˆ†é¡"
        - "ç°¡å–®å•ç­”"
        - "é—œéµå­—æå–"

    - task: "è¤‡é›œæ¨ç†ä»»å‹™"
      recommended_model: "GPT-4"
      cost_per_1k_tokens: "$0.06 (output)"
      use_cases:
        - "ä»£ç¢¼ç”Ÿæˆ"
        - "è¤‡é›œå·¥ä½œæµè¦åŠƒ"
        - "å¤šæ­¥é©Ÿæ¨ç†"

    - task: "æ‰¹é‡è™•ç†"
      recommended_model: "GPT-3.5 Turbo + æ‰¹è™•ç†"
      optimization: "åˆä½µå¤šå€‹è«‹æ±‚åˆ°å–®å€‹ API èª¿ç”¨"
      savings: "60% API èª¿ç”¨æ¬¡æ•¸"

  cost_optimization:
    - "ä½¿ç”¨ GPT-3.5 Turbo è™•ç† 70% ç°¡å–®ä»»å‹™"
    - "åƒ…å°è¤‡é›œä»»å‹™ä½¿ç”¨ GPT-4"
    - "å¯¦æ–½çµæœç·©å­˜ (Redis, 24 å°æ™‚ TTL)"
    - "Token å„ªåŒ– (ç§»é™¤å†—é¤˜å…§å®¹)"

  estimated_savings: "$6,000/æœˆ (å¾ $12,390 é™è‡³ $6,390)"
```

---

## é–‹ç™¼æ¸¬è©¦ç’°å¢ƒå„ªåŒ–

### ç’°å¢ƒè‡ªå‹•åŒ–ç®¡ç†

```python
# scripts/dev_environment_scheduler.py
"""
é–‹ç™¼æ¸¬è©¦ç’°å¢ƒè‡ªå‹•åŒ–èª¿åº¦å™¨ - å·¥ä½œæ™‚é–“è‡ªå‹•å•Ÿå‹•,éå·¥ä½œæ™‚é–“è‡ªå‹•é—œé–‰
"""
from datetime import datetime, time
import os
from azure.identity import DefaultAzureCredential
from azure.mgmt.containerservice import ContainerServiceClient

class DevEnvironmentScheduler:
    """é–‹ç™¼ç’°å¢ƒèª¿åº¦å™¨"""

    def __init__(self):
        self.subscription_id = os.getenv("AZURE_SUBSCRIPTION_ID")
        credential = DefaultAzureCredential()
        self.client = ContainerServiceClient(credential, self.subscription_id)

    def should_be_running(self) -> bool:
        """åˆ¤æ–·ç•¶å‰æ˜¯å¦æ‡‰è©²é‹è¡Œ"""
        now = datetime.now()
        current_time = now.time()
        is_weekday = now.weekday() < 5

        # å·¥ä½œæ—¥ 8:00-20:00 é‹è¡Œ
        if is_weekday and time(8, 0) <= current_time < time(20, 0):
            return True

        return False

    def start_dev_cluster(self):
        """å•Ÿå‹•é–‹ç™¼é›†ç¾¤"""
        print("ğŸš€ å•Ÿå‹•é–‹ç™¼é›†ç¾¤...")

        self.client.managed_clusters.begin_start(
            resource_group_name="aiagent-dev-rg",
            resource_name="aiagent-aks-dev"
        ).result()

        print("âœ… é–‹ç™¼é›†ç¾¤å·²å•Ÿå‹•")

    def stop_dev_cluster(self):
        """åœæ­¢é–‹ç™¼é›†ç¾¤"""
        print("ğŸ›‘ åœæ­¢é–‹ç™¼é›†ç¾¤...")

        self.client.managed_clusters.begin_stop(
            resource_group_name="aiagent-dev-rg",
            resource_name="aiagent-aks-dev"
        ).result()

        print("âœ… é–‹ç™¼é›†ç¾¤å·²åœæ­¢")

    def run_scheduled_check(self):
        """åŸ·è¡Œå®šæ™‚æª¢æŸ¥"""
        should_run = self.should_be_running()

        # ç²å–ç•¶å‰é›†ç¾¤ç‹€æ…‹
        cluster = self.client.managed_clusters.get(
            resource_group_name="aiagent-dev-rg",
            resource_name="aiagent-aks-dev"
        )

        current_state = cluster.power_state.code  # "Running" or "Stopped"

        if should_run and current_state == "Stopped":
            self.start_dev_cluster()
        elif not should_run and current_state == "Running":
            self.stop_dev_cluster()
        else:
            print(f"âœ… é›†ç¾¤ç‹€æ…‹æ­£ç¢º (æ‡‰è©²: {'é‹è¡Œ' if should_run else 'åœæ­¢'}, å¯¦éš›: {current_state})")

# ä½¿ç”¨ç¤ºä¾‹ (é€šé Azure Function æˆ– GitHub Actions å®šæ™‚åŸ·è¡Œ)
if __name__ == "__main__":
    scheduler = DevEnvironmentScheduler()
    scheduler.run_scheduled_check()
```

**éƒ¨ç½²ç‚º Azure Function (Timer Trigger)**:

```csharp
// functions/DevEnvironmentScheduler.cs
using Microsoft.Azure.Functions.Worker;
using Microsoft.Extensions.Logging;

public class DevEnvironmentScheduler
{
    private readonly ILogger _logger;

    public DevEnvironmentScheduler(ILoggerFactory loggerFactory)
    {
        _logger = loggerFactory.CreateLogger<DevEnvironmentScheduler>();
    }

    [Function("DevEnvironmentScheduler")]
    public async Task Run(
        [TimerTrigger("0 */30 * * * *")] TimerInfo timer)  // æ¯ 30 åˆ†é˜åŸ·è¡Œä¸€æ¬¡
    {
        _logger.LogInformation($"DevEnvironmentScheduler åŸ·è¡Œæ–¼: {DateTime.UtcNow}");

        // åŸ·è¡Œ Python è…³æœ¬æˆ–ç›´æ¥èª¿ç”¨ Azure SDK
        var process = new System.Diagnostics.Process
        {
            StartInfo = new System.Diagnostics.ProcessStartInfo
            {
                FileName = "python3",
                Arguments = "scripts/dev_environment_scheduler.py",
                RedirectStandardOutput = true,
                UseShellExecute = false
            }
        };

        process.Start();
        await process.WaitForExitAsync();

        var output = await process.StandardOutput.ReadToEndAsync();
        _logger.LogInformation(output);
    }
}
```

**æˆæœ¬ç¯€çœä¼°ç®—**:
- é–‹ç™¼é›†ç¾¤æˆæœ¬: $800/æœˆ (24/7 é‹è¡Œ)
- å„ªåŒ–å¾Œæˆæœ¬: $300/æœˆ (åƒ…å·¥ä½œæ™‚é–“é‹è¡Œ, ç´„ 50 å°æ™‚/é€±)
- **æœˆåº¦ç¯€çœ**: $500/æœˆ
- **å¹´åº¦ç¯€çœ**: $6,000/å¹´

---

## FinOps æœ€ä½³å¯¦è¸

### æˆæœ¬è²¬ä»»åˆ†é… (Cost Allocation)

```yaml
cost_allocation_strategy:
  tagging_policy:
    required_tags:
      - key: "Environment"
        values: ["Production", "Staging", "Development"]
      - key: "Team"
        values: ["Platform", "DataScience", "Frontend"]
      - key: "CostCenter"
        values: ["Engineering", "Operations", "R&D"]
      - key: "Project"
        values: ["AIAgent", "Analytics", "Infrastructure"]

  implementation:
    - "æ‰€æœ‰è³‡æºå¿…é ˆåŒ…å« required_tags"
    - "é€šé Azure Policy å¼·åˆ¶æ¨™è¨˜åˆè¦æ€§"
    - "æ¯æœˆç”Ÿæˆæˆæœ¬åˆ†é…å ±å‘Š"
    - "åœ˜éšŠ Chargeback æ¨¡å‹"

  cost_allocation_example:
    total_monthly_cost: "$13,203"
    breakdown:
      - team: "Platform"
        cost: "$8,500"
        percentage: "64%"
        resources: ["AKS", "PostgreSQL", "Redis"]
      - team: "DataScience"
        cost: "$3,700"
        percentage: "28%"
        resources: ["Azure OpenAI", "Cognitive Services"]
      - team: "Operations"
        cost: "$1,003"
        percentage: "8%"
        resources: ["Monitoring", "Security Center"]
```

### æˆæœ¬ç•°å¸¸æª¢æ¸¬

```python
# scripts/cost_anomaly_detector.py
"""
æˆæœ¬ç•°å¸¸æª¢æ¸¬å™¨ - ä½¿ç”¨çµ±è¨ˆæ–¹æ³•æª¢æ¸¬ç•°å¸¸æˆæœ¬
"""
import numpy as np
from datetime import datetime, timedelta
from typing import List, Tuple
from dataclasses import dataclass

@dataclass
class CostAnomaly:
    """æˆæœ¬ç•°å¸¸"""
    date: datetime
    actual_cost: float
    expected_cost: float
    deviation_percent: float
    severity: str  # "Low", "Medium", "High"
    resource_group: str

class CostAnomalyDetector:
    """æˆæœ¬ç•°å¸¸æª¢æ¸¬å™¨"""

    def __init__(self, threshold_std: float = 2.0):
        self.threshold_std = threshold_std  # æ¨™æº–å·®é–¾å€¼

    def detect_anomalies(
        self,
        historical_costs: List[Tuple[datetime, float]],
        resource_group: str = "All"
    ) -> List[CostAnomaly]:
        """æª¢æ¸¬æˆæœ¬ç•°å¸¸"""

        if len(historical_costs) < 7:
            print("âš ï¸  æ­·å²æ•¸æ“šä¸è¶³ (éœ€è¦è‡³å°‘ 7 å¤©)")
            return []

        # æå–æˆæœ¬æ•¸æ“š
        dates = [date for date, _ in historical_costs]
        costs = np.array([cost for _, cost in historical_costs])

        # è¨ˆç®—ç§»å‹•å¹³å‡å’Œæ¨™æº–å·®
        window_size = 7  # 7 å¤©ç§»å‹•çª—å£
        anomalies = []

        for i in range(window_size, len(costs)):
            window = costs[i - window_size:i]
            mean = np.mean(window)
            std = np.std(window)

            current_cost = costs[i]
            deviation = abs(current_cost - mean)
            deviation_std = deviation / std if std > 0 else 0
            deviation_percent = ((current_cost - mean) / mean) * 100

            # ç•°å¸¸æª¢æ¸¬ (è¶…é threshold_std å€‹æ¨™æº–å·®)
            if deviation_std > self.threshold_std:
                severity = self._calculate_severity(deviation_std)

                anomaly = CostAnomaly(
                    date=dates[i],
                    actual_cost=current_cost,
                    expected_cost=mean,
                    deviation_percent=deviation_percent,
                    severity=severity,
                    resource_group=resource_group
                )
                anomalies.append(anomaly)

        return anomalies

    def _calculate_severity(self, deviation_std: float) -> str:
        """è¨ˆç®—ç•°å¸¸åš´é‡ç¨‹åº¦"""
        if deviation_std > 4.0:
            return "High"
        elif deviation_std > 3.0:
            return "Medium"
        else:
            return "Low"

    def generate_alert(self, anomaly: CostAnomaly) -> dict:
        """ç”Ÿæˆå‘Šè­¦"""
        return {
            "title": f"æˆæœ¬ç•°å¸¸æª¢æ¸¬: {anomaly.resource_group}",
            "severity": anomaly.severity,
            "message": f"""
æª¢æ¸¬åˆ°æˆæœ¬ç•°å¸¸:

æ—¥æœŸ: {anomaly.date.strftime('%Y-%m-%d')}
è³‡æºçµ„: {anomaly.resource_group}
å¯¦éš›æˆæœ¬: ${anomaly.actual_cost:,.2f}
é æœŸæˆæœ¬: ${anomaly.expected_cost:,.2f}
åå·®: {anomaly.deviation_percent:+.1f}%
åš´é‡ç¨‹åº¦: {anomaly.severity}

å»ºè­°ç«‹å³æª¢æŸ¥è³‡æºä½¿ç”¨æƒ…æ³å’Œé…ç½®ã€‚
            """,
            "timestamp": datetime.utcnow().isoformat()
        }

# ä½¿ç”¨ç¤ºä¾‹
detector = CostAnomalyDetector(threshold_std=2.0)

# æ¨¡æ“¬æ­·å²æˆæœ¬æ•¸æ“š (æ—¥æœŸ, æˆæœ¬)
historical_costs = [
    (datetime(2025, 10, 1), 420.50),
    (datetime(2025, 10, 2), 435.20),
    (datetime(2025, 10, 3), 428.75),
    (datetime(2025, 10, 4), 441.30),
    (datetime(2025, 10, 5), 438.90),
    (datetime(2025, 10, 6), 445.60),
    (datetime(2025, 10, 7), 432.10),
    (datetime(2025, 10, 8), 850.00),  # ç•°å¸¸é«˜æˆæœ¬
]

anomalies = detector.detect_anomalies(historical_costs, resource_group="aiagent-prod-rg")

for anomaly in anomalies:
    alert = detector.generate_alert(anomaly)
    print(alert["message"])
```

---

## æª¢æŸ¥æ¸…å–®

### æˆæœ¬å„ªåŒ–æª¢æŸ¥æ¸…å–®

- [ ] **Right-Sizing (è³‡æºé©é…)**
  - [ ] æ¯å­£åº¦å¯©æŸ¥è³‡æºä½¿ç”¨ç‡å ±å‘Š
  - [ ] CPU/Memory ä½¿ç”¨ç‡åœ¨ 70-85% ç¯„åœ
  - [ ] è­˜åˆ¥ä¸¦èª¿æ•´éåº¦é…ç½®è³‡æº

- [ ] **é ç•™å¯¦ä¾‹ (Reserved Instances)**
  - [ ] é ç•™å¯¦ä¾‹è¦†è“‹ç‡ â‰¥ 70%
  - [ ] åŸºç¤å®¹é‡ä½¿ç”¨ 3 å¹´é ç•™
  - [ ] ç©©å®šå¢é•·ä½¿ç”¨ 1 å¹´é ç•™

- [ ] **è‡ªå‹•æ“´å±• (Auto-Scaling)**
  - [ ] HPA å·²é…ç½® (CPU 70%, Memory 80%)
  - [ ] Cluster Autoscaler å·²å•Ÿç”¨
  - [ ] æ™‚é–“èª¿åº¦ç¸®å®¹å·²å¯¦æ–½

- [ ] **å­˜å„²å„ªåŒ–**
  - [ ] Blob Storage ç”Ÿå‘½é€±æœŸç®¡ç†å·²é…ç½®
  - [ ] PostgreSQL å‚™ä»½å£“ç¸®ç‡ â‰¥ 70%
  - [ ] èˆŠå‚™ä»½è‡ªå‹•è½‰ç‚º Archive Tier

- [ ] **ç¶²çµ¡å„ªåŒ–**
  - [ ] Azure CDN å·²é…ç½® (éœæ…‹è³‡æº)
  - [ ] è·¨å€åŸŸæµé‡æœ€å°åŒ–
  - [ ] å£“ç¸®å·²å•Ÿç”¨ (Gzip, Brotli)

- [ ] **AI æœå‹™å„ªåŒ–**
  - [ ] Token å„ªåŒ–å·²å¯¦æ–½
  - [ ] çµæœç·©å­˜å·²å•Ÿç”¨ (Redis)
  - [ ] 70% ç°¡å–®ä»»å‹™ä½¿ç”¨ GPT-3.5 Turbo

- [ ] **é–‹ç™¼æ¸¬è©¦ç’°å¢ƒ**
  - [ ] é–‹ç™¼é›†ç¾¤è‡ªå‹•åŒ–èª¿åº¦å·²é…ç½®
  - [ ] åƒ…å·¥ä½œæ™‚é–“é‹è¡Œ
  - [ ] æ¸¬è©¦ç’°å¢ƒä½¿ç”¨ä½æˆæœ¬é…ç½®

- [ ] **FinOps å¯¦è¸**
  - [ ] æˆæœ¬æ¨™è¨˜æ”¿ç­–å·²å¯¦æ–½
  - [ ] æˆæœ¬ç•°å¸¸æª¢æ¸¬å·²å•Ÿç”¨
  - [ ] æ¯æœˆæˆæœ¬å¯©æŸ¥æœƒè­°

---

## ç¸½çµ

æœ¬æ–‡æª”æä¾›äº†å®Œæ•´çš„æˆæœ¬å„ªåŒ–ç­–ç•¥,æ¶µè“‹:

âœ… **Right-Sizing ç­–ç•¥** (è³‡æºä½¿ç”¨ç‡ç›£æ§, è‡ªå‹•èª¿æ•´æµç¨‹)
âœ… **é ç•™å¯¦ä¾‹å„ªåŒ–** (è¦†è“‹ç‡åˆ†æ, è³¼è²·ç­–ç•¥, 35-54% ç¯€çœ)
âœ… **è‡ªå‹•æ“´å±•ç­–ç•¥** (HPA å„ªåŒ–, Cluster Autoscaler, æ™‚é–“èª¿åº¦)
âœ… **å­˜å„²å„ªåŒ–** (ç”Ÿå‘½é€±æœŸç®¡ç†, å‚™ä»½å£“ç¸®, 70% ç©ºé–“ç¯€çœ)
âœ… **ç¶²çµ¡æˆæœ¬å„ªåŒ–** (CDN ç­–ç•¥, è·¨å€åŸŸå„ªåŒ–, $75/æœˆç¯€çœ)
âœ… **AI æœå‹™æˆæœ¬æ§åˆ¶** (Token å„ªåŒ–, æ¨¡å‹é¸æ“‡, $6,000/æœˆç¯€çœ)
âœ… **é–‹ç™¼æ¸¬è©¦ç’°å¢ƒå„ªåŒ–** (è‡ªå‹•åŒ–èª¿åº¦, $500/æœˆç¯€çœ)
âœ… **FinOps æœ€ä½³å¯¦è¸** (æˆæœ¬åˆ†é…, ç•°å¸¸æª¢æ¸¬, æŒçºŒå„ªåŒ–)

**ç¸½é«”æˆæœ¬å„ªåŒ–æ½›åŠ›**:
- çŸ­æœŸ (3å€‹æœˆ): ç¯€çœ 15-20% â†’ **$2,000-$2,600/æœˆ**
- ä¸­æœŸ (6å€‹æœˆ): ç¯€çœ 25-35% â†’ **$3,300-$4,600/æœˆ**
- é•·æœŸ (12å€‹æœˆ): ç¯€çœ 40-50% â†’ **$5,300-$6,600/æœˆ**

é€éæœ¬æŒ‡å—,é‹ç¶­åœ˜éšŠå¯ä»¥ç³»çµ±åŒ–åœ°é™ä½é›²ç«¯æˆæœ¬,åŒæ™‚ä¿æŒæœå‹™å“è³ªå’Œå¯ç”¨æ€§ã€‚
