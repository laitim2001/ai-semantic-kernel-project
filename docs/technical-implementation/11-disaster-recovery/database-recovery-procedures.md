# æ•¸æ“šåº«æ¢å¾©æµç¨‹æŒ‡å—

## æ¦‚è¿°

### æ–‡æª”ç›®çš„
æœ¬æ–‡æª”æä¾› AI Agent å·¥ä½œæµå¹³å°çš„å®Œæ•´æ•¸æ“šåº«æ¢å¾©æµç¨‹,æ¶µè“‹:
- **Point-in-Time Recovery (PITR)** (æ™‚é–“é»æ¢å¾©,ç²¾ç¢ºåˆ°ä»»æ„ç§’)
- **Transaction Log æ¢å¾©** (WAL æ—¥èªŒå›æ”¾)
- **å…¨é‡å‚™ä»½æ¢å¾©** (å¾ pg_dump/pg_dumpall æ¢å¾©)
- **éƒ¨åˆ†æ•¸æ“šæ¢å¾©** (å–®è¡¨ã€å–®åº«æ¢å¾©)
- **è·¨å€åŸŸæ¢å¾©** (å¾ DR ç«™é»æ¢å¾©åˆ°ä¸»ç«™é»)
- **æ•¸æ“šä¸€è‡´æ€§é©—è­‰** (æ¢å¾©å¾Œå®Œæ•´æ€§æª¢æŸ¥)
- **æ¢å¾©æ€§èƒ½å„ªåŒ–** (ä¸¦è¡Œæ¢å¾©ã€ç´¢å¼•é‡å»ºç­–ç•¥)

### æ¢å¾©å ´æ™¯åˆ†é¡

| å ´æ™¯é¡å‹ | æ•¸æ“šä¸Ÿå¤±ç¯„åœ | æ¢å¾©æ–¹æ³• | é è¨ˆ RTO | é¢¨éšªç´šåˆ¥ |
|---------|-------------|---------|----------|----------|
| **èª¤åˆªé™¤å–®æ¢è¨˜éŒ„** | å–®è¡Œæ•¸æ“š | PITR æ¢å¾© â†’ æå–å–®è¡Œ | 30 åˆ†é˜ | ğŸŸ¢ ä½ |
| **èª¤åˆªé™¤æ•´å¼µè¡¨** | å–®è¡¨æ•¸æ“š | PITR æ¢å¾© â†’ æå–å–®è¡¨ | 1 å°æ™‚ | ğŸŸ¡ ä¸­ |
| **èª¤åˆªé™¤æ•¸æ“šåº«** | æ•´å€‹æ•¸æ“šåº« | PITR æ¢å¾©æˆ–å…¨é‡å‚™ä»½ | 2 å°æ™‚ | ğŸ”´ é«˜ |
| **æ•¸æ“šåº«æå£** | éƒ¨åˆ†æˆ–å…¨éƒ¨æ•¸æ“š | WAL æ¢å¾© + å®Œæ•´æ€§æª¢æŸ¥ | 2-4 å°æ™‚ | ğŸ”´ é«˜ |
| **å€åŸŸæ€§ç½é›£** | æ•´å€‹ Azure Region | è·¨å€åŸŸ Geo-Restore | 4-8 å°æ™‚ | ğŸ”´ Critical |
| **æƒ¡æ„æ”»æ“Š (Ransomware)** | å…¨éƒ¨æ•¸æ“šåŠ å¯† | æœ€è¿‘æœªå—å½±éŸ¿çš„å‚™ä»½ | 2-6 å°æ™‚ | ğŸ”´ Critical |

---

## Point-in-Time Recovery (PITR)

### PITR åŸç†

**å·¥ä½œæ©Ÿåˆ¶**:
```
å…¨é‡å‚™ä»½ (Base Backup) + WAL æ—¥èªŒ (Incremental Changes) = ä»»æ„æ™‚é–“é»ç‹€æ…‹
```

**ç¤ºä¾‹æ™‚é–“ç·š**:
```
2025-01-15 00:00 â”€â”€â”€ å…¨é‡å‚™ä»½ (Checkpoint)
                 â”‚
2025-01-15 06:00 â”€â”€â”€ WAL æ–‡ä»¶ 000001
2025-01-15 12:00 â”€â”€â”€ WAL æ–‡ä»¶ 000002
2025-01-15 14:30 â”€â”€â”€ âŒ èª¤åˆªé™¤äº‹ä»¶ç™¼ç”Ÿ
2025-01-15 18:00 â”€â”€â”€ WAL æ–‡ä»¶ 000003
                 â”‚
æ¢å¾©ç›®æ¨™: 2025-01-15 14:29:59 (èª¤åˆªé™¤å‰ 1 ç§’)
```

### å‰ç½®æ¢ä»¶æª¢æŸ¥

**1. ç¢ºèª WAL æ­¸æª”å·²å•Ÿç”¨**:

```sql
-- é€£æ¥åˆ°æ•¸æ“šåº«ä¸¦æª¢æŸ¥ WAL é…ç½®
SHOW wal_level;        -- æ‡‰ç‚º 'replica' æˆ– 'logical'
SHOW archive_mode;     -- æ‡‰ç‚º 'on'
SHOW archive_command;  -- æ‡‰é…ç½®æ­£ç¢ºçš„æ­¸æª”è…³æœ¬

-- æª¢æŸ¥ç•¶å‰ WAL æ–‡ä»¶ä½ç½®
SELECT pg_current_wal_lsn();

-- æª¢æŸ¥æœ€å¾Œä¸€æ¬¡æˆåŠŸæ­¸æª”çš„ WAL æ–‡ä»¶
SELECT archived_count, last_archived_wal, last_archived_time
FROM pg_stat_archiver;
```

**2. ç¢ºèªå‚™ä»½æ–‡ä»¶å¯ç”¨æ€§**:

```bash
# åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„å…¨é‡å‚™ä»½
az storage blob list \
  --account-name aiagentbackup \
  --container-name postgresql-full \
  --prefix daily/ \
  --query "[].{Name:name, Size:properties.contentLength, LastModified:properties.lastModified}" \
  --output table

# åˆ—å‡ºæ‰€æœ‰ WAL æ­¸æª”æ–‡ä»¶
az storage blob list \
  --account-name aiagentbackup \
  --container-name postgresql-wal-archive \
  --prefix wal/ \
  --query "[].{Name:name, LastModified:properties.lastModified}" \
  --output table
```

### PITR æ¢å¾©æµç¨‹

#### Step 1: åœæ­¢ç”Ÿç”¢æ•¸æ“šåº« (å¦‚æœéœ€è¦åŸåœ°æ¢å¾©)

```bash
# å¦‚æœæ¢å¾©åˆ°ç¨ç«‹ç’°å¢ƒ,è·³éæ­¤æ­¥é©Ÿ
# å¦‚æœéœ€è¦åŸåœ°æ¢å¾©,å¿…é ˆå…ˆåœæ­¢æ•¸æ“šåº«

# Kubernetes æ–¹å¼
kubectl scale deployment postgresql-primary --replicas=0 -n aiagent-prod

# ç›´æ¥åœæ­¢ PostgreSQL
sudo systemctl stop postgresql
```

#### Step 2: ä¸‹è¼‰åŸºç¤å‚™ä»½

```bash
#!/bin/bash
# scripts/pitr-restore-step1-download-backup.sh

set -e

RESTORE_TARGET_TIME="2025-01-15 14:29:59"
BACKUP_ACCOUNT="aiagentbackup"
RESTORE_DIR="/var/lib/postgresql/restore"

echo "ğŸ” Finding base backup before $RESTORE_TARGET_TIME..."

# æ‰¾åˆ°ç›®æ¨™æ™‚é–“å‰çš„æœ€æ–°å…¨é‡å‚™ä»½
LATEST_BACKUP=$(az storage blob list \
  --account-name "$BACKUP_ACCOUNT" \
  --container-name "postgresql-full" \
  --prefix "daily/" \
  --query "sort_by([?properties.lastModified <= '$RESTORE_TARGET_TIME'].{name:name, time:properties.lastModified}, &time)[-1].name" \
  --output tsv)

if [ -z "$LATEST_BACKUP" ]; then
  echo "âŒ No suitable backup found before $RESTORE_TARGET_TIME"
  exit 1
fi

echo "âœ… Found backup: $LATEST_BACKUP"

# ä¸‹è¼‰ä¸¦è§£å£“ç¸®å‚™ä»½
echo "ğŸ“¥ Downloading backup..."
az storage blob download \
  --account-name "$BACKUP_ACCOUNT" \
  --container-name "postgresql-full" \
  --name "$LATEST_BACKUP" \
  --file "/tmp/backup.dump.gz" \
  --auth-mode login

echo "ğŸ“¦ Extracting backup..."
gunzip -c /tmp/backup.dump.gz > /tmp/backup.dump

echo "âœ… Backup downloaded and extracted"
```

#### Step 3: æ¢å¾©åŸºç¤å‚™ä»½

```bash
#!/bin/bash
# scripts/pitr-restore-step2-restore-base.sh

set -e

RESTORE_DIR="/var/lib/postgresql/restore"
PGDATA="$RESTORE_DIR/data"

# æ¸…ç†ä¸¦å‰µå»ºæ¢å¾©ç›®éŒ„
rm -rf "$RESTORE_DIR"
mkdir -p "$PGDATA"
chown -R postgres:postgres "$RESTORE_DIR"

# æ¢å¾©åŸºç¤å‚™ä»½
echo "ğŸ”„ Restoring base backup..."
su - postgres -c "psql -f /tmp/backup.dump postgres"

echo "âœ… Base backup restored to $PGDATA"
```

#### Step 4: é…ç½® recovery.conf (PostgreSQL 12+)

```bash
#!/bin/bash
# scripts/pitr-restore-step3-configure-recovery.sh

RESTORE_TARGET_TIME="2025-01-15 14:29:59"
PGDATA="/var/lib/postgresql/restore/data"

# PostgreSQL 12+ ä½¿ç”¨ postgresql.conf å’Œ standby.signal
cat >> "$PGDATA/postgresql.conf" <<EOF

# Point-in-Time Recovery Configuration
restore_command = 'az storage blob download --account-name aiagentbackup --container-name postgresql-wal-archive --name wal/%f --file %p --auth-mode login'
recovery_target_time = '$RESTORE_TARGET_TIME'
recovery_target_action = 'promote'

# å¯é¸: æš«åœåœ¨ç›®æ¨™æ™‚é–“é»,ä»¥ä¾¿æ‰‹å‹•æª¢æŸ¥
# recovery_target_action = 'pause'
EOF

# å‰µå»º recovery.signal æ–‡ä»¶ (è§¸ç™¼æ¢å¾©æ¨¡å¼)
touch "$PGDATA/recovery.signal"
chown postgres:postgres "$PGDATA/recovery.signal"

echo "âœ… Recovery configuration completed"
```

#### Step 5: å•Ÿå‹•æ•¸æ“šåº«ä¸¦ç­‰å¾…æ¢å¾©å®Œæˆ

```bash
#!/bin/bash
# scripts/pitr-restore-step4-start-recovery.sh

set -e

PGDATA="/var/lib/postgresql/restore/data"

# å•Ÿå‹• PostgreSQL (æ¢å¾©æ¨¡å¼)
echo "ğŸš€ Starting PostgreSQL in recovery mode..."
su - postgres -c "pg_ctl start -D $PGDATA"

# ç›£æ§æ¢å¾©é€²åº¦
echo "â³ Waiting for recovery to complete..."
while true; do
  # æª¢æŸ¥æ˜¯å¦é‚„åœ¨æ¢å¾©æ¨¡å¼
  RECOVERY_STATUS=$(su - postgres -c "psql -t -c 'SELECT pg_is_in_recovery();'")

  if [ "$RECOVERY_STATUS" = "f" ]; then
    echo "âœ… Recovery completed successfully!"
    break
  fi

  # é¡¯ç¤ºæ¢å¾©é€²åº¦
  WAL_STATUS=$(su - postgres -c "psql -t -c 'SELECT last_replay_lsn, replay_lag FROM pg_stat_replication;'")
  echo "ğŸ“Š Recovery progress: $WAL_STATUS"

  sleep 10
done

# é¡¯ç¤ºæ¢å¾©åˆ°çš„æœ€çµ‚æ™‚é–“é»
RECOVERY_TIME=$(su - postgres -c "psql -t -c 'SELECT pg_last_xact_replay_timestamp();'")
echo "ğŸ¯ Database restored to: $RECOVERY_TIME"
```

#### Step 6: æ•¸æ“šé©—è­‰

```sql
-- é©—è­‰æ¢å¾©çš„æ•¸æ“šå®Œæ•´æ€§

-- 1. æª¢æŸ¥è¡¨è¨˜éŒ„æ•¸
SELECT
  schemaname,
  tablename,
  n_live_tup AS row_count
FROM pg_stat_user_tables
ORDER BY n_live_tup DESC
LIMIT 20;

-- 2. æª¢æŸ¥æœ€æ–°æ•¸æ“šçš„æ™‚é–“æˆ³
SELECT
  'workflows' AS table_name,
  MAX(updated_at) AS latest_record
FROM workflows
UNION ALL
SELECT
  'workflow_executions',
  MAX(completed_at)
FROM workflow_executions;

-- 3. é©—è­‰é—œéµæ¥­å‹™æ•¸æ“š
SELECT COUNT(*) AS total_workflows FROM workflows;
SELECT COUNT(*) AS total_users FROM users;
SELECT COUNT(*) AS active_executions FROM workflow_executions WHERE status = 'Running';

-- 4. æª¢æŸ¥æ•¸æ“šåº«å®Œæ•´æ€§
SELECT datname, datallowconn, datistemplate
FROM pg_database
WHERE datname = 'aiagent_prod';
```

---

## Transaction Log (WAL) æ¢å¾©

### é©ç”¨å ´æ™¯
- æ•¸æ“šåº«å¯¦ä¾‹å´©æ½°å¾Œçš„è‡ªå‹•æ¢å¾©
- WAL æ–‡ä»¶å®Œæ•´ä½†æ•¸æ“šæ–‡ä»¶æå£
- å¾ Streaming Replication æ•…éšœä¸­æ¢å¾©

### WAL æ¢å¾©æµç¨‹

#### 1. æª¢æŸ¥ WAL æ–‡ä»¶å®Œæ•´æ€§

```bash
# é©—è­‰ WAL æ–‡ä»¶å¯è®€æ€§
pg_waldump /var/lib/postgresql/data/pg_wal/000000010000000000000001

# æª¢æŸ¥ WAL æ–‡ä»¶æ™‚é–“ç·š
pg_controldata /var/lib/postgresql/data | grep -E "Latest checkpoint|REDO|WAL"
```

#### 2. é…ç½® WAL å›æ”¾

```bash
# recovery.conf (PostgreSQL 11 åŠä»¥ä¸‹)
restore_command = 'cp /var/lib/postgresql/wal_archive/%f %p'
recovery_target = 'immediate'  # å›æ”¾æ‰€æœ‰å¯ç”¨ WAL
recovery_target_action = 'promote'
```

#### 3. å•Ÿå‹•æ¢å¾©

```bash
# å•Ÿå‹• PostgreSQL,è‡ªå‹•å›æ”¾ WAL
pg_ctl start -D /var/lib/postgresql/data

# ç›£æ§æ¢å¾©æ—¥èªŒ
tail -f /var/lib/postgresql/data/log/postgresql-*.log | grep -E "recovery|redo"
```

---

## å…¨é‡å‚™ä»½æ¢å¾©

### å¾ pg_dump æ¢å¾©å–®å€‹æ•¸æ“šåº«

```bash
#!/bin/bash
# scripts/restore-from-dump.sh

set -e

BACKUP_FILE="/backups/aiagent_prod-2025-01-15.dump"
TARGET_DB="aiagent_prod_restored"

# 1. å‰µå»ºæ–°æ•¸æ“šåº«
psql -U postgres -c "CREATE DATABASE $TARGET_DB;"

# 2. æ¢å¾©æ•¸æ“š (Custom Format)
pg_restore \
  --dbname=$TARGET_DB \
  --username=postgres \
  --verbose \
  --jobs=4 \  # ä¸¦è¡Œæ¢å¾© (4 å€‹ä¸¦ç™¼é€£æ¥)
  --no-owner \
  --no-acl \
  $BACKUP_FILE

echo "âœ… Database restored to $TARGET_DB"
```

### å¾ pg_dumpall æ¢å¾©æ‰€æœ‰æ•¸æ“šåº«

```bash
#!/bin/bash
# scripts/restore-all-databases.sh

set -e

BACKUP_FILE="/backups/all-databases-2025-01-15.sql"

# æ¢å¾©æ‰€æœ‰æ•¸æ“šåº« (åŒ…å« roles, tablespaces)
psql -U postgres -f $BACKUP_FILE

echo "âœ… All databases restored"
```

---

## éƒ¨åˆ†æ•¸æ“šæ¢å¾©

### æ¢å¾©å–®å¼µè¡¨

```bash
#!/bin/bash
# scripts/restore-single-table.sh

BACKUP_FILE="/backups/aiagent_prod-2025-01-15.dump"
TABLE_NAME="workflows"
TARGET_DB="aiagent_prod"

# 1. æ¢å¾©åˆ°è‡¨æ™‚æ•¸æ“šåº«
psql -U postgres -c "CREATE DATABASE temp_restore;"
pg_restore --dbname=temp_restore --username=postgres $BACKUP_FILE

# 2. è¤‡è£½å–®è¡¨æ•¸æ“šåˆ°ç”Ÿç”¢åº«
psql -U postgres -d temp_restore -c "\COPY $TABLE_NAME TO '/tmp/$TABLE_NAME.csv' CSV HEADER;"
psql -U postgres -d $TARGET_DB -c "\COPY $TABLE_NAME FROM '/tmp/$TABLE_NAME.csv' CSV HEADER;"

# 3. æ¸…ç†è‡¨æ™‚æ•¸æ“šåº«
psql -U postgres -c "DROP DATABASE temp_restore;"

echo "âœ… Table $TABLE_NAME restored"
```

### æ¢å¾©ç‰¹å®šæ™‚é–“ç¯„åœçš„æ•¸æ“š

```sql
-- å¾å‚™ä»½åº«è¤‡è£½ç‰¹å®šæ™‚é–“ç¯„åœçš„æ•¸æ“š

-- 1. é€£æ¥åˆ°æ¢å¾©çš„è‡¨æ™‚æ•¸æ“šåº«
\c aiagent_prod_restored

-- 2. æå–èª¤åˆªé™¤çš„æ•¸æ“š
CREATE TEMP TABLE recovered_workflows AS
SELECT *
FROM workflows
WHERE created_at >= '2025-01-15 00:00:00'
  AND created_at < '2025-01-15 14:30:00'  -- èª¤åˆªé™¤å‰çš„æ™‚é–“é»
  AND id NOT IN (SELECT id FROM aiagent_prod.workflows);  -- æ’é™¤æœªåˆªé™¤çš„

-- 3. è¤‡è£½åˆ°ç”Ÿç”¢åº«
INSERT INTO aiagent_prod.workflows
SELECT * FROM recovered_workflows;

-- 4. é©—è­‰æ¢å¾©çš„è¡Œæ•¸
SELECT COUNT(*) AS recovered_rows FROM recovered_workflows;
```

---

## è·¨å€åŸŸæ¢å¾© (Geo-Restore)

### å¾ DR ç«™é»æ¢å¾©åˆ°ä¸»ç«™é»

**å ´æ™¯**: Primary Region (East US) å®Œå…¨ä¸å¯ç”¨,éœ€è¦å¾ DR Region (West US 2) æ¢å¾©

#### Step 1: æ¿€æ´» DR ç«™é»çš„åªè®€å‰¯æœ¬

```bash
# å°‡ DR ç«™é»çš„ Read-Only Replica æå‡ç‚º Primary
az postgres flexible-server replica stop-replication \
  --resource-group aiagent-dr-rg \
  --name aiagent-postgres-dr \
  --yes

# é©—è­‰å¯«å…¥èƒ½åŠ›
psql -h aiagent-postgres-dr.postgres.database.azure.com \
     -U dbadmin \
     -d aiagent_prod \
     -c "CREATE TABLE test_write (id int); DROP TABLE test_write;"
```

#### Step 2: æ›´æ–°æ‡‰ç”¨é…ç½®æŒ‡å‘ DR ç«™é»

```yaml
# k8s/overlays/dr/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: database-config
  namespace: aiagent-prod
data:
  DATABASE_HOST: "aiagent-postgres-dr.postgres.database.azure.com"
  DATABASE_PORT: "5432"
  DATABASE_NAME: "aiagent_prod"
```

```bash
# æ‡‰ç”¨æ–°é…ç½®
kubectl apply -k k8s/overlays/dr/

# é‡å•Ÿæ‡‰ç”¨ Pods
kubectl rollout restart deployment/api-deployment -n aiagent-prod
```

#### Step 3: æ¢å¾©ä¸»ç«™é»å¾Œçš„æ•¸æ“šåŒæ­¥

```bash
# ç•¶ä¸»ç«™é»æ¢å¾©å¾Œ,å°‡ DR ç«™é»çš„æ•¸æ“šåŒæ­¥å›ä¸»ç«™é»

# 1. åœ¨ä¸»ç«™é»å‰µå»ºæ–°æ•¸æ“šåº«
psql -h aiagent-postgres-primary.postgres.database.azure.com \
     -U dbadmin \
     -c "CREATE DATABASE aiagent_prod_new;"

# 2. å¾ DR ç«™é»å°å‡ºæ•¸æ“š
pg_dump \
  -h aiagent-postgres-dr.postgres.database.azure.com \
  -U dbadmin \
  -d aiagent_prod \
  --format=custom \
  --file=/tmp/dr-site-data.dump

# 3. å°å…¥åˆ°ä¸»ç«™é»
pg_restore \
  -h aiagent-postgres-primary.postgres.database.azure.com \
  -U dbadmin \
  -d aiagent_prod_new \
  --jobs=8 \
  /tmp/dr-site-data.dump

# 4. é©—è­‰æ•¸æ“šä¸€è‡´æ€§
psql -h aiagent-postgres-primary.postgres.database.azure.com \
     -U dbadmin \
     -d aiagent_prod_new \
     -c "SELECT COUNT(*) FROM workflows;"

# 5. åˆ‡æ›åˆ°æ–°æ•¸æ“šåº«
psql -h aiagent-postgres-primary.postgres.database.azure.com \
     -U dbadmin \
     -c "ALTER DATABASE aiagent_prod RENAME TO aiagent_prod_old;"
psql -h aiagent-postgres-primary.postgres.database.azure.com \
     -U dbadmin \
     -c "ALTER DATABASE aiagent_prod_new RENAME TO aiagent_prod;"
```

---

## æ•¸æ“šä¸€è‡´æ€§é©—è­‰

### è‡ªå‹•åŒ–å®Œæ•´æ€§æª¢æŸ¥è…³æœ¬

```csharp
// src/Infrastructure/Recovery/DatabaseIntegrityChecker.cs
using Npgsql;

public sealed class DatabaseIntegrityChecker
{
    private readonly NpgsqlConnection _connection;
    private readonly ILogger<DatabaseIntegrityChecker> _logger;

    public async Task<IntegrityCheckResult> PerformFullCheckAsync(
        CancellationToken cancellationToken = default)
    {
        var results = new List<CheckResult>();

        // 1. æª¢æŸ¥æ•¸æ“šåº«é€£æ¥
        results.Add(await CheckDatabaseConnectionAsync(cancellationToken));

        // 2. æª¢æŸ¥è¡¨å®Œæ•´æ€§
        results.Add(await CheckTableIntegrityAsync(cancellationToken));

        // 3. æª¢æŸ¥å¤–éµç´„æŸ
        results.Add(await CheckForeignKeyConstraintsAsync(cancellationToken));

        // 4. æª¢æŸ¥ç´¢å¼•å®Œæ•´æ€§
        results.Add(await CheckIndexIntegrityAsync(cancellationToken));

        // 5. æª¢æŸ¥æ•¸æ“šä¸€è‡´æ€§
        results.Add(await CheckDataConsistencyAsync(cancellationToken));

        // 6. æª¢æŸ¥æ¥­å‹™è¦å‰‡
        results.Add(await CheckBusinessRulesAsync(cancellationToken));

        var allPassed = results.All(r => r.Passed);

        return new IntegrityCheckResult(
            Passed: allPassed,
            Checks: results,
            Timestamp: DateTime.UtcNow);
    }

    private async Task<CheckResult> CheckTableIntegrityAsync(CancellationToken cancellationToken)
    {
        try
        {
            // æª¢æŸ¥æ‰€æœ‰é—œéµè¡¨æ˜¯å¦å­˜åœ¨
            const string sql = @"
                SELECT tablename
                FROM pg_tables
                WHERE schemaname = 'public'
                  AND tablename IN ('users', 'workflows', 'workflow_executions', 'agents')";

            await using var cmd = new NpgsqlCommand(sql, _connection);
            var tables = new List<string>();

            await using var reader = await cmd.ExecuteReaderAsync(cancellationToken);
            while (await reader.ReadAsync(cancellationToken))
            {
                tables.Add(reader.GetString(0));
            }

            var expectedTables = new[] { "users", "workflows", "workflow_executions", "agents" };
            var missingTables = expectedTables.Except(tables).ToList();

            if (missingTables.Any())
            {
                return CheckResult.Failed(
                    "TableIntegrity",
                    $"Missing tables: {string.Join(", ", missingTables)}");
            }

            return CheckResult.Passed("TableIntegrity", $"All {tables.Count} critical tables exist");
        }
        catch (Exception ex)
        {
            _logger.LogError(ex, "Table integrity check failed");
            return CheckResult.Failed("TableIntegrity", ex.Message);
        }
    }

    private async Task<CheckResult> CheckForeignKeyConstraintsAsync(CancellationToken cancellationToken)
    {
        try
        {
            // æª¢æŸ¥å¤–éµç´„æŸé•è¦
            const string sql = @"
                SELECT conname, conrelid::regclass AS table_name
                FROM pg_constraint
                WHERE contype = 'f'
                  AND NOT convalidated";

            await using var cmd = new NpgsqlCommand(sql, _connection);
            await using var reader = await cmd.ExecuteReaderAsync(cancellationToken);

            var violations = new List<string>();
            while (await reader.ReadAsync(cancellationToken))
            {
                violations.Add($"{reader.GetString(0)} on {reader.GetString(1)}");
            }

            if (violations.Any())
            {
                return CheckResult.Failed(
                    "ForeignKeyConstraints",
                    $"Invalid constraints: {string.Join(", ", violations)}");
            }

            return CheckResult.Passed("ForeignKeyConstraints", "All foreign keys valid");
        }
        catch (Exception ex)
        {
            _logger.LogError(ex, "Foreign key check failed");
            return CheckResult.Failed("ForeignKeyConstraints", ex.Message);
        }
    }

    private async Task<CheckResult> CheckDataConsistencyAsync(CancellationToken cancellationToken)
    {
        try
        {
            var inconsistencies = new List<string>();

            // æª¢æŸ¥å­¤å…’è¨˜éŒ„ (workflow_executions æ²’æœ‰å°æ‡‰çš„ workflow)
            const string orphanCheckSql = @"
                SELECT COUNT(*)
                FROM workflow_executions we
                LEFT JOIN workflows w ON we.workflow_id = w.id
                WHERE w.id IS NULL";

            await using var cmd = new NpgsqlCommand(orphanCheckSql, _connection);
            var orphanCount = (long)(await cmd.ExecuteScalarAsync(cancellationToken) ?? 0L);

            if (orphanCount > 0)
            {
                inconsistencies.Add($"{orphanCount} orphaned workflow_executions");
            }

            // æª¢æŸ¥æ™‚é–“æˆ³é‚è¼¯éŒ¯èª¤ (created_at > updated_at)
            const string timestampCheckSql = @"
                SELECT COUNT(*)
                FROM workflows
                WHERE created_at > updated_at";

            cmd.CommandText = timestampCheckSql;
            var timestampErrors = (long)(await cmd.ExecuteScalarAsync(cancellationToken) ?? 0L);

            if (timestampErrors > 0)
            {
                inconsistencies.Add($"{timestampErrors} workflows with invalid timestamps");
            }

            if (inconsistencies.Any())
            {
                return CheckResult.Failed(
                    "DataConsistency",
                    string.Join("; ", inconsistencies));
            }

            return CheckResult.Passed("DataConsistency", "All data consistency checks passed");
        }
        catch (Exception ex)
        {
            _logger.LogError(ex, "Data consistency check failed");
            return CheckResult.Failed("DataConsistency", ex.Message);
        }
    }
}

public record IntegrityCheckResult(
    bool Passed,
    IReadOnlyList<CheckResult> Checks,
    DateTime Timestamp);

public record CheckResult(string CheckName, bool Passed, string Message)
{
    public static CheckResult Passed(string checkName, string message) =>
        new(checkName, true, message);
    public static CheckResult Failed(string checkName, string message) =>
        new(checkName, false, message);
}
```

### SQL å®Œæ•´æ€§æª¢æŸ¥è…³æœ¬

```sql
-- scripts/verify-database-integrity.sql
-- åŸ·è¡Œæ­¤è…³æœ¬ä»¥é©—è­‰æ•¸æ“šåº«æ¢å¾©å¾Œçš„å®Œæ•´æ€§

\echo '=== Database Integrity Verification Report ==='
\echo ''

-- 1. è¡¨è¨˜éŒ„æ•¸çµ±è¨ˆ
\echo '1. Table Row Counts:'
SELECT
  schemaname,
  tablename,
  n_live_tup AS row_count
FROM pg_stat_user_tables
ORDER BY n_live_tup DESC;

\echo ''

-- 2. å¤–éµç´„æŸé©—è­‰
\echo '2. Foreign Key Constraint Validation:'
SELECT
  tc.table_name,
  tc.constraint_name,
  kcu.column_name,
  ccu.table_name AS foreign_table_name,
  ccu.column_name AS foreign_column_name
FROM information_schema.table_constraints AS tc
JOIN information_schema.key_column_usage AS kcu
  ON tc.constraint_name = kcu.constraint_name
JOIN information_schema.constraint_column_usage AS ccu
  ON ccu.constraint_name = tc.constraint_name
WHERE tc.constraint_type = 'FOREIGN KEY'
  AND tc.table_schema = 'public';

\echo ''

-- 3. å­¤å…’è¨˜éŒ„æª¢æŸ¥
\echo '3. Orphaned Records Check:'
SELECT
  'workflow_executions' AS table_name,
  COUNT(*) AS orphan_count
FROM workflow_executions we
LEFT JOIN workflows w ON we.workflow_id = w.id
WHERE w.id IS NULL
UNION ALL
SELECT
  'agent_personas',
  COUNT(*)
FROM agent_personas ap
LEFT JOIN agents a ON ap.agent_id = a.id
WHERE a.id IS NULL;

\echo ''

-- 4. æ•¸æ“šæ™‚é–“ç¯„åœ
\echo '4. Data Time Range:'
SELECT
  'workflows' AS table_name,
  MIN(created_at) AS earliest,
  MAX(created_at) AS latest,
  MAX(updated_at) AS last_update
FROM workflows
UNION ALL
SELECT
  'workflow_executions',
  MIN(started_at),
  MAX(started_at),
  MAX(completed_at)
FROM workflow_executions;

\echo ''

-- 5. ç´¢å¼•å¥åº·ç‹€æ…‹
\echo '5. Index Health:'
SELECT
  schemaname,
  tablename,
  indexname,
  idx_scan AS index_scans,
  idx_tup_read AS tuples_read,
  idx_tup_fetch AS tuples_fetched
FROM pg_stat_user_indexes
ORDER BY idx_scan DESC
LIMIT 20;

\echo ''
\echo '=== Verification Complete ==='
```

---

## æ¢å¾©æ€§èƒ½å„ªåŒ–

### ä¸¦è¡Œæ¢å¾©é…ç½®

```bash
# ä½¿ç”¨ pg_restore ä¸¦è¡Œæ¢å¾© (é©ç”¨æ–¼ Custom Format)
pg_restore \
  --dbname=aiagent_prod \
  --username=postgres \
  --jobs=8 \  # 8 å€‹ä¸¦è¡Œé€£æ¥
  --verbose \
  /backups/aiagent_prod.dump

# æ•ˆèƒ½æå‡: 8 jobs å¯å°‡æ¢å¾©æ™‚é–“æ¸›å°‘ 60-70%
```

### ç´¢å¼•é‡å»ºç­–ç•¥

```sql
-- 1. æ¢å¾©æ™‚è·³éç´¢å¼•å‰µå»º (å…ˆå°å…¥æ•¸æ“š)
pg_restore \
  --dbname=aiagent_prod \
  --no-index \  # è·³éç´¢å¼•
  /backups/aiagent_prod.dump

-- 2. æ•¸æ“šå°å…¥å®Œæˆå¾Œ,ä¸¦è¡Œå‰µå»ºç´¢å¼•
-- PostgreSQL 11+ æ”¯æŒä¸¦è¡Œç´¢å¼•å‰µå»º
SET max_parallel_maintenance_workers = 4;

CREATE INDEX CONCURRENTLY idx_workflows_user_id ON workflows(user_id);
CREATE INDEX CONCURRENTLY idx_workflows_created_at ON workflows(created_at);
CREATE INDEX CONCURRENTLY idx_executions_workflow_id ON workflow_executions(workflow_id);

-- 3. é©—è­‰ç´¢å¼•å·²å‰µå»º
SELECT schemaname, tablename, indexname
FROM pg_indexes
WHERE schemaname = 'public'
ORDER BY tablename, indexname;
```

---

## å¸¸è¦‹æ¢å¾©å ´æ™¯è™•ç†

### å ´æ™¯ 1: èª¤åˆªé™¤å–®å€‹ç”¨æˆ¶çš„æ‰€æœ‰å·¥ä½œæµ

**å•é¡Œ**: ç”¨æˆ¶ ID=12345 çš„æ‰€æœ‰ workflows è¢«èª¤åˆªé™¤

**è§£æ±ºæ–¹æ¡ˆ**:

```bash
# 1. PITR æ¢å¾©åˆ°è‡¨æ™‚æ•¸æ“šåº«
./scripts/pitr-restore.sh "2025-01-15 14:29:59" temp_restore

# 2. æå–èª¤åˆªé™¤çš„æ•¸æ“š
psql -d temp_restore -c "
  COPY (
    SELECT *
    FROM workflows
    WHERE user_id = 12345
  ) TO '/tmp/user_12345_workflows.csv' CSV HEADER;
"

# 3. å°å…¥å›ç”Ÿç”¢åº«
psql -d aiagent_prod -c "
  COPY workflows
  FROM '/tmp/user_12345_workflows.csv' CSV HEADER;
"

# 4. é©—è­‰æ¢å¾©
psql -d aiagent_prod -c "
  SELECT COUNT(*) AS restored_workflows
  FROM workflows
  WHERE user_id = 12345;
"
```

### å ´æ™¯ 2: æ•¸æ“šåº«ä¸»å¾åˆ‡æ›å¾Œæ•¸æ“šä¸ä¸€è‡´

**å•é¡Œ**: Primary å¤±æ•—å¾Œåˆ‡æ›åˆ° Replica,ä½† Replica æœ‰æ•¸æ“šå»¶é²

**è§£æ±ºæ–¹æ¡ˆ**:

```sql
-- 1. æª¢æŸ¥ Replica çš„æ•¸æ“šå»¶é²
SELECT
  CASE WHEN pg_last_wal_receive_lsn() = pg_last_wal_replay_lsn()
    THEN 0
    ELSE EXTRACT(EPOCH FROM now() - pg_last_xact_replay_timestamp())
  END AS replication_lag_seconds;

-- 2. å¦‚æœå»¶é²å¯æ¥å— (< 1 åˆ†é˜),æå‡ Replica
SELECT pg_promote();

-- 3. å¦‚æœå»¶é²éå¤§,ç­‰å¾… WAL å›æ”¾å®Œæˆ
-- ç›£æ§ç›´åˆ° pg_last_wal_receive_lsn() = pg_last_wal_replay_lsn()
```

### å ´æ™¯ 3: æ•¸æ“šåº«ç£ç›¤ç©ºé–“ä¸è¶³å°è‡´å´©æ½°

**å•é¡Œ**: æ•¸æ“šåº«å› ç£ç›¤æ»¿è€Œç„¡æ³•å•Ÿå‹•

**è§£æ±ºæ–¹æ¡ˆ**:

```bash
# 1. æ¸…ç†èˆŠ WAL æ–‡ä»¶
cd /var/lib/postgresql/data/pg_wal
ls -lht | head -20  # æŸ¥çœ‹æœ€å¤§çš„ WAL æ–‡ä»¶
rm -f 000000010000000000000001  # åˆªé™¤èˆŠ WAL (è¬¹æ…æ“ä½œ!)

# 2. æ¸…ç†è‡¨æ™‚æ–‡ä»¶
rm -rf /var/lib/postgresql/data/base/pgsql_tmp/*

# 3. æ¸…ç†æ—¥èªŒæ–‡ä»¶
find /var/lib/postgresql/data/log -name "*.log" -mtime +7 -delete

# 4. å¢åŠ ç£ç›¤ç©ºé–“
# Azure Managed Disk æ“´å±•
az disk update \
  --resource-group aiagent-prod-rg \
  --name postgres-data-disk \
  --size-gb 512  # å¾ 256GB æ“´å±•åˆ° 512GB

# 5. é‡å•Ÿæ•¸æ“šåº«
pg_ctl start -D /var/lib/postgresql/data
```

---

## æª¢æŸ¥æ¸…å–®

### æ¢å¾©å‰æª¢æŸ¥æ¸…å–®

- [ ] **å‚™ä»½é©—è­‰**
  - [ ] ç¢ºèªå‚™ä»½æ–‡ä»¶å­˜åœ¨ä¸”å®Œæ•´
  - [ ] é©—è­‰ WAL æ­¸æª”æ–‡ä»¶å¯è¨ªå•
  - [ ] æª¢æŸ¥å‚™ä»½æ–‡ä»¶å¤§å°åˆç†

- [ ] **ç’°å¢ƒæº–å‚™**
  - [ ] ç¢ºèªæ¢å¾©ç›®æ¨™ç’°å¢ƒå¯ç”¨ (ç£ç›¤ç©ºé–“å……è¶³)
  - [ ] åœæ­¢æˆ–éš”é›¢ç”Ÿç”¢æ•¸æ“šåº« (å¦‚éœ€åŸåœ°æ¢å¾©)
  - [ ] é€šçŸ¥ç›¸é—œåœ˜éšŠæ¢å¾©è¨ˆåŠƒ

- [ ] **æ¬Šé™ç¢ºèª**
  - [ ] æ¢å¾©æ“ä½œäººå“¡å…·å‚™å¿…è¦æ¬Šé™
  - [ ] Azure Blob Storage è¨ªå•æ¬Šé™æ­£å¸¸
  - [ ] PostgreSQL ç®¡ç†å“¡æ¬Šé™å¯ç”¨

### æ¢å¾©å¾Œæª¢æŸ¥æ¸…å–®

- [ ] **æ•¸æ“šå®Œæ•´æ€§é©—è­‰**
  - [ ] åŸ·è¡Œå®Œæ•´æ€§æª¢æŸ¥è…³æœ¬
  - [ ] é©—è­‰é—œéµè¡¨è¨˜éŒ„æ•¸
  - [ ] æª¢æŸ¥å¤–éµç´„æŸ
  - [ ] é©—è­‰æ¥­å‹™è¦å‰‡

- [ ] **æ€§èƒ½é©—è­‰**
  - [ ] åŸ·è¡Œå…¸å‹æŸ¥è©¢ä¸¦é©—è­‰æ€§èƒ½
  - [ ] æª¢æŸ¥ç´¢å¼•æ˜¯å¦å®Œæ•´
  - [ ] é©—è­‰çµ±è¨ˆä¿¡æ¯å·²æ›´æ–°

- [ ] **æ‡‰ç”¨é›†æˆæ¸¬è©¦**
  - [ ] å•Ÿå‹•æ‡‰ç”¨ä¸¦é©—è­‰æ•¸æ“šåº«é€£æ¥
  - [ ] åŸ·è¡Œå†’ç…™æ¸¬è©¦ (Smoke Test)
  - [ ] é©—è­‰é—œéµæ¥­å‹™æµç¨‹

- [ ] **æ–‡æª”è¨˜éŒ„**
  - [ ] è¨˜éŒ„æ¢å¾©éç¨‹å’Œæ™‚é–“
  - [ ] è¨˜éŒ„é‡åˆ°çš„å•é¡Œå’Œè§£æ±ºæ–¹æ¡ˆ
  - [ ] æ›´æ–°ç½é›£æ¢å¾©æ‰‹å†Š

---

## ç¸½çµ

æœ¬æ–‡æª”æä¾›äº†å®Œæ•´çš„æ•¸æ“šåº«æ¢å¾©æµç¨‹æŒ‡å—,æ¶µè“‹:

âœ… **Point-in-Time Recovery** (ç²¾ç¢ºåˆ°ç§’çš„æ™‚é–“é»æ¢å¾©)
âœ… **Transaction Log æ¢å¾©** (WAL æ—¥èªŒå›æ”¾)
âœ… **å…¨é‡å‚™ä»½æ¢å¾©** (pg_dump/pg_dumpall)
âœ… **éƒ¨åˆ†æ•¸æ“šæ¢å¾©** (å–®è¡¨ã€å–®åº«ã€ç‰¹å®šæ™‚é–“ç¯„åœ)
âœ… **è·¨å€åŸŸæ¢å¾©** (Geo-Restore, DR ç«™é»æ¿€æ´»)
âœ… **æ•¸æ“šä¸€è‡´æ€§é©—è­‰** (è‡ªå‹•åŒ–å®Œæ•´æ€§æª¢æŸ¥)
âœ… **æ€§èƒ½å„ªåŒ–** (ä¸¦è¡Œæ¢å¾©ã€ç´¢å¼•é‡å»ºç­–ç•¥)

é€éæœ¬æŒ‡å—,é‹ç¶­åœ˜éšŠå¯ä»¥åœ¨å„ç¨®ç½é›£å ´æ™¯ä¸‹å¿«é€Ÿã€æº–ç¢ºåœ°æ¢å¾©æ•¸æ“šåº«,ç¢ºä¿:
- **RTO â‰¤ 2 å°æ™‚**: å¿«é€Ÿæ¢å¾©æ¥­å‹™é‹è¡Œ
- **RPO â‰¤ 15 åˆ†é˜**: æœ€å°åŒ–æ•¸æ“šä¸Ÿå¤±
- **100% æ•¸æ“šå®Œæ•´æ€§**: ç¢ºä¿æ¢å¾©æ•¸æ“šçš„æ­£ç¢ºæ€§
