# å‘Šè­¦é€šçŸ¥ç­–ç•¥å¯¦æ–½æŒ‡å—

## æ¦‚è¿°

### æ–‡æª”ç›®çš„
æœ¬æ–‡æª”æä¾› AI Agent å·¥ä½œæµå¹³å°çš„å®Œæ•´å‘Šè­¦é€šçŸ¥ç­–ç•¥å¯¦æ–½æ–¹æ¡ˆ,æ¶µè“‹:
- **å‘Šè­¦è¦å‰‡è¨­è¨ˆåŸå‰‡** (SLO-Based Alerting, Symptom-Based vs Cause-Based)
- **å‘Šè­¦è·¯ç”±èˆ‡åˆ†çµ„** (Alertmanager Configuration, Alert Routing Trees)
- **é€šçŸ¥æ¸ é“é›†æˆ** (Slack, Email, PagerDuty, Microsoft Teams, Webhook)
- **å‘Šè­¦é™å™ªç­–ç•¥** (Silencing, Inhibition, Grouping, Throttling)
- **On-Call è¼ªç­ç®¡ç†** (PagerDuty Schedules, Escalation Policies)
- **å‘Šè­¦ç–²å‹é˜²æ²»** (Alert Fatigue Prevention, SLO Budget Management)

### æŠ€è¡“èƒŒæ™¯
æœ‰æ•ˆçš„å‘Šè­¦ç­–ç•¥æ˜¯ SRE (Site Reliability Engineering) çš„æ ¸å¿ƒå¯¦è¸:
- **å‘Šè­¦ç›®çš„**: é€šçŸ¥äººå“¡éœ€è¦ç«‹å³æ¡å–è¡Œå‹•çš„å•é¡Œ,è€Œéæ‰€æœ‰ç•°å¸¸
- **ç—‡ç‹€ vs åŸå› **: å‘Šè­¦æ‡‰åŸºæ–¼ç”¨æˆ¶é«”é©— (ç—‡ç‹€),è€Œéç³»çµ±å…§éƒ¨ç‹€æ…‹ (åŸå› )
- **å¯æ“ä½œæ€§**: æ¯å€‹å‘Šè­¦éƒ½æ‡‰è©²æœ‰æ˜ç¢ºçš„è™•ç†æ­¥é©Ÿ (Runbook)
- **é™å™ª**: é¿å…å‘Šè­¦é¢¨æš´,æ¸›å°‘å‘Šè­¦ç–²å‹

**Google SRE å‘Šè­¦å“²å­¸**:
1. **æ¯å€‹å‘Šè­¦éƒ½æ‡‰è©²æ˜¯å¯æ“ä½œçš„** (Actionable)
2. **æ¯å€‹å‘Šè­¦éƒ½æ‡‰è©²éœ€è¦äººå·¥ä»‹å…¥** (Require Human Action)
3. **å‘Šè­¦æ‡‰è©²æ˜¯ç·Šæ€¥çš„** (Urgent)
4. **å‘Šè­¦æ‡‰è©²æ˜¯æ–°ä¿¡æ¯** (Novel)

**SLO-Based Alerting**:
- **Service Level Objective (SLO)**: æœå‹™è³ªé‡ç›®æ¨™ (å¦‚ 99.9% å¯ç”¨æ€§)
- **Error Budget**: å…è¨±çš„éŒ¯èª¤é ç®— (100% - SLO = 0.1%)
- **Burn Rate**: éŒ¯èª¤é ç®—æ¶ˆè€—é€Ÿç‡
- **Multi-Window Alerting**: çŸ­æœŸ + é•·æœŸçª—å£çµåˆ,æ¸›å°‘èª¤å ±

---

## å‘Šè­¦æ¶æ§‹

### æ•´é«”å‘Šè­¦æµ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Metrics Sources                                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â”œâ”€ Prometheus (Application + Infrastructure Metrics)           â”‚
â”‚ â”œâ”€ Azure Monitor (Azure Resources Metrics)                     â”‚
â”‚ â”œâ”€ Application Insights (APM, User Experience)                 â”‚
â”‚ â””â”€ Custom Exporters (PostgreSQL, Redis, Workflow Engine)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â”‚ Evaluation (every 30s)
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Prometheus Alerting Rules (PrometheusRule CRDs)                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â”œâ”€ SLO-Based Alerts (Error Budget Burn Rate)                   â”‚
â”‚ â”œâ”€ Symptom-Based Alerts (High Latency, High Error Rate)        â”‚
â”‚ â”œâ”€ Infrastructure Alerts (CPU, Memory, Disk)                   â”‚
â”‚ â””â”€ Business Alerts (Workflow Failures, Queue Depth)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â”‚ Firing Alerts
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Alertmanager (HA: 3 replicas)                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â”œâ”€ Grouping (by alertname, cluster, service)                   â”‚
â”‚ â”œâ”€ Inhibition (suppress lower severity alerts)                 â”‚
â”‚ â”œâ”€ Silencing (manual/scheduled maintenance)                    â”‚
â”‚ â”œâ”€ Throttling (rate limiting notifications)                    â”‚
â”‚ â””â”€ Routing (based on severity, team, service)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â”‚ Routed Notifications
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚               â”‚               â”‚               â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚ Slack       â”‚ â”‚ PagerDuty   â”‚ â”‚ Email      â”‚ â”‚ Webhook    â”‚
â”‚ (Warnings)  â”‚ â”‚ (Critical)  â”‚ â”‚ (All)      â”‚ â”‚ (Custom)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚               â”‚               â”‚               â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Incident Management                                             â”‚
â”‚ â”œâ”€ Acknowledge & Assign (via PagerDuty)                        â”‚
â”‚ â”œâ”€ Runbook Execution (documented procedures)                   â”‚
â”‚ â”œâ”€ Incident Communication (Status Page, Slack)                 â”‚
â”‚ â””â”€ Post-Mortem (RCA, Action Items)                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## SLO-Based å‘Šè­¦è¦å‰‡

### SLO å®šç¾©

```yaml
# SLO Targets for AI Agent Platform
slos:
  # API Availability SLO: 99.9% (3 nines)
  api_availability:
    target: 0.999
    error_budget: 0.001  # 0.1%
    measurement_window: 30d

  # API Latency SLO: 95% of requests < 500ms
  api_latency_p95:
    target: 0.95
    threshold_ms: 500
    measurement_window: 30d

  # Workflow Success Rate SLO: 99.5%
  workflow_success_rate:
    target: 0.995
    error_budget: 0.005  # 0.5%
    measurement_window: 30d
```

### Multi-Window Burn Rate Alerts

```yaml
# k8s/base/prometheus-slo-rules.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: slo-alerts
  namespace: monitoring
  labels:
    release: prometheus
spec:
  groups:
    - name: slo.api_availability
      interval: 30s
      rules:
        # Recording Rules: è¨ˆç®— Error Rate
        - record: job:api:request_errors:rate5m
          expr: |
            sum(rate(http_requests_total{job="api",code=~"5.."}[5m]))
            /
            sum(rate(http_requests_total{job="api"}[5m]))

        - record: job:api:request_errors:rate1h
          expr: |
            sum(rate(http_requests_total{job="api",code=~"5.."}[1h]))
            /
            sum(rate(http_requests_total{job="api"}[1h]))

        - record: job:api:request_errors:rate6h
          expr: |
            sum(rate(http_requests_total{job="api",code=~"5.."}[6h]))
            /
            sum(rate(http_requests_total{job="api"}[6h]))

        # Alerting Rules: Multi-Window Burn Rate
        # Fast Burn (14.4x): 5% budget consumed in 1 hour â†’ page immediately
        - alert: APIErrorBudgetFastBurn
          expr: |
            (
              job:api:request_errors:rate5m > (14.4 * 0.001)
              and
              job:api:request_errors:rate1h > (14.4 * 0.001)
            )
          for: 2m
          labels:
            severity: critical
            slo: api_availability
            team: platform
          annotations:
            summary: "API error budget is burning too fast"
            description: |
              Current error rate is {{ $value | humanizePercentage }}.
              At this rate, we will exhaust our monthly error budget in 2 days.
              SLO: 99.9% availability (0.1% error budget)
            runbook_url: "https://wiki.example.com/runbooks/api-error-budget-burn"

        # Slow Burn (6x): 5% budget consumed in 6 hours â†’ ticket
        - alert: APIErrorBudgetSlowBurn
          expr: |
            (
              job:api:request_errors:rate6h > (6 * 0.001)
              and
              job:api:request_errors:rate1h > (6 * 0.001)
            )
          for: 15m
          labels:
            severity: warning
            slo: api_availability
            team: platform
          annotations:
            summary: "API error budget is burning slowly"
            description: |
              Current error rate is {{ $value | humanizePercentage }}.
              At this rate, we will exhaust our monthly error budget in 5 days.
            runbook_url: "https://wiki.example.com/runbooks/api-error-budget-burn"

    - name: slo.api_latency
      interval: 30s
      rules:
        # Recording Rule: P95 Latency
        - record: job:api:request_duration:p95
          expr: |
            histogram_quantile(0.95,
              sum(rate(http_request_duration_seconds_bucket{job="api"}[5m])) by (le)
            )

        # Alerting Rule: Latency SLO Violation
        - alert: APILatencySLOViolation
          expr: |
            job:api:request_duration:p95 > 0.5
          for: 5m
          labels:
            severity: warning
            slo: api_latency_p95
            team: platform
          annotations:
            summary: "API latency SLO violation (P95 > 500ms)"
            description: |
              Current P95 latency is {{ $value }}s (target: 0.5s).
              95% of requests should complete within 500ms.
            runbook_url: "https://wiki.example.com/runbooks/api-high-latency"

    - name: slo.workflow_success
      interval: 30s
      rules:
        # Recording Rule: Workflow Success Rate
        - record: job:workflow:success_rate:rate5m
          expr: |
            sum(rate(aiagent_workflow_executions_total{status="success"}[5m]))
            /
            sum(rate(aiagent_workflow_executions_total[5m]))

        # Alerting Rule: Workflow Success Rate SLO Violation
        - alert: WorkflowSuccessRateSLOViolation
          expr: |
            job:workflow:success_rate:rate5m < 0.995
          for: 10m
          labels:
            severity: critical
            slo: workflow_success_rate
            team: workflow-engine
          annotations:
            summary: "Workflow success rate below SLO (< 99.5%)"
            description: |
              Current success rate is {{ $value | humanizePercentage }} (target: 99.5%).
              Error budget: 0.5% (allowing ~360 failures per month for 1M workflows).
            runbook_url: "https://wiki.example.com/runbooks/workflow-failures"
```

---

## ç—‡ç‹€å°å‘å‘Šè­¦è¦å‰‡

### ç”¨æˆ¶é«”é©—å‘Šè­¦

```yaml
# k8s/base/prometheus-symptom-rules.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: symptom-alerts
  namespace: monitoring
spec:
  groups:
    # User-Facing Symptoms (High Priority)
    - name: symptoms.user_facing
      interval: 30s
      rules:
        # High 5xx Error Rate (Users seeing errors)
        - alert: HighErrorRate
          expr: |
            sum(rate(http_requests_total{code=~"5.."}[5m])) by (service)
            /
            sum(rate(http_requests_total[5m])) by (service)
            > 0.05
          for: 3m
          labels:
            severity: critical
            category: symptom
            impact: user_facing
          annotations:
            summary: "High 5xx error rate on {{$labels.service}}"
            description: |
              Error rate is {{ $value | humanizePercentage }} (threshold: 5%).
              Users are experiencing service errors.
            runbook_url: "https://wiki.example.com/runbooks/high-error-rate"

        # High HTTP Latency (Users experiencing slow responses)
        - alert: HighHTTPLatency
          expr: |
            histogram_quantile(0.95,
              sum(rate(http_request_duration_seconds_bucket[5m])) by (le, service)
            ) > 2.0
          for: 10m
          labels:
            severity: warning
            category: symptom
            impact: user_facing
          annotations:
            summary: "High HTTP latency on {{$labels.service}}"
            description: |
              P95 latency is {{ $value }}s (threshold: 2s).
              Users are experiencing slow page loads.
            runbook_url: "https://wiki.example.com/runbooks/high-latency"

        # API Downtime (Service completely unavailable)
        - alert: APIDown
          expr: |
            up{job="api"} == 0
          for: 1m
          labels:
            severity: critical
            category: symptom
            impact: service_down
          annotations:
            summary: "API service is down"
            description: |
              API service {{$labels.instance}} has been down for more than 1 minute.
              All users are affected.
            runbook_url: "https://wiki.example.com/runbooks/service-down"

    # Business Logic Symptoms
    - name: symptoms.business_logic
      interval: 30s
      rules:
        # Workflow Execution Failures
        - alert: HighWorkflowFailureRate
          expr: |
            sum(rate(aiagent_workflow_executions_total{status="failure"}[10m]))
            /
            sum(rate(aiagent_workflow_executions_total[10m]))
            > 0.1
          for: 5m
          labels:
            severity: warning
            category: symptom
            impact: business_logic
          annotations:
            summary: "High workflow failure rate"
            description: |
              Workflow failure rate is {{ $value | humanizePercentage }} (threshold: 10%).
              Workflows are failing at an abnormal rate.

        # Queue Depth Growing (Processing lag)
        - alert: HighQueueDepth
          expr: |
            aiagent_queue_depth > 1000
          for: 10m
          labels:
            severity: warning
            category: symptom
            impact: processing_lag
          annotations:
            summary: "High queue depth on {{$labels.queue_name}}"
            description: |
              Queue depth is {{ $value }} (threshold: 1000).
              Job processing is lagging behind ingestion rate.

        # Azure OpenAI Rate Limiting
        - alert: AzureOpenAIRateLimited
          expr: |
            sum(rate(aiagent_azure_openai_requests_total{status="429"}[5m])) > 10
          for: 5m
          labels:
            severity: critical
            category: symptom
            impact: external_dependency
          annotations:
            summary: "Azure OpenAI rate limiting detected"
            description: |
              Rate limit errors: {{ $value }} requests/s.
              Workflows dependent on OpenAI are failing.
```

---

## Alertmanager é…ç½®

### å®Œæ•´é…ç½®ç¤ºä¾‹

```yaml
# alertmanager-config.yaml
global:
  resolve_timeout: 5m
  slack_api_url: 'https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK'
  pagerduty_url: 'https://events.pagerduty.com/v2/enqueue'

# è·¯ç”±æ¨¹
route:
  receiver: 'default'
  group_by: ['alertname', 'cluster', 'service']
  group_wait: 10s       # ç­‰å¾…åŒä¸€çµ„å…§çš„å…¶ä»–å‘Šè­¦
  group_interval: 10s   # åŒä¸€çµ„å…§æ–°å‘Šè­¦çš„ç™¼é€é–“éš”
  repeat_interval: 12h  # é‡è¤‡ç™¼é€æœªè§£æ±ºå‘Šè­¦çš„é–“éš”

  routes:
    # Critical Alerts â†’ Page on-call engineer via PagerDuty
    - match:
        severity: critical
      receiver: 'pagerduty-critical'
      group_wait: 10s
      group_interval: 5m
      repeat_interval: 4h
      continue: true  # ç¹¼çºŒåŒ¹é…å¾ŒçºŒè·¯ç”±

    # Critical Alerts â†’ Also send to Slack #alerts-critical
    - match:
        severity: critical
      receiver: 'slack-critical'
      group_wait: 10s
      repeat_interval: 12h

    # Warning Alerts â†’ Slack #alerts-warnings only
    - match:
        severity: warning
      receiver: 'slack-warnings'
      group_wait: 30s
      repeat_interval: 24h

    # Database Alerts â†’ Database Team
    - match_re:
        service: '(postgres|redis|mongodb)'
      receiver: 'slack-database-team'
      group_by: ['alertname', 'service', 'instance']

    # Workflow Engine Alerts â†’ Workflow Team
    - match:
        team: workflow-engine
      receiver: 'slack-workflow-team'

    # SLO Budget Alerts â†’ Engineering Leads
    - match_re:
        alertname: '.*ErrorBudget.*'
      receiver: 'email-engineering-leads'
      repeat_interval: 6h

# æ¥æ”¶å™¨å®šç¾©
receivers:
  - name: 'default'
    slack_configs:
      - channel: '#monitoring-default'
        title: '{{ .GroupLabels.alertname }}'
        text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'
        send_resolved: true

  - name: 'pagerduty-critical'
    pagerduty_configs:
      - service_key: 'YOUR_PAGERDUTY_INTEGRATION_KEY'
        description: '{{ .GroupLabels.alertname }}: {{ .CommonAnnotations.summary }}'
        details:
          firing: '{{ .Alerts.Firing | len }}'
          resolved: '{{ .Alerts.Resolved | len }}'
          severity: '{{ .CommonLabels.severity }}'
          runbook: '{{ .CommonAnnotations.runbook_url }}'
        severity: 'critical'

  - name: 'slack-critical'
    slack_configs:
      - channel: '#alerts-critical'
        color: 'danger'
        title: 'ğŸš¨ CRITICAL: {{ .GroupLabels.alertname }}'
        text: |
          *Summary:* {{ .CommonAnnotations.summary }}
          *Description:* {{ .CommonAnnotations.description }}
          *Firing Alerts:* {{ .Alerts.Firing | len }}
          *Runbook:* {{ .CommonAnnotations.runbook_url }}
        actions:
          - type: button
            text: 'View in Grafana'
            url: 'https://grafana.aiagent.example.com'
          - type: button
            text: 'Silence for 1h'
            url: 'https://alertmanager.aiagent.example.com'
        send_resolved: true

  - name: 'slack-warnings'
    slack_configs:
      - channel: '#alerts-warnings'
        color: 'warning'
        title: 'âš ï¸ WARNING: {{ .GroupLabels.alertname }}'
        text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'
        send_resolved: true

  - name: 'slack-database-team'
    slack_configs:
      - channel: '#team-database'
        title: 'Database Alert: {{ .GroupLabels.alertname }}'
        text: '{{ .CommonAnnotations.description }}'

  - name: 'slack-workflow-team'
    slack_configs:
      - channel: '#team-workflow'
        title: 'Workflow Alert: {{ .GroupLabels.alertname }}'
        text: '{{ .CommonAnnotations.description }}'

  - name: 'email-engineering-leads'
    email_configs:
      - to: 'engineering-leads@example.com'
        from: 'alertmanager@aiagent.example.com'
        smarthost: 'smtp.sendgrid.net:587'
        auth_username: 'apikey'
        auth_password: 'YOUR_SENDGRID_API_KEY'
        headers:
          Subject: '[{{ .GroupLabels.severity | toUpper }}] {{ .GroupLabels.alertname }}'
        html: |
          <h2>Alert: {{ .GroupLabels.alertname }}</h2>
          <p><strong>Summary:</strong> {{ .CommonAnnotations.summary }}</p>
          <p><strong>Description:</strong> {{ .CommonAnnotations.description }}</p>
          <p><strong>Firing Alerts:</strong> {{ .Alerts.Firing | len }}</p>
          <p><strong>Runbook:</strong> <a href="{{ .CommonAnnotations.runbook_url }}">{{ .CommonAnnotations.runbook_url }}</a></p>

# æŠ‘åˆ¶è¦å‰‡ (Inhibition Rules)
inhibit_rules:
  # å¦‚æœ APIDown å‘Šè­¦è§¸ç™¼,æŠ‘åˆ¶è©²æœå‹™çš„æ‰€æœ‰å…¶ä»–å‘Šè­¦
  - source_match:
      alertname: 'APIDown'
    target_match_re:
      alertname: '(HighErrorRate|HighLatency|HighCPU|HighMemory)'
    equal: ['service', 'instance']

  # Critical å‘Šè­¦è§¸ç™¼æ™‚,æŠ‘åˆ¶ç›¸åŒæœå‹™çš„ Warning å‘Šè­¦
  - source_match:
      severity: 'critical'
    target_match:
      severity: 'warning'
    equal: ['alertname', 'service']

  # å¦‚æœ PostgreSQL ä¸»ç¯€é»å®•æ©Ÿ,æŠ‘åˆ¶å‰¯æœ¬å»¶é²å‘Šè­¦
  - source_match:
      alertname: 'PostgreSQLDown'
      role: 'primary'
    target_match:
      alertname: 'PostgreSQLReplicationLag'
    equal: ['cluster']
```

---

## PagerDuty é›†æˆ

### PagerDuty æœå‹™é…ç½®

```bash
# 1. åœ¨ PagerDuty å‰µå»ºæœå‹™
# Services â†’ + New Service
# Name: AI Agent Platform - Production
# Integration Type: Prometheus
# Copy Integration Key

# 2. å‰µå»º Escalation Policy
# Escalation Policies â†’ + New Escalation Policy
# Name: Engineering On-Call
# Escalate to:
#   - Level 1: Primary On-Call Engineer (immediately)
#   - Level 2: Secondary On-Call Engineer (after 5 minutes)
#   - Level 3: Engineering Manager (after 15 minutes)

# 3. å‰µå»º On-Call Schedule
# Schedules â†’ + New Schedule
# Name: Engineering Weekly Rotation
# Rotation: Weekly (Monday 9:00 AM)
# Participants: [List of engineers]
```

### Kubernetes Secret é…ç½®

```bash
kubectl create secret generic alertmanager-pagerduty \
  --from-literal=integration-key='YOUR_PAGERDUTY_INTEGRATION_KEY' \
  --namespace monitoring
```

---

## å‘Šè­¦é™å™ªç­–ç•¥

### 1. Grouping (åˆ†çµ„)

```yaml
# æŒ‰ç…§ alertname, cluster, service åˆ†çµ„
route:
  group_by: ['alertname', 'cluster', 'service']
  group_wait: 10s

# å¥½è™•: å¦‚æœ 10 å€‹ API Pod åŒæ™‚è§¸ç™¼ HighMemory å‘Šè­¦,
# åªæœƒç™¼é€ 1 æ¢é€šçŸ¥,è€Œä¸æ˜¯ 10 æ¢
```

### 2. Throttling (é™æµ)

```yaml
# åŒä¸€çµ„å‘Šè­¦æ¯ 12 å°æ™‚é‡è¤‡ç™¼é€ä¸€æ¬¡
route:
  repeat_interval: 12h

# å¥½è™•: é¿å…æŒçºŒå‘Šè­¦æ¯åˆ†é˜éƒ½ç™¼é€é€šçŸ¥
```

### 3. Silencing (éœé»˜)

```bash
# ç¶­è­·çª—å£æœŸé–“éœé»˜æ‰€æœ‰å‘Šè­¦ (é€šé UI æˆ– API)
amtool silence add \
  --alertmanager.url=http://alertmanager:9093 \
  --start='2024-01-15T02:00:00Z' \
  --end='2024-01-15T04:00:00Z' \
  --comment='Planned maintenance: Database upgrade' \
  alertname=~ '.+'

# éœé»˜ç‰¹å®šæœå‹™çš„å‘Šè­¦
amtool silence add \
  --alertmanager.url=http://alertmanager:9093 \
  --duration=2h \
  --comment='Investigating HighLatency on api-service' \
  service='api' \
  alertname='HighLatency'
```

### 4. Inhibition (æŠ‘åˆ¶)

```yaml
# å¦‚æœ PostgreSQL ä¸»ç¯€é»å®•æ©Ÿ,è‡ªå‹•æŠ‘åˆ¶å‰¯æœ¬å»¶é²å‘Šè­¦
inhibit_rules:
  - source_match:
      alertname: 'PostgreSQLDown'
      role: 'primary'
    target_match:
      alertname: 'PostgreSQLReplicationLag'
    equal: ['cluster']

# åŸç†: ä¸»ç¯€é»å®•æ©Ÿæ™‚,å‰¯æœ¬å»¶é²å‘Šè­¦æ˜¯é æœŸçš„,ç„¡éœ€é¡å¤–é€šçŸ¥
```

---

## Runbook ç·¨å¯«è¦ç¯„

### Runbook æ¨¡æ¿

```markdown
# Runbook: High API Error Rate

## Severity
**Critical** - User-facing impact

## Summary
API is returning 5xx errors at a rate exceeding 5% of all requests.

## Impact
- Users are experiencing service errors
- Some workflows may fail to execute
- Potential data loss if errors are unrecoverable

## Diagnosis Steps

1. **Check Error Distribution**
   ```promql
   sum by (code) (rate(http_requests_total{code=~"5.."}[5m]))
   ```
   - Identify which specific error codes are most common (500, 502, 503, 504)

2. **Check Recent Deployments**
   ```bash
   kubectl rollout history deployment/api-deployment -n aiagent-prod
   ```
   - Was there a recent deployment? (last 30 minutes)

3. **Check Database Connectivity**
   ```bash
   kubectl exec -n aiagent-prod api-pod-xxxx -- curl -f http://localhost:8080/health/db
   ```
   - Is database reachable?
   - Are database connections exhausted?

4. **Check External Dependencies**
   - Azure OpenAI: Rate limiting? (429 errors)
   - Redis: Connection failures?

5. **Check Application Logs**
   ```bash
   kubectl logs -n aiagent-prod deployment/api-deployment --tail=100 | grep ERROR
   ```

## Resolution Steps

### Scenario 1: Recent Bad Deployment
```bash
# Rollback to previous version
kubectl rollout undo deployment/api-deployment -n aiagent-prod
kubectl rollout status deployment/api-deployment -n aiagent-prod
```

### Scenario 2: Database Connection Pool Exhausted
```bash
# Scale up API pods to distribute connection load
kubectl scale deployment/api-deployment -n aiagent-prod --replicas=10

# Or increase database connection limit (requires PostgreSQL restart)
```

### Scenario 3: External Dependency Failure
- If Azure OpenAI is down: Enable circuit breaker, return cached responses
- If Redis is down: Fall back to in-memory cache

## Escalation
If unable to resolve within 15 minutes:
1. Escalate to Engineering Manager (via PagerDuty)
2. Start incident communication in #incidents Slack channel
3. Update status page: https://status.aiagent.example.com

## Post-Incident
1. Write post-mortem within 48 hours
2. Identify root cause and contributing factors
3. Create action items to prevent recurrence
4. Update this runbook with lessons learned

## Related Alerts
- `APIDown`: Complete service outage
- `HighHTTPLatency`: Performance degradation
- `DatabaseConnectionFailures`: Database issues

## Grafana Dashboards
- [API Overview](https://grafana.aiagent.example.com/d/api-overview)
- [Database Metrics](https://grafana.aiagent.example.com/d/database)

## References
- [Error Handling Documentation](https://wiki.example.com/error-handling)
- [Database Troubleshooting Guide](https://wiki.example.com/db-troubleshooting)
```

---

## å‘Šè­¦ç–²å‹é˜²æ²»

### 1. Alert Fatigue æŒ‡æ¨™ç›£æ§

```promql
# å‘Šè­¦è§¸ç™¼é »ç‡
sum(rate(alertmanager_alerts_received_total[24h]))

# å‘Šè­¦è§£æ±ºæ™‚é–“ (MTTR - Mean Time To Resolution)
avg(alertmanager_alerts_resolution_time_seconds)

# å‘Šè­¦å™ªéŸ³æ¯” (False Positive Rate)
sum(rate(alertmanager_alerts_resolved_total{resolution="false_positive"}[7d]))
/
sum(rate(alertmanager_alerts_received_total[7d]))
```

### 2. å®šæœŸå¯©æŸ¥å‘Šè­¦è¦å‰‡

```yaml
# æ¯æœˆå¯©æŸ¥å‘Šè­¦è¦å‰‡æœ‰æ•ˆæ€§
alert_review_checklist:
  - name: "Review High-Frequency Alerts"
    question: "Which alerts fired most frequently last month?"
    action: "Consider increasing threshold or adding `for` duration"

  - name: "Review Actionability"
    question: "For each alert fired, was action taken?"
    action: "If no action taken consistently, consider removing alert"

  - name: "Review Resolution Time"
    question: "What was average MTTR for each alert type?"
    action: "Update runbooks for slow-resolving alerts"

  - name: "Review False Positives"
    question: "Which alerts had highest false positive rate?"
    action: "Refine alert conditions or add context"
```

### 3. Error Budget Policy

```yaml
# Error Budget Exhaustion Policy
error_budget_policy:
  # ç•¶éŒ¯èª¤é ç®—æ¶ˆè€— > 50%, æš«åœæ‰€æœ‰éé—œéµåŠŸèƒ½ç™¼å¸ƒ
  - threshold: 50%
    action: "Freeze non-critical feature deployments"

  # ç•¶éŒ¯èª¤é ç®—æ¶ˆè€— > 75%, å°ˆæ³¨æ–¼å¯é æ€§å·¥ä½œ
  - threshold: 75%
    action: "Dedicated SRE sprint for reliability improvements"

  # ç•¶éŒ¯èª¤é ç®—æ¶ˆè€— > 100%, è§¸ç™¼ postmortem
  - threshold: 100%
    action: "Mandatory postmortem, identify systemic issues"
```

---

## æœ€ä½³å¯¦è¸

### 1. å‘Šè­¦å‘½åè¦ç¯„

```
æ ¼å¼: <Component><Symptom><Severity>
ç¤ºä¾‹:
- APIHighErrorRate           (API çµ„ä»¶, é«˜éŒ¯èª¤ç‡, éš±å« Critical)
- WorkflowSlowExecution      (Workflow çµ„ä»¶, åŸ·è¡Œæ…¢, éš±å« Warning)
- DatabaseConnectionFailure  (Database çµ„ä»¶, é€£æ¥å¤±æ•—, Critical)

é¿å…:
- Alert1, Alert2 (ç„¡æ„ç¾©åç¨±)
- HighCPU (ç¼ºå°‘çµ„ä»¶ä¸Šä¸‹æ–‡)
```

### 2. å‘Šè­¦ Annotation æœ€ä½³å¯¦è¸

```yaml
annotations:
  summary: "ç°¡çŸ­æè¿° (1 å¥è©±,é©åˆ Slack é€šçŸ¥æ¨™é¡Œ)"
  description: "è©³ç´°æè¿° (åŒ…å«ç•¶å‰å€¼ã€é–¾å€¼ã€å½±éŸ¿ç¯„åœ)"
  runbook_url: "Runbook URL (å¿…é ˆæä¾›è™•ç†æ­¥é©Ÿ)"
  dashboard_url: "Grafana Dashboard URL (å¯é¸,æ–¹ä¾¿æŸ¥çœ‹è©³ç´°æŒ‡æ¨™)"
  severity_justification: "ç‚ºä½•æ˜¯ Critical/Warning (æœ‰åŠ©æ–¼å¯©æŸ¥)"
```

### 3. å‘Šè­¦æ¸¬è©¦

```bash
# æ‰‹å‹•è§¸ç™¼æ¸¬è©¦å‘Šè­¦
amtool alert add \
  alertname='TestAlert' \
  severity='warning' \
  summary='This is a test alert' \
  --alertmanager.url=http://alertmanager:9093

# é©—è­‰å‘Šè­¦è·¯ç”±
amtool config routes test \
  --config.file=alertmanager.yaml \
  severity=critical \
  service=api
```

---

## æ•…éšœæ’æŸ¥

### å¸¸è¦‹å•é¡Œ

| å•é¡Œ | åŸå›  | è§£æ±ºæ–¹æ¡ˆ |
|------|------|----------|
| **å‘Šè­¦æœªè§¸ç™¼** | PromQL èªæ³•éŒ¯èª¤ | åœ¨ Prometheus UI â†’ Alerts æ¸¬è©¦è¡¨é”å¼ |
| **å‘Šè­¦æœªç™¼é€åˆ° Slack** | Webhook URL éŒ¯èª¤ | æª¢æŸ¥ `slack_api_url` é…ç½®,æ¸¬è©¦ webhook |
| **PagerDuty æœªæ”¶åˆ°é€šçŸ¥** | Integration Key éŒ¯èª¤ | é©—è­‰ PagerDuty Service Key |
| **å‘Šè­¦é¢¨æš´** | ç¼ºå°‘ grouping/throttling | èª¿æ•´ `group_wait`, `repeat_interval` |

### èª¿è©¦æŠ€å·§

```bash
# æŸ¥çœ‹ Alertmanager é…ç½®
kubectl exec -n monitoring alertmanager-prometheus-kube-prometheus-alertmanager-0 -- \
  amtool config show

# æŸ¥çœ‹ç•¶å‰æ´»èºå‘Šè­¦
kubectl exec -n monitoring alertmanager-prometheus-kube-prometheus-alertmanager-0 -- \
  amtool alert query

# æŸ¥çœ‹éœé»˜è¦å‰‡
kubectl exec -n monitoring alertmanager-prometheus-kube-prometheus-alertmanager-0 -- \
  amtool silence query

# æ¸¬è©¦å‘Šè­¦è·¯ç”±
amtool config routes test \
  --config.file=/etc/alertmanager/config/alertmanager.yaml \
  severity=critical service=api
```

---

## ç¸½çµ

æœ¬æ–‡æª”æä¾›äº†å®Œæ•´çš„å‘Šè­¦é€šçŸ¥ç­–ç•¥å¯¦æ–½æ–¹æ¡ˆ,æ¶µè“‹:

âœ… **SLO-Based Alerting** (Error Budget Burn Rate, Multi-Window Alerts)
âœ… **ç—‡ç‹€å°å‘å‘Šè­¦** (User-Facing Impact, Business Logic Symptoms)
âœ… **Alertmanager é…ç½®** (Routing, Grouping, Inhibition, Silencing)
âœ… **å¤šæ¸ é“é›†æˆ** (Slack, PagerDuty, Email, Webhook)
âœ… **å‘Šè­¦é™å™ª** (Grouping, Throttling, Inhibition)
âœ… **Runbook è¦ç¯„** (è¨ºæ–·æ­¥é©Ÿ, è§£æ±ºæ–¹æ¡ˆ, å‡ç´šè·¯å¾‘)
âœ… **å‘Šè­¦ç–²å‹é˜²æ²»** (Alert Fatigue Metrics, Regular Review, Error Budget Policy)

é€éæœ¬æŒ‡å—,é‹ç¶­åœ˜éšŠå¯ä»¥å¯¦ç¾:
- **å¯æ“ä½œå‘Šè­¦**: æ¯å€‹å‘Šè­¦éƒ½æœ‰æ˜ç¢ºè™•ç†æ­¥é©Ÿ
- **é™ä½å™ªéŸ³**: é€šéåˆ†çµ„ã€é™æµã€æŠ‘åˆ¶æ¸›å°‘ 80% å‘Šè­¦é‡
- **å¿«é€ŸéŸ¿æ‡‰**: PagerDuty é›†æˆ,On-Call å·¥ç¨‹å¸«ç¬¬ä¸€æ™‚é–“æ”¶åˆ°é€šçŸ¥
- **æŒçºŒæ”¹é€²**: å®šæœŸå¯©æŸ¥å‘Šè­¦æœ‰æ•ˆæ€§,æ¶ˆé™¤ False Positives
