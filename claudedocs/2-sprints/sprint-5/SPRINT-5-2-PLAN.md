# SPRINT-5-2-PLAN.md - Sprint 5 è¨ˆåŠƒæ›¸ï¼šKnowledge ç®¡ç†èˆ‡ RAG æª¢ç´¢å¯¦æ–½è¨ˆåŠƒ

**ç‰ˆæœ¬**: v2.1
**Sprint ç·¨è™Ÿ**: Sprint 5
**Sprint é€±æœŸ**: Week 13-15 (3 é€±)
**Phase**: Phase 1A - åŸºç¤å¹³å° (Foundation Platform)
**è¨ˆåŠƒæ—¥æœŸ**: 2026-01-06 ~ 2026-01-26
**ç‹€æ…‹**: ğŸ“‹ è¨ˆåŠƒéšæ®µ (Planned)
**å‰µå»ºæ—¥æœŸ**: 2025-11-13
**æœ€å¾Œæ›´æ–°**: 2025-11-13

---

## ğŸ“‘ ç›®éŒ„ (Table of Contents)

1. [è¦åŠƒæ–‡æª”åƒè€ƒ](#è¦åŠƒæ–‡æª”åƒè€ƒ)
2. [ç¬¬ä¸€éƒ¨åˆ†ï¼šæœ¬ Sprint è¦å»ºç«‹ä»€éº¼](#ç¬¬ä¸€éƒ¨åˆ†æœ¬-sprint-è¦å»ºç«‹ä»€éº¼-what-to-build)
3. [ç¬¬äºŒéƒ¨åˆ†ï¼šæŠ€è¡“å¯¦æ–½æ–¹æ¡ˆ](#ç¬¬äºŒéƒ¨åˆ†æŠ€è¡“å¯¦æ–½æ–¹æ¡ˆ-how-to-build)
4. [ç¬¬ä¸‰éƒ¨åˆ†ï¼šç·¨ç¢¼è¦ç¯„](#ç¬¬ä¸‰éƒ¨åˆ†ç·¨ç¢¼è¦ç¯„)
5. [ç¬¬å››éƒ¨åˆ†ï¼šè³ªé‡ä¿è­‰](#ç¬¬å››éƒ¨åˆ†è³ªé‡ä¿è­‰)
6. [ç¬¬äº”éƒ¨åˆ†ï¼šåƒè€ƒæ–‡æª”](#ç¬¬äº”éƒ¨åˆ†åƒè€ƒæ–‡æª”)
7. [ä½¿ç”¨æŒ‡å—](#ä½¿ç”¨æŒ‡å—)
8. [ç‰ˆæœ¬æ­·å²](#ç‰ˆæœ¬æ­·å²)

---

## è¦åŠƒæ–‡æª”åƒè€ƒ

**è¦åŠƒæ–‡æª”åƒè€ƒ**:
- ğŸ“‹ [MVP Scope Definition](../../1-planning/MVP-SCOPE-DEFINITION.md) - Phase 1A ç¯„åœ
- ğŸ“Š [Sprint Allocation Analysis](../../1-planning/SPRINT-ALLOCATION-ANALYSIS.md#sprint-5) - Sprint 5 åˆ†æ
- ğŸ¯ [Development Strategy](../../1-planning/DEVELOPMENT-STRATEGY.md) - Knowledge RAG ç³»çµ±ç­–ç•¥
- ğŸ“ [Architecture Design Document](../../../docs/architecture/Architecture-Design-Document.md) - ç³»çµ±æ¶æ§‹
- ğŸ“– [Module 05 - Agent Memory](../../../docs/user-stories/modules/module-05-agent-memory.md) - US 5.1-5.2 å®Œæ•´è¦æ ¼

---

## ç¬¬ä¸€éƒ¨åˆ†ï¼šæœ¬ Sprint è¦å»ºç«‹ä»€éº¼ (What to Build)

### US 4.1: æ–‡æª”ä¸Šå‚³èˆ‡è™•ç† (8 SP)

**User Story å®Œæ•´è¦æ ¼**: [US 5.1 - çŸ¥è­˜åº«æ–‡ä»¶ä¸Šå‚³èˆ‡è™•ç†](../../../docs/user-stories/modules/module-05-agent-memory.md#us-51)

#### ä¸€ã€MVP ç¯„åœå®šç¾©

**å¿…é ˆå¯¦ç¾åŠŸèƒ½ (P0 - æœ¬ Sprint)**:
- [x] **æ–‡æª”ä¸Šå‚³ API**: æ”¯æ´ PDF, DOCX, TXT, MD æ ¼å¼
  - å¤šæª”æ¡ˆä¸Šå‚³ï¼ˆæœ€å¤š 10 å€‹æª”æ¡ˆï¼‰
  - å–®æª”å¤§å°é™åˆ¶: 50MB
  - æ‰¹æ¬¡ä¸Šå‚³é€²åº¦è¿½è¹¤
  - æª”æ¡ˆé¡å‹é©—è­‰èˆ‡å®‰å…¨æª¢æŸ¥
  - **åƒè€ƒ**: [Document Upload API Design](../../../docs/api/knowledge-api-design.md#upload)

- [x] **æ–‡æª”è§£æå¼•æ“**: æå–ç´”æ–‡å­—å…§å®¹
  - PDF è§£æå™¨ï¼ˆæ”¯æ´ OCRï¼‰
  - DOCX è§£æå™¨ï¼ˆä¿ç•™çµæ§‹ï¼‰
  - TXT/MD è§£æå™¨ï¼ˆç›´æ¥è®€å–ï¼‰
  - å…ƒæ•¸æ“šæå–ï¼ˆæ¨™é¡Œã€ä½œè€…ã€æ—¥æœŸï¼‰
  - **åƒè€ƒ**: [Document Parsing Strategy](../../../docs/technical-implementation/01-backend-net9/10-document-parsing.md)

- [x] **æ–‡æª”åˆ†å¡Š (Chunking)**: æ™ºèƒ½åˆ‡åˆ†æ–‡æœ¬
  - Fixed-size Chunking: 512-2048 tokensï¼ˆå¯é…ç½®ï¼‰
  - Overlap: 0-200 tokensï¼ˆå¯é…ç½®ï¼‰
  - èªç¾©é‚Šç•Œä¿ç•™ï¼ˆæ®µè½ã€ç« ç¯€ï¼‰
  - Chunk å…ƒæ•¸æ“šè¨˜éŒ„ï¼ˆä¾†æºæ–‡æª”ã€ä½ç½®ï¼‰
  - **åƒè€ƒ**: [Chunking Strategies](../../../docs/technical-implementation/01-backend-net9/11-chunking-strategies.md)

- [x] **å‘é‡åŒ– (Embedding)**: æ–‡æœ¬å‘é‡è½‰æ›
  - Azure OpenAI text-embedding-ada-002
  - Batch Embeddingï¼ˆæå‡æ•ˆç‡ï¼‰
  - Embedding å¿«å–æ©Ÿåˆ¶
  - å‘é‡ç¶­åº¦: 1536
  - **åƒè€ƒ**: [Embedding Service](../../../docs/technical-implementation/01-backend-net9/12-embedding-service.md)

- [x] **å‘é‡ç´¢å¼• (Qdrant)**: å‘é‡è³‡æ–™åº«å„²å­˜
  - Qdrant Collection å»ºç«‹èˆ‡ç®¡ç†
  - Vector æ’å…¥èˆ‡æ›´æ–°
  - Metadata éæ¿¾æ”¯æ´
  - HNSW ç´¢å¼•å„ªåŒ–
  - **åƒè€ƒ**: [Qdrant Design](../../../docs/database/qdrant-design.md)

- [x] **Azure Blob Storage æ•´åˆ**: åŸå§‹æª”æ¡ˆå„²å­˜
  - Blob Container ç®¡ç†
  - åˆ†å±¤å„²å­˜ï¼ˆHot/Coolï¼‰
  - SAS Token å®‰å…¨è¨ªå•
  - æª”æ¡ˆç”Ÿå‘½é€±æœŸç®¡ç†
  - **åƒè€ƒ**: [Storage Architecture](../../../docs/architecture/storage-architecture.md)

**æ˜ç¢ºæ’é™¤ Phase 2 åŠŸèƒ½ (å»¶å¾Œ)**:
- âŒ **é€²éš OCR**: è¤‡é›œåœ–è¡¨ã€æ‰‹å¯«æ–‡å­—è­˜åˆ¥ - å»¶å¾Œåˆ° Phase 2
- âŒ **å¤šèªè¨€æ”¯æ´**: è‡ªå‹•èªè¨€æª¢æ¸¬ - å»¶å¾Œåˆ° Sprint 6
- âŒ **æ–‡æª”ç‰ˆæœ¬æ§åˆ¶**: å®Œæ•´ç‰ˆæœ¬æ­·å² - å»¶å¾Œåˆ° Sprint 6
- âŒ **å¢é‡æ›´æ–°**: åƒ…æ›´æ–°è®Šæ›´çš„ Chunks - å»¶å¾Œåˆ° Phase 2
- âŒ **åœ–ç‰‡èˆ‡è¡¨æ ¼æå–**: åœ–ç‰‡æè¿°ã€è¡¨æ ¼çµæ§‹åŒ– - å»¶å¾Œåˆ° Phase 2

**MVP ç¯„åœåƒè€ƒ**:
- ğŸ“– [MVP Scope Definition](../../1-planning/MVP-SCOPE-DEFINITION.md#knowledge-management) - Knowledge ç³»çµ±åœ¨ Phase 1A çš„ç¯„åœ
- ğŸ“– [Sprint Allocation Analysis](../../1-planning/SPRINT-ALLOCATION-ANALYSIS.md#sprint-5) - Sprint 5 ä»»å‹™åˆ†é…

#### äºŒã€è©³ç´°æŠ€è¡“è¦æ ¼

##### US 4.1.1: æ–‡æª”ä¸Šå‚³ API

**API Endpoint**:
```http
POST /api/v1/documents/upload
Content-Type: multipart/form-data

Request Body:
- files: File[] (æœ€å¤š 10 å€‹æª”æ¡ˆ)
- agentId: Guid (é—œè¯çš„ Agent ID)
- tags: string[] (å¯é¸çš„æ¨™ç±¤)
- description: string (å¯é¸çš„æè¿°)

Response:
{
  "uploadId": "uuid",
  "status": "processing",
  "files": [
    {
      "fileName": "document.pdf",
      "fileSize": 1024000,
      "status": "uploaded",
      "documentId": "uuid"
    }
  ],
  "totalFiles": 1,
  "estimatedProcessingTime": "2m30s"
}
```

**é©—è­‰è¦å‰‡**:
- æª”æ¡ˆé¡å‹ç™½åå–®: `.pdf`, `.docx`, `.txt`, `.md`
- å–®æª”å¤§å°é™åˆ¶: 50MB
- æ‰¹æ¬¡ä¸Šå‚³é™åˆ¶: æœ€å¤š 10 å€‹æª”æ¡ˆ
- ç¸½å¤§å°é™åˆ¶: 500MB
- æª”æ¡ˆåç¨±é©—è­‰: ä¸å…è¨±ç‰¹æ®Šå­—å…ƒã€è·¯å¾‘ç©¿è¶Š
- ç—…æ¯’æƒæ: æ•´åˆ ClamAV æˆ– Azure Defender

**å¯¦ä½œç¨‹å¼ç¢¼ç¯„ä¾‹**:
```csharp
// Application/Documents/Commands/UploadDocument/UploadDocumentCommand.cs
public sealed record UploadDocumentCommand : IRequest<Result<UploadDocumentResponse>>
{
    public required Guid AgentId { get; init; }
    public required IFormFileCollection Files { get; init; }
    public string[]? Tags { get; init; }
    public string? Description { get; init; }
}

// Application/Documents/Commands/UploadDocument/UploadDocumentCommandHandler.cs
public sealed class UploadDocumentCommandHandler
    : IRequestHandler<UploadDocumentCommand, Result<UploadDocumentResponse>>
{
    private readonly IDocumentRepository _documentRepo;
    private readonly IBlobStorageService _blobStorage;
    private readonly IBackgroundJobService _backgroundJobs;
    private readonly ILogger<UploadDocumentCommandHandler> _logger;

    public async Task<Result<UploadDocumentResponse>> Handle(
        UploadDocumentCommand request,
        CancellationToken cancellationToken)
    {
        var uploadId = Guid.NewGuid();
        var documentIds = new List<Guid>();

        foreach (var file in request.Files)
        {
            // 1. é©—è­‰æª”æ¡ˆ
            var validationResult = await ValidateFileAsync(file);
            if (!validationResult.IsSuccess)
            {
                return Result<UploadDocumentResponse>.Failure(validationResult.Error);
            }

            // 2. ä¸Šå‚³åˆ° Azure Blob Storage
            var blobUri = await _blobStorage.UploadAsync(
                file.OpenReadStream(),
                file.FileName,
                file.ContentType,
                cancellationToken);

            // 3. å»ºç«‹ Document Entity
            var document = Document.Create(
                agentId: request.AgentId,
                fileName: file.FileName,
                fileSize: file.Length,
                contentType: file.ContentType,
                blobUri: blobUri,
                tags: request.Tags,
                description: request.Description);

            await _documentRepo.AddAsync(document, cancellationToken);
            documentIds.Add(document.Id);

            // 4. æ’ç¨‹èƒŒæ™¯è™•ç†ä»»å‹™ï¼ˆè§£æã€åˆ†å¡Šã€å‘é‡åŒ–ï¼‰
            await _backgroundJobs.EnqueueAsync<IDocumentProcessingJob>(
                job => job.ProcessDocumentAsync(document.Id, cancellationToken));
        }

        await _documentRepo.SaveChangesAsync(cancellationToken);

        return Result<UploadDocumentResponse>.Success(new UploadDocumentResponse
        {
            UploadId = uploadId,
            Status = "processing",
            DocumentIds = documentIds,
            TotalFiles = request.Files.Count,
            EstimatedProcessingTime = EstimateProcessingTime(request.Files)
        });
    }

    private async Task<Result> ValidateFileAsync(IFormFile file)
    {
        // æª”æ¡ˆé¡å‹é©—è­‰
        var allowedExtensions = new[] { ".pdf", ".docx", ".txt", ".md" };
        var extension = Path.GetExtension(file.FileName).ToLowerInvariant();

        if (!allowedExtensions.Contains(extension))
        {
            return Result.Failure($"ä¸æ”¯æ´çš„æª”æ¡ˆé¡å‹: {extension}");
        }

        // æª”æ¡ˆå¤§å°é©—è­‰
        const long maxFileSize = 50 * 1024 * 1024; // 50MB
        if (file.Length > maxFileSize)
        {
            return Result.Failure($"æª”æ¡ˆå¤§å°è¶…éé™åˆ¶ (50MB): {file.FileName}");
        }

        // æª”æ¡ˆåç¨±é©—è­‰ï¼ˆé˜²æ­¢è·¯å¾‘ç©¿è¶Šï¼‰
        if (file.FileName.Contains("..") || file.FileName.Contains("/") || file.FileName.Contains("\\"))
        {
            return Result.Failure($"æª”æ¡ˆåç¨±åŒ…å«éæ³•å­—å…ƒ: {file.FileName}");
        }

        return Result.Success();
    }

    private string EstimateProcessingTime(IFormFileCollection files)
    {
        // ç°¡åŒ–ä¼°ç®—: 1MB = 10 ç§’
        var totalSizeMB = files.Sum(f => f.Length) / (1024.0 * 1024.0);
        var estimatedSeconds = (int)(totalSizeMB * 10);
        return TimeSpan.FromSeconds(estimatedSeconds).ToString(@"m'm's's'");
    }
}
```

**FluentValidation é©—è­‰å™¨**:
```csharp
// Application/Documents/Commands/UploadDocument/UploadDocumentCommandValidator.cs
public sealed class UploadDocumentCommandValidator
    : AbstractValidator<UploadDocumentCommand>
{
    public UploadDocumentCommandValidator()
    {
        RuleFor(x => x.AgentId)
            .NotEmpty().WithMessage("Agent ID is required");

        RuleFor(x => x.Files)
            .NotEmpty().WithMessage("è‡³å°‘éœ€è¦ä¸Šå‚³ä¸€å€‹æª”æ¡ˆ")
            .Must(files => files.Count <= 10)
            .WithMessage("å–®æ¬¡ä¸Šå‚³ä¸èƒ½è¶…é 10 å€‹æª”æ¡ˆ");

        RuleFor(x => x.Files)
            .Must(files => files.Sum(f => f.Length) <= 500 * 1024 * 1024)
            .WithMessage("æ‰¹æ¬¡ä¸Šå‚³ç¸½å¤§å°ä¸èƒ½è¶…é 500MB");

        RuleForEach(x => x.Files)
            .Must(file => file.Length <= 50 * 1024 * 1024)
            .WithMessage("å–®å€‹æª”æ¡ˆå¤§å°ä¸èƒ½è¶…é 50MB");

        RuleForEach(x => x.Files)
            .Must(file => {
                var allowedExtensions = new[] { ".pdf", ".docx", ".txt", ".md" };
                var extension = Path.GetExtension(file.FileName).ToLowerInvariant();
                return allowedExtensions.Contains(extension);
            })
            .WithMessage("åƒ…æ”¯æ´ PDF, DOCX, TXT, MD æ ¼å¼");
    }
}
```

**åƒè€ƒæ–‡æª”**:
- ğŸ“ [Document Upload API](../../../docs/api/knowledge-api-design.md#upload)
- ğŸ“ [Validation Strategy](../../../docs/technical-implementation/01-backend-net9/06-validation-strategy.md)
- ğŸ“ [Azure Blob Storage Integration](../../../docs/technical-implementation/01-backend-net9/13-azure-storage.md)

---

##### US 4.1.2: æ–‡æª”è§£æå¼•æ“

**IDocumentParser ä»‹é¢è¨­è¨ˆ**:
```csharp
// Application/Interfaces/IDocumentParser.cs
public interface IDocumentParser
{
    /// <summary>
    /// æ”¯æ´çš„æª”æ¡ˆé¡å‹
    /// </summary>
    string[] SupportedExtensions { get; }

    /// <summary>
    /// è§£ææ–‡æª”ï¼Œæå–ç´”æ–‡å­—å…§å®¹å’Œå…ƒæ•¸æ“š
    /// </summary>
    Task<DocumentParseResult> ParseAsync(
        Stream fileStream,
        string fileName,
        CancellationToken cancellationToken = default);
}

// Application/Documents/Models/DocumentParseResult.cs
public sealed class DocumentParseResult
{
    public required string Content { get; init; }
    public required DocumentMetadata Metadata { get; init; }
    public int PageCount { get; init; }
    public int CharacterCount { get; init; }
    public TimeSpan ParseDuration { get; init; }
}

public sealed class DocumentMetadata
{
    public string? Title { get; init; }
    public string? Author { get; init; }
    public DateTime? CreatedDate { get; init; }
    public DateTime? ModifiedDate { get; init; }
    public string? Language { get; init; }
    public Dictionary<string, string>? CustomProperties { get; init; }
}
```

**PDF è§£æå™¨å¯¦ä½œ**:
```csharp
// Infrastructure/Services/DocumentParsers/PdfDocumentParser.cs
public sealed class PdfDocumentParser : IDocumentParser
{
    private readonly ILogger<PdfDocumentParser> _logger;
    private readonly IOcrService _ocrService; // å¯é¸çš„ OCR æ”¯æ´

    public string[] SupportedExtensions => new[] { ".pdf" };

    public async Task<DocumentParseResult> ParseAsync(
        Stream fileStream,
        string fileName,
        CancellationToken cancellationToken = default)
    {
        var startTime = DateTime.UtcNow;

        try
        {
            using var pdfDocument = PdfDocument.Open(fileStream);
            var content = new StringBuilder();
            var metadata = ExtractMetadata(pdfDocument);

            foreach (var page in pdfDocument.GetPages())
            {
                var pageText = page.Text;

                // å¦‚æœé é¢æ²’æœ‰æ–‡å­—ï¼ˆæƒæçš„ PDFï¼‰ï¼Œå˜—è©¦ OCR
                if (string.IsNullOrWhiteSpace(pageText) && _ocrService != null)
                {
                    var pageImage = await RenderPageToImageAsync(page);
                    pageText = await _ocrService.RecognizeTextAsync(pageImage, cancellationToken);
                }

                content.AppendLine(pageText);
            }

            return new DocumentParseResult
            {
                Content = content.ToString(),
                Metadata = metadata,
                PageCount = pdfDocument.NumberOfPages,
                CharacterCount = content.Length,
                ParseDuration = DateTime.UtcNow - startTime
            };
        }
        catch (Exception ex)
        {
            _logger.LogError(ex, "Failed to parse PDF: {FileName}", fileName);
            throw new DocumentParseException($"PDF è§£æå¤±æ•—: {fileName}", ex);
        }
    }

    private DocumentMetadata ExtractMetadata(PdfDocument pdf)
    {
        var info = pdf.Information;

        return new DocumentMetadata
        {
            Title = info.Title,
            Author = info.Author,
            CreatedDate = info.CreationDate,
            ModifiedDate = info.ModifiedDate,
            CustomProperties = new Dictionary<string, string>
            {
                ["Producer"] = info.Producer ?? "",
                ["Creator"] = info.Creator ?? ""
            }
        };
    }

    private async Task<byte[]> RenderPageToImageAsync(Page page)
    {
        // ä½¿ç”¨ PdfPig æˆ–å…¶ä»– PDF æ¸²æŸ“åº«
        // å°‡é é¢æ¸²æŸ“ç‚ºåœ–ç‰‡ä¾› OCR ä½¿ç”¨
        // å¯¦ä½œç´°ç¯€ç•¥...
        throw new NotImplementedException("OCR support coming in Phase 2");
    }
}
```

**DOCX è§£æå™¨å¯¦ä½œ**:
```csharp
// Infrastructure/Services/DocumentParsers/DocxDocumentParser.cs
public sealed class DocxDocumentParser : IDocumentParser
{
    private readonly ILogger<DocxDocumentParser> _logger;

    public string[] SupportedExtensions => new[] { ".docx", ".doc" };

    public async Task<DocumentParseResult> ParseAsync(
        Stream fileStream,
        string fileName,
        CancellationToken cancellationToken = default)
    {
        var startTime = DateTime.UtcNow;

        try
        {
            using var document = WordprocessingDocument.Open(fileStream, false);
            var body = document.MainDocumentPart?.Document.Body;

            if (body == null)
            {
                throw new DocumentParseException("DOCX æ–‡ä»¶ä¸åŒ…å«æœ‰æ•ˆå…§å®¹");
            }

            var content = new StringBuilder();
            var metadata = ExtractMetadata(document);

            // æå–æ®µè½æ–‡å­—
            foreach (var paragraph in body.Elements<Paragraph>())
            {
                var text = paragraph.InnerText;
                if (!string.IsNullOrWhiteSpace(text))
                {
                    content.AppendLine(text);
                }
            }

            // æå–è¡¨æ ¼å…§å®¹
            foreach (var table in body.Elements<Table>())
            {
                foreach (var row in table.Elements<TableRow>())
                {
                    var rowText = string.Join(" | ",
                        row.Elements<TableCell>().Select(c => c.InnerText));
                    content.AppendLine(rowText);
                }
            }

            return new DocumentParseResult
            {
                Content = content.ToString(),
                Metadata = metadata,
                PageCount = EstimatePageCount(content.Length),
                CharacterCount = content.Length,
                ParseDuration = DateTime.UtcNow - startTime
            };
        }
        catch (Exception ex)
        {
            _logger.LogError(ex, "Failed to parse DOCX: {FileName}", fileName);
            throw new DocumentParseException($"DOCX è§£æå¤±æ•—: {fileName}", ex);
        }
    }

    private DocumentMetadata ExtractMetadata(WordprocessingDocument document)
    {
        var coreProperties = document.PackageProperties;

        return new DocumentMetadata
        {
            Title = coreProperties.Title,
            Author = coreProperties.Creator,
            CreatedDate = coreProperties.Created,
            ModifiedDate = coreProperties.Modified
        };
    }

    private int EstimatePageCount(int characterCount)
    {
        // ä¼°ç®—: æ¯é ç´„ 2000 å€‹å­—å…ƒ
        return Math.Max(1, (int)Math.Ceiling(characterCount / 2000.0));
    }
}
```

**TXT/MD è§£æå™¨å¯¦ä½œ**:
```csharp
// Infrastructure/Services/DocumentParsers/TextDocumentParser.cs
public sealed class TextDocumentParser : IDocumentParser
{
    private readonly ILogger<TextDocumentParser> _logger;

    public string[] SupportedExtensions => new[] { ".txt", ".md" };

    public async Task<DocumentParseResult> ParseAsync(
        Stream fileStream,
        string fileName,
        CancellationToken cancellationToken = default)
    {
        var startTime = DateTime.UtcNow;

        try
        {
            using var reader = new StreamReader(fileStream, detectEncodingFromByteOrderMarks: true);
            var content = await reader.ReadToEndAsync();

            return new DocumentParseResult
            {
                Content = content,
                Metadata = new DocumentMetadata
                {
                    Title = Path.GetFileNameWithoutExtension(fileName)
                },
                PageCount = 1,
                CharacterCount = content.Length,
                ParseDuration = DateTime.UtcNow - startTime
            };
        }
        catch (Exception ex)
        {
            _logger.LogError(ex, "Failed to parse text file: {FileName}", fileName);
            throw new DocumentParseException($"æ–‡æœ¬æª”æ¡ˆè§£æå¤±æ•—: {fileName}", ex);
        }
    }
}
```

**Document Parser Factory**:
```csharp
// Infrastructure/Services/DocumentParsers/DocumentParserFactory.cs
public sealed class DocumentParserFactory : IDocumentParserFactory
{
    private readonly IEnumerable<IDocumentParser> _parsers;

    public DocumentParserFactory(IEnumerable<IDocumentParser> parsers)
    {
        _parsers = parsers;
    }

    public IDocumentParser GetParser(string fileName)
    {
        var extension = Path.GetExtension(fileName).ToLowerInvariant();

        var parser = _parsers.FirstOrDefault(p =>
            p.SupportedExtensions.Contains(extension));

        if (parser == null)
        {
            throw new UnsupportedFileTypeException(
                $"ä¸æ”¯æ´çš„æª”æ¡ˆé¡å‹: {extension}");
        }

        return parser;
    }
}
```

**åƒè€ƒæ–‡æª”**:
- ğŸ“ [Document Parsing Strategy](../../../docs/technical-implementation/01-backend-net9/10-document-parsing.md)
- ğŸ“¦ [PdfPig Library](https://github.com/UglyToad/PdfPig)
- ğŸ“¦ [DocumentFormat.OpenXml](https://github.com/OfficeDev/Open-XML-SDK)

---

##### US 4.1.3: æ–‡æª”åˆ†å¡Š (Chunking)

**IDocumentChunker ä»‹é¢è¨­è¨ˆ**:
```csharp
// Application/Interfaces/IDocumentChunker.cs
public interface IDocumentChunker
{
    /// <summary>
    /// åˆ†å¡Šç­–ç•¥åç¨±
    /// </summary>
    string StrategyName { get; }

    /// <summary>
    /// å°‡æ–‡æª”å…§å®¹åˆ‡åˆ†ç‚ºå¤šå€‹ Chunks
    /// </summary>
    Task<IReadOnlyList<DocumentChunk>> ChunkAsync(
        string content,
        ChunkingOptions options,
        CancellationToken cancellationToken = default);
}

// Application/Documents/Models/ChunkingOptions.cs
public sealed class ChunkingOptions
{
    public int ChunkSize { get; init; } = 1024; // tokens
    public int OverlapSize { get; init; } = 100; // tokens
    public bool PreserveSentenceBoundary { get; init; } = true;
    public bool PreserveParagraphBoundary { get; init; } = true;
}

// Domain/Entities/DocumentChunk.cs
public sealed class DocumentChunk
{
    public Guid Id { get; private set; }
    public Guid DocumentId { get; private set; }
    public int ChunkIndex { get; private set; }
    public string Content { get; private set; }
    public int TokenCount { get; private set; }
    public int StartPosition { get; private set; }
    public int EndPosition { get; private set; }
    public float[] Embedding { get; private set; } // 1536 ç¶­å‘é‡
    public DateTime CreatedAt { get; private set; }

    public static DocumentChunk Create(
        Guid documentId,
        int chunkIndex,
        string content,
        int tokenCount,
        int startPosition,
        int endPosition)
    {
        return new DocumentChunk
        {
            Id = Guid.NewGuid(),
            DocumentId = documentId,
            ChunkIndex = chunkIndex,
            Content = content,
            TokenCount = tokenCount,
            StartPosition = startPosition,
            EndPosition = endPosition,
            CreatedAt = DateTime.UtcNow
        };
    }

    public void SetEmbedding(float[] embedding)
    {
        if (embedding.Length != 1536)
        {
            throw new ArgumentException("Embedding ç¶­åº¦å¿…é ˆç‚º 1536", nameof(embedding));
        }

        Embedding = embedding;
    }
}
```

**Fixed-Size Chunker å¯¦ä½œ**:
```csharp
// Infrastructure/Services/DocumentChunkers/FixedSizeChunker.cs
public sealed class FixedSizeChunker : IDocumentChunker
{
    private readonly ITokenCounter _tokenCounter;
    private readonly ILogger<FixedSizeChunker> _logger;

    public string StrategyName => "FixedSize";

    public async Task<IReadOnlyList<DocumentChunk>> ChunkAsync(
        string content,
        ChunkingOptions options,
        CancellationToken cancellationToken = default)
    {
        var chunks = new List<DocumentChunk>();
        var sentences = SplitIntoSentences(content);

        var currentChunk = new StringBuilder();
        var currentTokenCount = 0;
        var chunkIndex = 0;
        var startPosition = 0;

        foreach (var sentence in sentences)
        {
            var sentenceTokenCount = await _tokenCounter.CountTokensAsync(sentence);

            // å¦‚æœåŠ å…¥é€™å¥è©±æœƒè¶…é chunk sizeï¼Œå‰‡å»ºç«‹æ–° chunk
            if (currentTokenCount + sentenceTokenCount > options.ChunkSize && currentChunk.Length > 0)
            {
                var chunkContent = currentChunk.ToString().Trim();
                chunks.Add(new DocumentChunk
                {
                    ChunkIndex = chunkIndex++,
                    Content = chunkContent,
                    TokenCount = currentTokenCount,
                    StartPosition = startPosition,
                    EndPosition = startPosition + chunkContent.Length
                });

                // è™•ç† Overlap
                var overlapContent = GetOverlapContent(chunkContent, options.OverlapSize);
                currentChunk = new StringBuilder(overlapContent);
                currentTokenCount = await _tokenCounter.CountTokensAsync(overlapContent);
                startPosition += chunkContent.Length - overlapContent.Length;
            }

            currentChunk.Append(sentence).Append(" ");
            currentTokenCount += sentenceTokenCount;
        }

        // æœ€å¾Œä¸€å€‹ chunk
        if (currentChunk.Length > 0)
        {
            var chunkContent = currentChunk.ToString().Trim();
            chunks.Add(new DocumentChunk
            {
                ChunkIndex = chunkIndex,
                Content = chunkContent,
                TokenCount = currentTokenCount,
                StartPosition = startPosition,
                EndPosition = startPosition + chunkContent.Length
            });
        }

        _logger.LogInformation(
            "Chunked document into {ChunkCount} chunks with average size {AvgSize} tokens",
            chunks.Count,
            chunks.Count > 0 ? chunks.Average(c => c.TokenCount) : 0);

        return chunks;
    }

    private IEnumerable<string> SplitIntoSentences(string content)
    {
        // ç°¡åŒ–çš„å¥å­åˆ†å‰²é‚è¼¯ï¼ˆç”Ÿç”¢ç’°å¢ƒå¯è€ƒæ…®ä½¿ç”¨ NLP åº«ï¼‰
        var sentences = content.Split(
            new[] { '.', '!', '?', 'ã€‚', 'ï¼', 'ï¼Ÿ', '\n' },
            StringSplitOptions.RemoveEmptyEntries);

        return sentences.Select(s => s.Trim()).Where(s => !string.IsNullOrEmpty(s));
    }

    private string GetOverlapContent(string content, int overlapTokens)
    {
        if (overlapTokens <= 0) return string.Empty;

        // å¾æœ«å°¾å– overlapTokens æ•¸é‡çš„æ–‡å­—
        var words = content.Split(' ', StringSplitOptions.RemoveEmptyEntries);
        var overlapWords = words.TakeLast(Math.Min(overlapTokens, words.Length));
        return string.Join(" ", overlapWords);
    }
}
```

**Token Counter å¯¦ä½œ**:
```csharp
// Infrastructure/Services/TikTokenCounter.cs
public sealed class TikTokenCounter : ITokenCounter
{
    private readonly TikTokenizer _tokenizer;

    public TikTokenCounter()
    {
        // ä½¿ç”¨ cl100k_base encodingï¼ˆGPT-3.5, GPT-4ï¼‰
        _tokenizer = TikTokenizer.CreateForModel("gpt-3.5-turbo");
    }

    public Task<int> CountTokensAsync(string text)
    {
        var tokens = _tokenizer.Encode(text);
        return Task.FromResult(tokens.Count);
    }

    public Task<IReadOnlyList<int>> EncodeAsync(string text)
    {
        var tokens = _tokenizer.Encode(text);
        return Task.FromResult<IReadOnlyList<int>>(tokens);
    }
}
```

**åƒè€ƒæ–‡æª”**:
- ğŸ“ [Chunking Strategies](../../../docs/technical-implementation/01-backend-net9/11-chunking-strategies.md)
- ğŸ“¦ [TikToken Library](https://github.com/microsoft/Tokenizer)
- ğŸ“š [Chunking Best Practices](https://www.pinecone.io/learn/chunking-strategies/)

---

##### US 4.1.4: å‘é‡åŒ– (Embedding)

**IEmbeddingService ä»‹é¢è¨­è¨ˆ**:
```csharp
// Application/Interfaces/IEmbeddingService.cs
public interface IEmbeddingService
{
    /// <summary>
    /// ç”Ÿæˆå–®å€‹æ–‡æœ¬çš„å‘é‡è¡¨ç¤º
    /// </summary>
    Task<float[]> GenerateEmbeddingAsync(
        string text,
        CancellationToken cancellationToken = default);

    /// <summary>
    /// æ‰¹é‡ç”Ÿæˆå‘é‡ï¼ˆæå‡æ•ˆç‡ï¼‰
    /// </summary>
    Task<IReadOnlyList<float[]>> GenerateBatchEmbeddingsAsync(
        IEnumerable<string> texts,
        CancellationToken cancellationToken = default);

    /// <summary>
    /// å‘é‡ç¶­åº¦
    /// </summary>
    int Dimensions { get; }
}
```

**OpenAI Embedding Service å¯¦ä½œ**:
```csharp
// Infrastructure/Services/OpenAIEmbeddingService.cs
public sealed class OpenAIEmbeddingService : IEmbeddingService
{
    private readonly OpenAIClient _client;
    private readonly IMemoryCache _cache;
    private readonly ILogger<OpenAIEmbeddingService> _logger;
    private const string ModelName = "text-embedding-ada-002";

    public int Dimensions => 1536;

    public OpenAIEmbeddingService(
        OpenAIClient client,
        IMemoryCache cache,
        ILogger<OpenAIEmbeddingService> logger)
    {
        _client = client;
        _cache = cache;
        _logger = logger;
    }

    public async Task<float[]> GenerateEmbeddingAsync(
        string text,
        CancellationToken cancellationToken = default)
    {
        // æª¢æŸ¥å¿«å–
        var cacheKey = $"embedding:{ComputeHash(text)}";
        if (_cache.TryGetValue<float[]>(cacheKey, out var cachedEmbedding))
        {
            _logger.LogDebug("Using cached embedding for text hash: {Hash}", cacheKey);
            return cachedEmbedding!;
        }

        try
        {
            var embeddingOptions = new EmbeddingsOptions(ModelName, new[] { text });
            var response = await _client.GetEmbeddingsAsync(embeddingOptions, cancellationToken);
            var embedding = response.Value.Data[0].Embedding.ToArray();

            // å¿«å–å‘é‡ï¼ˆ1 å°æ™‚ï¼‰
            _cache.Set(cacheKey, embedding, TimeSpan.FromHours(1));

            _logger.LogInformation("Generated embedding for text (length: {Length})", text.Length);
            return embedding;
        }
        catch (Exception ex)
        {
            _logger.LogError(ex, "Failed to generate embedding");
            throw new EmbeddingGenerationException("å‘é‡ç”Ÿæˆå¤±æ•—", ex);
        }
    }

    public async Task<IReadOnlyList<float[]>> GenerateBatchEmbeddingsAsync(
        IEnumerable<string> texts,
        CancellationToken cancellationToken = default)
    {
        var textList = texts.ToList();
        var embeddings = new List<float[]>();

        // OpenAI API é™åˆ¶: æ¯æ¬¡æœ€å¤š 2048 å€‹è¼¸å…¥
        const int batchSize = 100;

        for (int i = 0; i < textList.Count; i += batchSize)
        {
            var batch = textList.Skip(i).Take(batchSize).ToList();

            try
            {
                var embeddingOptions = new EmbeddingsOptions(ModelName, batch);
                var response = await _client.GetEmbeddingsAsync(embeddingOptions, cancellationToken);

                foreach (var data in response.Value.Data)
                {
                    embeddings.Add(data.Embedding.ToArray());
                }

                _logger.LogInformation(
                    "Generated {Count} embeddings in batch (batch {Current}/{Total})",
                    batch.Count, i / batchSize + 1, (textList.Count + batchSize - 1) / batchSize);

                // é¿å…è¶…é Rate Limit
                await Task.Delay(100, cancellationToken);
            }
            catch (Exception ex)
            {
                _logger.LogError(ex, "Failed to generate batch embeddings");
                throw new EmbeddingGenerationException($"æ‰¹é‡å‘é‡ç”Ÿæˆå¤±æ•— (batch {i / batchSize + 1})", ex);
            }
        }

        return embeddings;
    }

    private static string ComputeHash(string text)
    {
        using var sha256 = SHA256.Create();
        var bytes = Encoding.UTF8.GetBytes(text);
        var hash = sha256.ComputeHash(bytes);
        return Convert.ToBase64String(hash);
    }
}
```

**åƒè€ƒæ–‡æª”**:
- ğŸ“ [Embedding Service](../../../docs/technical-implementation/01-backend-net9/12-embedding-service.md)
- ğŸ“š [OpenAI Embeddings Guide](https://platform.openai.com/docs/guides/embeddings)
- ğŸ“š [Azure OpenAI Embeddings](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/embeddings)

---

##### US 4.1.5: å‘é‡ç´¢å¼• (Qdrant)

**IVectorStoreService ä»‹é¢è¨­è¨ˆ**:
```csharp
// Application/Interfaces/IVectorStoreService.cs
public interface IVectorStoreService
{
    /// <summary>
    /// å»ºç«‹ Collection
    /// </summary>
    Task CreateCollectionAsync(
        string collectionName,
        int vectorSize,
        CancellationToken cancellationToken = default);

    /// <summary>
    /// æ’å…¥å‘é‡
    /// </summary>
    Task UpsertVectorsAsync(
        string collectionName,
        IEnumerable<VectorRecord> records,
        CancellationToken cancellationToken = default);

    /// <summary>
    /// åˆªé™¤å‘é‡
    /// </summary>
    Task DeleteVectorsAsync(
        string collectionName,
        IEnumerable<Guid> ids,
        CancellationToken cancellationToken = default);

    /// <summary>
    /// å‘é‡æœå°‹
    /// </summary>
    Task<IReadOnlyList<SearchResult>> SearchAsync(
        string collectionName,
        float[] queryVector,
        int limit = 10,
        float scoreThreshold = 0.7f,
        Dictionary<string, object>? filter = null,
        CancellationToken cancellationToken = default);
}

// Application/Documents/Models/VectorRecord.cs
public sealed class VectorRecord
{
    public required Guid Id { get; init; }
    public required float[] Vector { get; init; }
    public required Dictionary<string, object> Metadata { get; init; }
}

// Application/Documents/Models/SearchResult.cs
public sealed class SearchResult
{
    public required Guid Id { get; init; }
    public required float Score { get; init; }
    public required Dictionary<string, object> Metadata { get; init; }
}
```

**Qdrant Service å¯¦ä½œ**:
```csharp
// Infrastructure/Services/QdrantVectorStoreService.cs
public sealed class QdrantVectorStoreService : IVectorStoreService
{
    private readonly QdrantClient _client;
    private readonly ILogger<QdrantVectorStoreService> _logger;

    public QdrantVectorStoreService(
        QdrantClient client,
        ILogger<QdrantVectorStoreService> logger)
    {
        _client = client;
        _logger = logger;
    }

    public async Task CreateCollectionAsync(
        string collectionName,
        int vectorSize,
        CancellationToken cancellationToken = default)
    {
        try
        {
            // æª¢æŸ¥ Collection æ˜¯å¦å·²å­˜åœ¨
            var collections = await _client.ListCollectionsAsync(cancellationToken);
            if (collections.Any(c => c.Name == collectionName))
            {
                _logger.LogInformation("Collection {CollectionName} already exists", collectionName);
                return;
            }

            // å»ºç«‹ Collectionï¼ˆä½¿ç”¨ HNSW ç´¢å¼•ï¼‰
            await _client.CreateCollectionAsync(
                collectionName,
                new VectorParams
                {
                    Size = (ulong)vectorSize,
                    Distance = Distance.Cosine, // Cosine Similarity
                    HnswConfig = new HnswConfigDiff
                    {
                        M = 16,  // é„°å±…æ•¸é‡ï¼ˆè¶Šå¤§è¶Šç²¾ç¢ºä½†è¶Šæ…¢ï¼‰
                        EfConstruct = 100  // ç´¢å¼•å»ºç«‹æ™‚çš„æœå°‹æ·±åº¦
                    }
                },
                cancellationToken: cancellationToken);

            _logger.LogInformation(
                "Created Qdrant collection: {CollectionName} with vector size {VectorSize}",
                collectionName, vectorSize);
        }
        catch (Exception ex)
        {
            _logger.LogError(ex, "Failed to create Qdrant collection: {CollectionName}", collectionName);
            throw new VectorStoreException($"å»ºç«‹ Collection å¤±æ•—: {collectionName}", ex);
        }
    }

    public async Task UpsertVectorsAsync(
        string collectionName,
        IEnumerable<VectorRecord> records,
        CancellationToken cancellationToken = default)
    {
        try
        {
            var points = records.Select(r => new PointStruct
            {
                Id = new PointId { Uuid = r.Id.ToString() },
                Vectors = r.Vector,
                Payload = r.Metadata.ToDictionary(
                    kvp => kvp.Key,
                    kvp => new Value { StringValue = kvp.Value?.ToString() })
            }).ToList();

            await _client.UpsertAsync(
                collectionName,
                points,
                cancellationToken: cancellationToken);

            _logger.LogInformation(
                "Upserted {Count} vectors to collection: {CollectionName}",
                points.Count, collectionName);
        }
        catch (Exception ex)
        {
            _logger.LogError(ex, "Failed to upsert vectors to collection: {CollectionName}", collectionName);
            throw new VectorStoreException($"å‘é‡æ’å…¥å¤±æ•—: {collectionName}", ex);
        }
    }

    public async Task DeleteVectorsAsync(
        string collectionName,
        IEnumerable<Guid> ids,
        CancellationToken cancellationToken = default)
    {
        try
        {
            var pointIds = ids.Select(id => new PointId { Uuid = id.ToString() }).ToList();

            await _client.DeleteAsync(
                collectionName,
                pointIds,
                cancellationToken: cancellationToken);

            _logger.LogInformation(
                "Deleted {Count} vectors from collection: {CollectionName}",
                pointIds.Count, collectionName);
        }
        catch (Exception ex)
        {
            _logger.LogError(ex, "Failed to delete vectors from collection: {CollectionName}", collectionName);
            throw new VectorStoreException($"å‘é‡åˆªé™¤å¤±æ•—: {collectionName}", ex);
        }
    }

    public async Task<IReadOnlyList<SearchResult>> SearchAsync(
        string collectionName,
        float[] queryVector,
        int limit = 10,
        float scoreThreshold = 0.7f,
        Dictionary<string, object>? filter = null,
        CancellationToken cancellationToken = default)
    {
        try
        {
            var searchParams = new SearchParams
            {
                HnswEf = 128, // æœå°‹æ™‚çš„æ·±åº¦ï¼ˆè¶Šå¤§è¶Šç²¾ç¢ºä½†è¶Šæ…¢ï¼‰
                Exact = false // ä½¿ç”¨è¿‘ä¼¼æœå°‹ï¼ˆHNSWï¼‰
            };

            Filter? qdrantFilter = null;
            if (filter != null && filter.Any())
            {
                // å»ºç«‹ Qdrant éæ¿¾æ¢ä»¶
                qdrantFilter = BuildFilter(filter);
            }

            var searchResults = await _client.SearchAsync(
                collectionName,
                queryVector,
                limit: (ulong)limit,
                scoreThreshold: scoreThreshold,
                filter: qdrantFilter,
                searchParams: searchParams,
                cancellationToken: cancellationToken);

            var results = searchResults.Select(r => new SearchResult
            {
                Id = Guid.Parse(r.Id.Uuid),
                Score = r.Score,
                Metadata = r.Payload.ToDictionary(
                    kvp => kvp.Key,
                    kvp => (object)(kvp.Value.StringValue ?? ""))
            }).ToList();

            _logger.LogInformation(
                "Searched collection {CollectionName}, found {Count} results",
                collectionName, results.Count);

            return results;
        }
        catch (Exception ex)
        {
            _logger.LogError(ex, "Failed to search collection: {CollectionName}", collectionName);
            throw new VectorStoreException($"å‘é‡æœå°‹å¤±æ•—: {collectionName}", ex);
        }
    }

    private Filter BuildFilter(Dictionary<string, object> filter)
    {
        // ç°¡åŒ–çš„éæ¿¾æ¢ä»¶å»ºç«‹ï¼ˆå¯¦éš›ç”Ÿç”¢ç’°å¢ƒéœ€è¦æ›´è¤‡é›œçš„é‚è¼¯ï¼‰
        var conditions = filter.Select(kvp => new Condition
        {
            Field = new FieldCondition
            {
                Key = kvp.Key,
                Match = new Match { Keyword = kvp.Value.ToString() }
            }
        }).ToList();

        return new Filter
        {
            Must = { conditions }
        };
    }
}
```

**åƒè€ƒæ–‡æª”**:
- ğŸ“ [Qdrant Design](../../../docs/database/qdrant-design.md)
- ğŸ“š [Qdrant Documentation](https://qdrant.tech/documentation/)
- ğŸ“š [HNSW Algorithm](https://arxiv.org/abs/1603.09320)

---

##### US 4.1.6: èƒŒæ™¯è™•ç†ä»»å‹™

**Document Processing Job**:
```csharp
// Infrastructure/BackgroundJobs/DocumentProcessingJob.cs
public sealed class DocumentProcessingJob : IDocumentProcessingJob
{
    private readonly IDocumentRepository _documentRepo;
    private readonly IBlobStorageService _blobStorage;
    private readonly IDocumentParserFactory _parserFactory;
    private readonly IDocumentChunker _chunker;
    private readonly IEmbeddingService _embeddingService;
    private readonly IVectorStoreService _vectorStore;
    private readonly ILogger<DocumentProcessingJob> _logger;

    public async Task ProcessDocumentAsync(
        Guid documentId,
        CancellationToken cancellationToken = default)
    {
        _logger.LogInformation("Starting document processing for ID: {DocumentId}", documentId);

        try
        {
            // 1. è¼‰å…¥ Document Entity
            var document = await _documentRepo.GetByIdAsync(documentId, cancellationToken);
            if (document == null)
            {
                _logger.LogError("Document not found: {DocumentId}", documentId);
                return;
            }

            document.UpdateStatus(DocumentStatus.Processing);
            await _documentRepo.SaveChangesAsync(cancellationToken);

            // 2. å¾ Blob Storage ä¸‹è¼‰æª”æ¡ˆ
            _logger.LogInformation("Downloading file from blob storage: {BlobUri}", document.BlobUri);
            await using var fileStream = await _blobStorage.DownloadAsync(
                document.BlobUri, cancellationToken);

            // 3. è§£ææ–‡æª”
            _logger.LogInformation("Parsing document: {FileName}", document.FileName);
            var parser = _parserFactory.GetParser(document.FileName);
            var parseResult = await parser.ParseAsync(fileStream, document.FileName, cancellationToken);

            document.UpdateMetadata(parseResult.Metadata);
            document.UpdatePageCount(parseResult.PageCount);

            // 4. æ–‡æª”åˆ†å¡Š
            _logger.LogInformation("Chunking document: {DocumentId}", documentId);
            var chunkingOptions = new ChunkingOptions
            {
                ChunkSize = 1024,
                OverlapSize = 100,
                PreserveSentenceBoundary = true
            };

            var chunks = await _chunker.ChunkAsync(
                parseResult.Content, chunkingOptions, cancellationToken);

            _logger.LogInformation("Created {ChunkCount} chunks for document {DocumentId}",
                chunks.Count, documentId);

            // 5. ç”Ÿæˆå‘é‡ï¼ˆæ‰¹é‡ï¼‰
            _logger.LogInformation("Generating embeddings for {ChunkCount} chunks", chunks.Count);
            var chunkTexts = chunks.Select(c => c.Content).ToList();
            var embeddings = await _embeddingService.GenerateBatchEmbeddingsAsync(
                chunkTexts, cancellationToken);

            // 6. é—œè¯å‘é‡èˆ‡ Chunks
            for (int i = 0; i < chunks.Count; i++)
            {
                chunks[i].SetEmbedding(embeddings[i]);
            }

            // 7. å„²å­˜åˆ° Qdrant
            _logger.LogInformation("Storing vectors in Qdrant for document {DocumentId}", documentId);
            var collectionName = $"agent_{document.AgentId}";

            // ç¢ºä¿ Collection å­˜åœ¨
            await _vectorStore.CreateCollectionAsync(
                collectionName, _embeddingService.Dimensions, cancellationToken);

            // æ’å…¥å‘é‡
            var vectorRecords = chunks.Select(chunk => new VectorRecord
            {
                Id = chunk.Id,
                Vector = chunk.Embedding,
                Metadata = new Dictionary<string, object>
                {
                    ["document_id"] = document.Id.ToString(),
                    ["chunk_index"] = chunk.ChunkIndex,
                    ["content"] = chunk.Content,
                    ["token_count"] = chunk.TokenCount
                }
            }).ToList();

            await _vectorStore.UpsertVectorsAsync(collectionName, vectorRecords, cancellationToken);

            // 8. æ›´æ–°æ–‡æª”ç‹€æ…‹
            document.UpdateStatus(DocumentStatus.Completed);
            document.UpdateChunkCount(chunks.Count);
            await _documentRepo.SaveChangesAsync(cancellationToken);

            _logger.LogInformation(
                "Document processing completed successfully for ID: {DocumentId}, Chunks: {ChunkCount}",
                documentId, chunks.Count);
        }
        catch (Exception ex)
        {
            _logger.LogError(ex, "Document processing failed for ID: {DocumentId}", documentId);

            // æ›´æ–°æ–‡æª”ç‹€æ…‹ç‚ºå¤±æ•—
            var document = await _documentRepo.GetByIdAsync(documentId, cancellationToken);
            if (document != null)
            {
                document.UpdateStatus(DocumentStatus.Failed);
                document.UpdateErrorMessage(ex.Message);
                await _documentRepo.SaveChangesAsync(cancellationToken);
            }

            throw;
        }
    }
}
```

**Hangfire é…ç½®**:
```csharp
// API/Program.cs - Hangfire è¨»å†Š
builder.Services.AddHangfire(config =>
{
    config.UsePostgreSqlStorage(builder.Configuration.GetConnectionString("DefaultConnection"));
    config.UseRecommendedSerializerSettings();
});

builder.Services.AddHangfireServer(options =>
{
    options.WorkerCount = 5; // åŒæ™‚è™•ç† 5 å€‹ä»»å‹™
    options.Queues = new[] { "document-processing", "default" };
});

// è¨»å†ŠèƒŒæ™¯ä»»å‹™
builder.Services.AddScoped<IDocumentProcessingJob, DocumentProcessingJob>();

// å•Ÿç”¨ Hangfire Dashboard
app.UseHangfireDashboard("/hangfire", new DashboardOptions
{
    Authorization = new[] { new HangfireAuthorizationFilter() }
});
```

**åƒè€ƒæ–‡æª”**:
- ğŸ“ [Background Jobs](../../../docs/technical-implementation/01-backend-net9/14-background-jobs.md)
- ğŸ“¦ [Hangfire](https://www.hangfire.io/)
- ğŸ“š [Hangfire Best Practices](https://docs.hangfire.io/en/latest/best-practices.html)

---

### US 4.2: RAG æª¢ç´¢ç­–ç•¥é…ç½® (5 SP)

**User Story å®Œæ•´è¦æ ¼**: [US 5.2 - ç²¾ç¢ºæª¢ç´¢ç­–ç•¥é…ç½®](../../../docs/user-stories/modules/module-05-agent-memory.md#us-52)

#### ä¸€ã€MVP ç¯„åœå®šç¾©

**å¿…é ˆå¯¦ç¾åŠŸèƒ½ (P0 - æœ¬ Sprint)**:
- [x] **èªç¾©æœå°‹ (Semantic Search)**: å‘é‡ç›¸ä¼¼åº¦æª¢ç´¢
  - Qdrant Vector Search
  - Cosine Similarity
  - ç›¸ä¼¼åº¦é–¾å€¼: 0.7-0.9ï¼ˆå¯é…ç½®ï¼‰
  - Top-K çµæœ: 1-20ï¼ˆå¯é…ç½®ï¼‰
  - **åƒè€ƒ**: [Semantic Search](../../../docs/technical-implementation/01-backend-net9/15-semantic-search.md)

- [x] **é—œéµå­—æœå°‹ (Keyword Search)**: BM25 æ¼”ç®—æ³•
  - å…¨æ–‡æª¢ç´¢ï¼ˆPostgreSQL FTSï¼‰
  - BM25 æ¼”ç®—æ³•å¯¦ä½œ
  - é—œéµè©æ¬Šé‡èª¿æ•´
  - åŒç¾©è©æ”¯æ´
  - **åƒè€ƒ**: [Keyword Search](../../../docs/technical-implementation/01-backend-net9/16-keyword-search.md)

- [x] **æ··åˆæª¢ç´¢ (Hybrid Search)**: Semantic + Keyword
  - RRF (Reciprocal Rank Fusion) æ¼”ç®—æ³•
  - æ¬Šé‡é…ç½®: Semantic 70% + Keyword 30%ï¼ˆå¯èª¿æ•´ï¼‰
  - çµæœå»é‡èˆ‡æ’åº
  - **åƒè€ƒ**: [Hybrid Search](../../../docs/technical-implementation/01-backend-net9/17-hybrid-search.md)

- [x] **Re-ranking**: Cross-Encoder é‡æ’åº
  - HuggingFace Cross-Encoder æ¨¡å‹
  - æŸ¥è©¢èˆ‡æ–‡æª”çš„ç›¸é—œæ€§è©•åˆ†
  - é‡æ’åº Top-K çµæœ
  - **åƒè€ƒ**: [Re-ranking Strategy](../../../docs/technical-implementation/01-backend-net9/18-reranking.md)

- [x] **æª¢ç´¢æº–ç¢ºç‡æ¸¬è©¦æ¡†æ¶**: è‡ªå‹•åŒ–è©•ä¼°
  - æ¨™æº–æ¸¬è©¦é›†ï¼ˆ100+ Q&A pairsï¼‰
  - è‡ªå‹•åŒ–è©•ä¼°ï¼ˆMRR, NDCGï¼‰
  - æº–ç¢ºç‡ç›®æ¨™: â‰¥90%
  - **åƒè€ƒ**: [RAG Testing](../../../docs/testing/rag-accuracy-testing.md)

**æ˜ç¢ºæ’é™¤ Phase 2 åŠŸèƒ½ (å»¶å¾Œ)**:
- âŒ **Query Expansion**: æŸ¥è©¢æ“´å±•èˆ‡æ”¹å¯« - å»¶å¾Œåˆ° Phase 2
- âŒ **Contextual Compression**: ä¸Šä¸‹æ–‡å£“ç¸® - å»¶å¾Œåˆ° Phase 2
- âŒ **Multi-Query Retrieval**: å¤šæŸ¥è©¢æª¢ç´¢ - å»¶å¾Œåˆ° Sprint 6
- âŒ **Parent Document Retriever**: çˆ¶æ–‡æª”æª¢ç´¢å™¨ - å»¶å¾Œåˆ° Phase 2

**MVP ç¯„åœåƒè€ƒ**:
- ğŸ“– [MVP Scope Definition](../../1-planning/MVP-SCOPE-DEFINITION.md#rag-retrieval) - RAG åœ¨ Phase 1A çš„ç¯„åœ
- ğŸ“– [Sprint Allocation Analysis](../../1-planning/SPRINT-ALLOCATION-ANALYSIS.md#sprint-5) - Sprint 5 RAG ä»»å‹™

#### äºŒã€è©³ç´°æŠ€è¡“è¦æ ¼

##### US 4.2.1: èªç¾©æœå°‹ (Semantic Search)

**ISemanticSearchService ä»‹é¢è¨­è¨ˆ**:
```csharp
// Application/Interfaces/ISemanticSearchService.cs
public interface ISemanticSearchService
{
    /// <summary>
    /// åŸ·è¡Œèªç¾©æœå°‹
    /// </summary>
    Task<IReadOnlyList<SearchResultDto>> SearchAsync(
        string query,
        Guid agentId,
        SemanticSearchOptions options,
        CancellationToken cancellationToken = default);
}

// Application/RAG/Models/SemanticSearchOptions.cs
public sealed class SemanticSearchOptions
{
    public int TopK { get; init; } = 10;
    public float ScoreThreshold { get; init; } = 0.7f;
    public Dictionary<string, object>? Metadata Filter { get; init; }
}

// Application/RAG/DTOs/SearchResultDto.cs
public sealed class SearchResultDto
{
    public required Guid ChunkId { get; init; }
    public required Guid DocumentId { get; init; }
    public required string Content { get; init; }
    public required float Score { get; init; }
    public int ChunkIndex { get; init; }
    public Dictionary<string, object>? Metadata { get; init; }
}
```

**Semantic Search Service å¯¦ä½œ**:
```csharp
// Infrastructure/Services/SemanticSearchService.cs
public sealed class SemanticSearchService : ISemanticSearchService
{
    private readonly IEmbeddingService _embeddingService;
    private readonly IVectorStoreService _vectorStore;
    private readonly ILogger<SemanticSearchService> _logger;

    public async Task<IReadOnlyList<SearchResultDto>> SearchAsync(
        string query,
        Guid agentId,
        SemanticSearchOptions options,
        CancellationToken cancellationToken = default)
    {
        _logger.LogInformation(
            "Performing semantic search for Agent {AgentId}, Query: {Query}",
            agentId, query);

        try
        {
            // 1. ç”ŸæˆæŸ¥è©¢å‘é‡
            var queryEmbedding = await _embeddingService.GenerateEmbeddingAsync(
                query, cancellationToken);

            // 2. Qdrant å‘é‡æœå°‹
            var collectionName = $"agent_{agentId}";
            var searchResults = await _vectorStore.SearchAsync(
                collectionName,
                queryEmbedding,
                limit: options.TopK,
                scoreThreshold: options.ScoreThreshold,
                filter: options.MetadataFilter,
                cancellationToken: cancellationToken);

            // 3. è½‰æ›ç‚º DTO
            var results = searchResults.Select(r => new SearchResultDto
            {
                ChunkId = r.Id,
                DocumentId = Guid.Parse(r.Metadata["document_id"]?.ToString() ?? ""),
                Content = r.Metadata["content"]?.ToString() ?? "",
                Score = r.Score,
                ChunkIndex = int.Parse(r.Metadata["chunk_index"]?.ToString() ?? "0"),
                Metadata = r.Metadata
            }).ToList();

            _logger.LogInformation(
                "Semantic search completed, found {Count} results with scores >= {Threshold}",
                results.Count, options.ScoreThreshold);

            return results;
        }
        catch (Exception ex)
        {
            _logger.LogError(ex, "Semantic search failed for Agent {AgentId}", agentId);
            throw new SearchException("èªç¾©æœå°‹å¤±æ•—", ex);
        }
    }
}
```

**åƒè€ƒæ–‡æª”**:
- ğŸ“ [Semantic Search](../../../docs/technical-implementation/01-backend-net9/15-semantic-search.md)
- ğŸ“š [Vector Search Best Practices](https://qdrant.tech/documentation/concepts/search/)

---

##### US 4.2.2: é—œéµå­—æœå°‹ (Keyword Search)

**IKeywordSearchService ä»‹é¢è¨­è¨ˆ**:
```csharp
// Application/Interfaces/IKeywordSearchService.cs
public interface IKeywordSearchService
{
    /// <summary>
    /// åŸ·è¡Œé—œéµå­—æœå°‹ï¼ˆBM25ï¼‰
    /// </summary>
    Task<IReadOnlyList<SearchResultDto>> SearchAsync(
        string query,
        Guid agentId,
        KeywordSearchOptions options,
        CancellationToken cancellationToken = default);
}

// Application/RAG/Models/KeywordSearchOptions.cs
public sealed class KeywordSearchOptions
{
    public int TopK { get; init; } = 10;
    public bool UseSynonyms { get; init; } = true;
    public Dictionary<string, float>? TermBoosts { get; init; }
}
```

**PostgreSQL Full-Text Search å¯¦ä½œ**:
```csharp
// Infrastructure/Services/KeywordSearchService.cs
public sealed class KeywordSearchService : IKeywordSearchService
{
    private readonly ApplicationDbContext _dbContext;
    private readonly ILogger<KeywordSearchService> _logger;

    public async Task<IReadOnlyList<SearchResultDto>> SearchAsync(
        string query,
        Guid agentId,
        KeywordSearchOptions options,
        CancellationToken cancellationToken = default)
    {
        _logger.LogInformation(
            "Performing keyword search for Agent {AgentId}, Query: {Query}",
            agentId, query);

        try
        {
            // 1. é è™•ç†æŸ¥è©¢ï¼ˆåˆ†è©ã€ç§»é™¤åœç”¨è©ï¼‰
            var processedQuery = PreprocessQuery(query);

            // 2. PostgreSQL FTS æŸ¥è©¢ï¼ˆä½¿ç”¨ BM25 æ¼”ç®—æ³•ï¼‰
            var results = await _dbContext.DocumentChunks
                .Where(c => c.Document.AgentId == agentId)
                .Select(c => new
                {
                    Chunk = c,
                    Rank = EF.Functions.FullTextSearchRank(
                        c.ContentVector,
                        EF.Functions.ToTsQuery("english", processedQuery))
                })
                .Where(x => x.Rank > 0)
                .OrderByDescending(x => x.Rank)
                .Take(options.TopK)
                .ToListAsync(cancellationToken);

            // 3. è½‰æ›ç‚º DTO
            var searchResults = results.Select(r => new SearchResultDto
            {
                ChunkId = r.Chunk.Id,
                DocumentId = r.Chunk.DocumentId,
                Content = r.Chunk.Content,
                Score = r.Rank,
                ChunkIndex = r.Chunk.ChunkIndex,
                Metadata = new Dictionary<string, object>
                {
                    ["document_id"] = r.Chunk.DocumentId.ToString(),
                    ["chunk_index"] = r.Chunk.ChunkIndex
                }
            }).ToList();

            _logger.LogInformation(
                "Keyword search completed, found {Count} results",
                searchResults.Count);

            return searchResults;
        }
        catch (Exception ex)
        {
            _logger.LogError(ex, "Keyword search failed for Agent {AgentId}", agentId);
            throw new SearchException("é—œéµå­—æœå°‹å¤±æ•—", ex);
        }
    }

    private string PreprocessQuery(string query)
    {
        // 1. åˆ†è©
        var words = query.Split(' ', StringSplitOptions.RemoveEmptyEntries);

        // 2. ç§»é™¤åœç”¨è©
        var stopWords = new HashSet<string> { "the", "a", "an", "is", "are", "of", "in" };
        var filteredWords = words.Where(w => !stopWords.Contains(w.ToLower()));

        // 3. çµ„åˆç‚º tsquery æ ¼å¼
        return string.Join(" & ", filteredWords);
    }
}
```

**PostgreSQL FTS ç´¢å¼•é…ç½®**:
```sql
-- Migration: Add Full-Text Search Index
ALTER TABLE document_chunks
ADD COLUMN content_vector tsvector
GENERATED ALWAYS AS (to_tsvector('english', content)) STORED;

CREATE INDEX idx_document_chunks_content_fts
ON document_chunks USING GIN (content_vector);
```

**åƒè€ƒæ–‡æª”**:
- ğŸ“ [Keyword Search](../../../docs/technical-implementation/01-backend-net9/16-keyword-search.md)
- ğŸ“š [PostgreSQL Full-Text Search](https://www.postgresql.org/docs/current/textsearch.html)
- ğŸ“š [BM25 Algorithm](https://en.wikipedia.org/wiki/Okapi_BM25)

---

##### US 4.2.3: æ··åˆæª¢ç´¢ (Hybrid Search) + Re-ranking

**IHybridSearchService ä»‹é¢è¨­è¨ˆ**:
```csharp
// Application/Interfaces/IHybridSearchService.cs
public interface IHybridSearchService
{
    /// <summary>
    /// åŸ·è¡Œæ··åˆæª¢ç´¢ï¼ˆSemantic + Keyword + Re-rankingï¼‰
    /// </summary>
    Task<IReadOnlyList<SearchResultDto>> SearchAsync(
        string query,
        Guid agentId,
        HybridSearchOptions options,
        CancellationToken cancellationToken = default);
}

// Application/RAG/Models/HybridSearchOptions.cs
public sealed class HybridSearchOptions
{
    public int TopK { get; init; } = 10;
    public float SemanticWeight { get; init; } = 0.7f;
    public float KeywordWeight { get; init; } = 0.3f;
    public bool EnableReranking { get; init; } = true;
    public float SemanticScoreThreshold { get; init; } = 0.7f;
}
```

**Hybrid Search Service å¯¦ä½œ**:
```csharp
// Infrastructure/Services/HybridSearchService.cs
public sealed class HybridSearchService : IHybridSearchService
{
    private readonly ISemanticSearchService _semanticSearch;
    private readonly IKeywordSearchService _keywordSearch;
    private readonly IReranker _reranker;
    private readonly ILogger<HybridSearchService> _logger;

    public async Task<IReadOnlyList<SearchResultDto>> SearchAsync(
        string query,
        Guid agentId,
        HybridSearchOptions options,
        CancellationToken cancellationToken = default)
    {
        _logger.LogInformation(
            "Performing hybrid search for Agent {AgentId}, Query: {Query}",
            agentId, query);

        try
        {
            // 1. ä¸¦è¡ŒåŸ·è¡Œèªç¾©æœå°‹å’Œé—œéµå­—æœå°‹
            var semanticTask = _semanticSearch.SearchAsync(
                query, agentId,
                new SemanticSearchOptions
                {
                    TopK = options.TopK * 2, // å–å…©å€çµæœç”¨æ–¼èåˆ
                    ScoreThreshold = options.SemanticScoreThreshold
                },
                cancellationToken);

            var keywordTask = _keywordSearch.SearchAsync(
                query, agentId,
                new KeywordSearchOptions { TopK = options.TopK * 2 },
                cancellationToken);

            await Task.WhenAll(semanticTask, keywordTask);

            var semanticResults = await semanticTask;
            var keywordResults = await keywordTask;

            _logger.LogInformation(
                "Retrieved {SemanticCount} semantic results and {KeywordCount} keyword results",
                semanticResults.Count, keywordResults.Count);

            // 2. RRF (Reciprocal Rank Fusion) èåˆ
            var fusedResults = FuseResultsWithRRF(
                semanticResults,
                keywordResults,
                options.SemanticWeight,
                options.KeywordWeight,
                options.TopK);

            _logger.LogInformation("Fused results using RRF, total: {Count}", fusedResults.Count);

            // 3. Re-rankingï¼ˆå¯é¸ï¼‰
            if (options.EnableReranking)
            {
                fusedResults = await _reranker.RerankAsync(
                    query, fusedResults, cancellationToken);

                _logger.LogInformation("Re-ranked results using Cross-Encoder");
            }

            return fusedResults.Take(options.TopK).ToList();
        }
        catch (Exception ex)
        {
            _logger.LogError(ex, "Hybrid search failed for Agent {AgentId}", agentId);
            throw new SearchException("æ··åˆæª¢ç´¢å¤±æ•—", ex);
        }
    }

    private List<SearchResultDto> FuseResultsWithRRF(
        IReadOnlyList<SearchResultDto> semanticResults,
        IReadOnlyList<SearchResultDto> keywordResults,
        float semanticWeight,
        float keywordWeight,
        int topK)
    {
        const int k = 60; // RRF å¸¸æ•¸

        // è¨ˆç®—æ¯å€‹çµæœçš„ RRF åˆ†æ•¸
        var scoresDict = new Dictionary<Guid, (SearchResultDto Result, float Score)>();

        // èªç¾©æœå°‹çµæœ
        for (int i = 0; i < semanticResults.Count; i++)
        {
            var result = semanticResults[i];
            var rrfScore = semanticWeight / (k + i + 1);

            if (scoresDict.ContainsKey(result.ChunkId))
            {
                scoresDict[result.ChunkId] = (
                    result,
                    scoresDict[result.ChunkId].Score + rrfScore
                );
            }
            else
            {
                scoresDict[result.ChunkId] = (result, rrfScore);
            }
        }

        // é—œéµå­—æœå°‹çµæœ
        for (int i = 0; i < keywordResults.Count; i++)
        {
            var result = keywordResults[i];
            var rrfScore = keywordWeight / (k + i + 1);

            if (scoresDict.ContainsKey(result.ChunkId))
            {
                scoresDict[result.ChunkId] = (
                    scoresDict[result.ChunkId].Result,
                    scoresDict[result.ChunkId].Score + rrfScore
                );
            }
            else
            {
                scoresDict[result.ChunkId] = (result, rrfScore);
            }
        }

        // æ ¹æ“š RRF åˆ†æ•¸æ’åº
        return scoresDict
            .OrderByDescending(kvp => kvp.Value.Score)
            .Select(kvp => new SearchResultDto
            {
                ChunkId = kvp.Key,
                DocumentId = kvp.Value.Result.DocumentId,
                Content = kvp.Value.Result.Content,
                Score = kvp.Value.Score, // RRF åˆ†æ•¸
                ChunkIndex = kvp.Value.Result.ChunkIndex,
                Metadata = kvp.Value.Result.Metadata
            })
            .Take(topK * 2) // å–å…©å€çµæœä¾› Re-ranking ä½¿ç”¨
            .ToList();
    }
}
```

**Cross-Encoder Re-ranker å¯¦ä½œ**:
```csharp
// Infrastructure/Services/CrossEncoderReranker.cs
public sealed class CrossEncoderReranker : IReranker
{
    private readonly HttpClient _httpClient;
    private readonly ILogger<CrossEncoderReranker> _logger;

    public async Task<List<SearchResultDto>> RerankAsync(
        string query,
        IReadOnlyList<SearchResultDto> results,
        CancellationToken cancellationToken = default)
    {
        if (!results.Any()) return new List<SearchResultDto>();

        try
        {
            // å‘¼å« HuggingFace API æˆ–æœ¬åœ°æ¨¡å‹
            var requestPayload = new
            {
                inputs = new
                {
                    query,
                    passages = results.Select(r => r.Content).ToArray()
                }
            };

            var response = await _httpClient.PostAsJsonAsync(
                "https://api-inference.huggingface.co/models/cross-encoder/ms-marco-MiniLM-L-6-v2",
                requestPayload,
                cancellationToken);

            response.EnsureSuccessStatusCode();

            var scores = await response.Content.ReadFromJsonAsync<float[]>(cancellationToken);

            // æ ¹æ“š Cross-Encoder åˆ†æ•¸é‡æ–°æ’åº
            var rankedResults = results
                .Select((result, index) => new SearchResultDto
                {
                    ChunkId = result.ChunkId,
                    DocumentId = result.DocumentId,
                    Content = result.Content,
                    Score = scores![index], // Cross-Encoder åˆ†æ•¸
                    ChunkIndex = result.ChunkIndex,
                    Metadata = result.Metadata
                })
                .OrderByDescending(r => r.Score)
                .ToList();

            _logger.LogInformation("Re-ranked {Count} results using Cross-Encoder", rankedResults.Count);

            return rankedResults;
        }
        catch (Exception ex)
        {
            _logger.LogWarning(ex, "Re-ranking failed, returning original order");
            return results.ToList();
        }
    }
}
```

**åƒè€ƒæ–‡æª”**:
- ğŸ“ [Hybrid Search](../../../docs/technical-implementation/01-backend-net9/17-hybrid-search.md)
- ğŸ“ [Re-ranking Strategy](../../../docs/technical-implementation/01-backend-net9/18-reranking.md)
- ğŸ“š [RRF Algorithm](https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf)
- ğŸ“š [Cross-Encoder Models](https://www.sbert.net/examples/applications/cross-encoder/README.html)

---

##### US 4.2.4: æª¢ç´¢æº–ç¢ºç‡æ¸¬è©¦æ¡†æ¶

**æ¸¬è©¦é›†æ ¼å¼**:
```json
// test-dataset.json
{
  "dataset_name": "agent_knowledge_qa",
  "version": "1.0",
  "questions": [
    {
      "id": "q001",
      "question": "å¦‚ä½•è¨­å®š Agent çš„ Persona é…ç½®ï¼Ÿ",
      "expected_answer": "é€é Persona é…ç½®æª”ï¼ˆJSON/YAMLï¼‰è¨­å®šè§’è‰²ã€æºé€šé¢¨æ ¼ã€å°ˆæ¥­é ˜åŸŸç­‰",
      "expected_sources": ["persona-config-guide.md", "api-documentation.pdf"],
      "expected_chunk_ids": ["chunk_uuid_1", "chunk_uuid_2"],
      "difficulty": "easy"
    },
    {
      "id": "q002",
      "question": "Plugin ç†±é‡è¼‰çš„åŸç†æ˜¯ä»€éº¼ï¼Ÿ",
      "expected_answer": "ä½¿ç”¨ AssemblyLoadContext å¯¦ç¾å‹•æ…‹è¼‰å…¥èˆ‡å¸è¼‰",
      "expected_sources": ["plugin-architecture.md"],
      "difficulty": "hard"
    }
  ]
}
```

**è©•ä¼°æŒ‡æ¨™è¨ˆç®—**:
```csharp
// Infrastructure/Services/RAGEvaluationService.cs
public sealed class RAGEvaluationService : IRAGEvaluationService
{
    private readonly IHybridSearchService _hybridSearch;
    private readonly ILogger<RAGEvaluationService> _logger;

    public async Task<EvaluationReport> EvaluateAsync(
        string testDatasetPath,
        Guid agentId,
        HybridSearchOptions searchOptions,
        CancellationToken cancellationToken = default)
    {
        // 1. è¼‰å…¥æ¸¬è©¦é›†
        var dataset = await LoadTestDatasetAsync(testDatasetPath);

        var totalQuestions = dataset.Questions.Count;
        var correctResults = 0;
        var mrrSum = 0.0;
        var ndcgSum = 0.0;

        foreach (var question in dataset.Questions)
        {
            // 2. åŸ·è¡Œæœå°‹
            var searchResults = await _hybridSearch.SearchAsync(
                question.Question,
                agentId,
                searchOptions,
                cancellationToken);

            // 3. è©•ä¼°çµæœ
            var evaluation = EvaluateQuestionResult(question, searchResults);

            if (evaluation.IsCorrect) correctResults++;
            mrrSum += evaluation.MRR;
            ndcgSum += evaluation.NDCG;

            _logger.LogInformation(
                "Question {Id}: MRR={MRR:F3}, NDCG={NDCG:F3}, Correct={IsCorrect}",
                question.Id, evaluation.MRR, evaluation.NDCG, evaluation.IsCorrect);
        }

        // 4. è¨ˆç®—æ•´é«”æŒ‡æ¨™
        var accuracy = (double)correctResults / totalQuestions * 100;
        var avgMRR = mrrSum / totalQuestions;
        var avgNDCG = ndcgSum / totalQuestions;

        var report = new EvaluationReport
        {
            TotalQuestions = totalQuestions,
            CorrectResults = correctResults,
            Accuracy = accuracy,
            MeanReciprocalRank = avgMRR,
            NormalizedDiscountedCumulativeGain = avgNDCG,
            PassThreshold = accuracy >= 90.0
        };

        _logger.LogInformation(
            "Evaluation completed: Accuracy={Accuracy:F2}%, MRR={MRR:F3}, NDCG={NDCG:F3}",
            accuracy, avgMRR, avgNDCG);

        return report;
    }

    private QuestionEvaluation EvaluateQuestionResult(
        TestQuestion question,
        IReadOnlyList<SearchResultDto> searchResults)
    {
        // MRR: Mean Reciprocal Rank
        double mrr = 0.0;
        for (int i = 0; i < searchResults.Count; i++)
        {
            if (question.ExpectedChunkIds.Contains(searchResults[i].ChunkId))
            {
                mrr = 1.0 / (i + 1);
                break;
            }
        }

        // NDCG: Normalized Discounted Cumulative Gain
        double dcg = 0.0;
        for (int i = 0; i < searchResults.Count; i++)
        {
            int relevance = question.ExpectedChunkIds.Contains(searchResults[i].ChunkId) ? 1 : 0;
            dcg += relevance / Math.Log2(i + 2);
        }

        // IDCG: Ideal DCG
        double idcg = 0.0;
        for (int i = 0; i < Math.Min(question.ExpectedChunkIds.Count, searchResults.Count); i++)
        {
            idcg += 1.0 / Math.Log2(i + 2);
        }

        double ndcg = idcg > 0 ? dcg / idcg : 0.0;

        // åˆ¤å®šç‚ºæ­£ç¢ºï¼šTop-1 æˆ– Top-3 åŒ…å«é æœŸçµæœ
        bool isCorrect = searchResults.Take(3).Any(r =>
            question.ExpectedChunkIds.Contains(r.ChunkId));

        return new QuestionEvaluation
        {
            MRR = mrr,
            NDCG = ndcg,
            IsCorrect = isCorrect
        };
    }

    private async Task<TestDataset> LoadTestDatasetAsync(string path)
    {
        var json = await File.ReadAllTextAsync(path);
        return JsonSerializer.Deserialize<TestDataset>(json)!;
    }
}
```

**Evaluation Report DTO**:
```csharp
// Application/RAG/DTOs/EvaluationReport.cs
public sealed class EvaluationReport
{
    public int TotalQuestions { get; init; }
    public int CorrectResults { get; init; }
    public double Accuracy { get; init; } // ç™¾åˆ†æ¯”
    public double MeanReciprocalRank { get; init; } // MRR
    public double NormalizedDiscountedCumulativeGain { get; init; } // NDCG
    public bool PassThreshold { get; init; } // æ˜¯å¦é”åˆ° 90% æº–ç¢ºç‡
    public DateTime EvaluatedAt { get; init; } = DateTime.UtcNow;
}
```

**åƒè€ƒæ–‡æª”**:
- ğŸ“ [RAG Accuracy Testing](../../../docs/testing/rag-accuracy-testing.md)
- ğŸ“š [Information Retrieval Metrics](https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval))
- ğŸ“š [NDCG Calculation](https://en.wikipedia.org/wiki/Discounted_cumulative_gain)

---

## ç¬¬äºŒéƒ¨åˆ†ï¼šæŠ€è¡“å¯¦æ–½æ–¹æ¡ˆ (How to Build)

**å®Œæ•´æŠ€è¡“æ–‡æª”**: [Backend Technical Guide](../../../docs/technical-implementation/01-backend-net9/README.md)

### Backend å¯¦æ–½

#### Clean Architecture ç›®éŒ„çµæ§‹

**æ¶æ§‹è¨­è¨ˆåƒè€ƒ**:
- ğŸ—ï¸ [ADR-001: Clean Architecture](../../../docs/architecture/adr/ADR-001-clean-architecture.md) - æ¶æ§‹åˆ†å±¤æ±ºç­–
- ğŸ—ï¸ [Monorepo Setup](../../../docs/technical-implementation/01-backend-net9/01-monorepo-setup.md) - å°ˆæ¡ˆçµæ§‹

```
AIAgentPlatform.sln
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ AIAgentPlatform.Domain/                     # é ˜åŸŸå±¤
â”‚   â”‚   â”œâ”€â”€ Entities/
â”‚   â”‚   â”‚   â”œâ”€â”€ Document.cs                          # æ–‡æª”å¯¦é«” (US 4.1)
â”‚   â”‚   â”‚   â””â”€â”€ DocumentChunk.cs                     # Chunk å¯¦é«”
â”‚   â”‚   â”œâ”€â”€ ValueObjects/
â”‚   â”‚   â”‚   â”œâ”€â”€ DocumentMetadata.cs                  # æ–‡æª”å…ƒæ•¸æ“š VO
â”‚   â”‚   â”‚   â””â”€â”€ ChunkingOptions.cs                   # åˆ†å¡Šé¸é … VO
â”‚   â”‚   â”œâ”€â”€ Enums/
â”‚   â”‚   â”‚   â””â”€â”€ DocumentStatus.cs                    # æ–‡æª”ç‹€æ…‹ Enum
â”‚   â”‚   â””â”€â”€ Interfaces/
â”‚   â”‚       â””â”€â”€ IDocumentRepository.cs
â”‚   â”‚
â”‚   â”œâ”€â”€ AIAgentPlatform.Application/                # æ‡‰ç”¨å±¤
â”‚   â”‚   â”œâ”€â”€ Documents/
â”‚   â”‚   â”‚   â”œâ”€â”€ Commands/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ UploadDocument/
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ UploadDocumentCommand.cs
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ UploadDocumentCommandHandler.cs
â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ UploadDocumentCommandValidator.cs
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ DeleteDocument/
â”‚   â”‚   â”‚   â””â”€â”€ Queries/
â”‚   â”‚   â”‚       â”œâ”€â”€ GetDocumentById/
â”‚   â”‚   â”‚       â””â”€â”€ GetDocumentsList/
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ RAG/
â”‚   â”‚   â”‚   â”œâ”€â”€ Commands/
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ PerformHybridSearch/
â”‚   â”‚   â”‚   â”‚       â”œâ”€â”€ HybridSearchCommand.cs
â”‚   â”‚   â”‚   â”‚       â””â”€â”€ HybridSearchCommandHandler.cs
â”‚   â”‚   â”‚   â””â”€â”€ Models/
â”‚   â”‚   â”‚       â”œâ”€â”€ SemanticSearchOptions.cs
â”‚   â”‚   â”‚       â”œâ”€â”€ KeywordSearchOptions.cs
â”‚   â”‚   â”‚       â””â”€â”€ HybridSearchOptions.cs
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ Interfaces/
â”‚   â”‚       â”œâ”€â”€ IDocumentParser.cs
â”‚   â”‚       â”œâ”€â”€ IDocumentChunker.cs
â”‚   â”‚       â”œâ”€â”€ IEmbeddingService.cs
â”‚   â”‚       â”œâ”€â”€ IVectorStoreService.cs
â”‚   â”‚       â”œâ”€â”€ ISemanticSearchService.cs
â”‚   â”‚       â”œâ”€â”€ IKeywordSearchService.cs
â”‚   â”‚       â”œâ”€â”€ IHybridSearchService.cs
â”‚   â”‚       â””â”€â”€ IReranker.cs
â”‚   â”‚
â”‚   â”œâ”€â”€ AIAgentPlatform.Infrastructure/             # åŸºç¤è¨­æ–½å±¤
â”‚   â”‚   â”œâ”€â”€ Persistence/
â”‚   â”‚   â”‚   â”œâ”€â”€ ApplicationDbContext.cs
â”‚   â”‚   â”‚   â”œâ”€â”€ Configurations/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ DocumentConfiguration.cs
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ DocumentChunkConfiguration.cs
â”‚   â”‚   â”‚   â””â”€â”€ Repositories/
â”‚   â”‚   â”‚       â””â”€â”€ DocumentRepository.cs
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ Services/
â”‚   â”‚   â”‚   â”œâ”€â”€ DocumentParsers/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ PdfDocumentParser.cs
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ DocxDocumentParser.cs
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ TextDocumentParser.cs
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ DocumentParserFactory.cs
â”‚   â”‚   â”‚   â”œâ”€â”€ DocumentChunkers/
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ FixedSizeChunker.cs
â”‚   â”‚   â”‚   â”œâ”€â”€ OpenAIEmbeddingService.cs
â”‚   â”‚   â”‚   â”œâ”€â”€ QdrantVectorStoreService.cs
â”‚   â”‚   â”‚   â”œâ”€â”€ SemanticSearchService.cs
â”‚   â”‚   â”‚   â”œâ”€â”€ KeywordSearchService.cs
â”‚   â”‚   â”‚   â”œâ”€â”€ HybridSearchService.cs
â”‚   â”‚   â”‚   â”œâ”€â”€ CrossEncoderReranker.cs
â”‚   â”‚   â”‚   â”œâ”€â”€ BlobStorageService.cs
â”‚   â”‚   â”‚   â””â”€â”€ TikTokenCounter.cs
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ BackgroundJobs/
â”‚   â”‚   â”‚   â””â”€â”€ DocumentProcessingJob.cs
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ Migrations/
â”‚   â”‚       â””â”€â”€ 20260106_AddDocumentAndChunk.cs
â”‚   â”‚
â”‚   â””â”€â”€ AIAgentPlatform.API/                        # API å±¤
â”‚       â”œâ”€â”€ Controllers/
â”‚       â”‚   â”œâ”€â”€ DocumentsController.cs              # æ–‡æª” CRUD API
â”‚       â”‚   â””â”€â”€ RAGController.cs                    # RAG æœå°‹ API
â”‚       â””â”€â”€ Program.cs
â”‚
â””â”€â”€ tests/
    â”œâ”€â”€ AIAgentPlatform.UnitTests/
    â”‚   â”œâ”€â”€ Domain/
    â”‚   â”‚   â”œâ”€â”€ DocumentTests.cs
    â”‚   â”‚   â””â”€â”€ DocumentChunkTests.cs
    â”‚   â”œâ”€â”€ Application/
    â”‚   â”‚   â”œâ”€â”€ Documents/
    â”‚   â”‚   â”‚   â””â”€â”€ UploadDocumentCommandHandlerTests.cs
    â”‚   â”‚   â””â”€â”€ RAG/
    â”‚   â”‚       â””â”€â”€ HybridSearchCommandHandlerTests.cs
    â”‚   â””â”€â”€ Infrastructure/
    â”‚       â”œâ”€â”€ Services/
    â”‚       â”‚   â”œâ”€â”€ PdfDocumentParserTests.cs
    â”‚       â”‚   â”œâ”€â”€ FixedSizeChunkerTests.cs
    â”‚       â”‚   â”œâ”€â”€ OpenAIEmbeddingServiceTests.cs
    â”‚       â”‚   â”œâ”€â”€ QdrantVectorStoreServiceTests.cs
    â”‚       â”‚   â””â”€â”€ HybridSearchServiceTests.cs
    â”‚       â””â”€â”€ Repositories/
    â”‚           â””â”€â”€ DocumentRepositoryTests.cs
    â”‚
    â””â”€â”€ AIAgentPlatform.IntegrationTests/
        â”œâ”€â”€ Documents/
        â”‚   â””â”€â”€ DocumentsControllerTests.cs
        â”œâ”€â”€ RAG/
        â”‚   â”œâ”€â”€ RAGControllerTests.cs
        â”‚   â””â”€â”€ RAGAccuracyTests.cs
        â””â”€â”€ BackgroundJobs/
            â””â”€â”€ DocumentProcessingJobTests.cs
```

**åˆ†å±¤è¨­è¨ˆåƒè€ƒ**:
- ğŸ“ [Domain Layer Design](../../../docs/architecture/layered-architecture/Domain-Layer.md)
- ğŸ“ [Application Layer Design](../../../docs/architecture/layered-architecture/Application-Layer.md)
- ğŸ“ [Infrastructure Layer Design](../../../docs/architecture/layered-architecture/Infrastructure-Layer.md)
- ğŸ“ [API Layer Design](../../../docs/architecture/layered-architecture/API-Layer.md)

---

#### æ ¸å¿ƒå¯¦æ–½ç­–ç•¥

**æ¶æ§‹æ¨¡å¼åƒè€ƒ**:
- ğŸ“š [Architecture Design Document](../../../docs/architecture/Architecture-Design-Document.md) - å®Œæ•´æ¶æ§‹è¨­è¨ˆ

**1. CQRS with MediatR**:
- Commands: UploadDocument, DeleteDocument, ProcessDocument (å¯«å…¥æ“ä½œ)
- Queries: GetDocumentById, GetDocumentsList, HybridSearch (è®€å–æ“ä½œ)
- å¥½è™•: é—œæ³¨é»åˆ†é›¢, æ˜“æ–¼æ¸¬è©¦, æ”¯æ´èƒŒæ™¯ä»»å‹™

**åƒè€ƒ**:
- ğŸ“ [ADR-002: CQRS Pattern](../../../docs/architecture/adr/ADR-002-cqrs-pattern.md) - CQRS æ¶æ§‹æ±ºç­–
- ğŸ“ [CQRS Implementation](../../../docs/technical-implementation/01-backend-net9/05-cqrs-implementation.md) - MediatR å¯¦ä½œæŒ‡å—

**2. Repository Pattern**:
- DocumentRepository: å°è£ Document å’Œ DocumentChunk è³‡æ–™å­˜å–
- æ”¯æ´ PostgreSQL å’Œ Qdrant é›™è³‡æ–™ä¾†æº
- å¥½è™•: éš”é›¢æ¥­å‹™é‚è¼¯èˆ‡è³‡æ–™å­˜å–, æ˜“æ–¼æ¸¬è©¦

**åƒè€ƒ**:
- ğŸ“ [ADR-003: Repository Pattern](../../../docs/architecture/adr/ADR-003-repository-pattern.md) - Repository æ¶æ§‹æ±ºç­–
- ğŸ“ [Data Access Layer](../../../docs/technical-implementation/01-backend-net9/04-data-access-layer.md) - Repository å¯¦ä½œ

**3. Background Jobs (Hangfire)**:
- DocumentProcessingJob: æ–‡æª”è§£æ â†’ åˆ†å¡Š â†’ å‘é‡åŒ– â†’ ç´¢å¼•
- æ”¯æ´é‡è©¦æ©Ÿåˆ¶ï¼ˆæœ€å¤š 3 æ¬¡ï¼‰
- æ”¯æ´ä»»å‹™é€²åº¦è¿½è¹¤

**åƒè€ƒ**:
- ğŸ“ [Background Jobs](../../../docs/technical-implementation/01-backend-net9/14-background-jobs.md)
- ğŸ“¦ [Hangfire](https://www.hangfire.io/)

**4. Dependency Injection**:
- æ‰€æœ‰æœå‹™é€é DI å®¹å™¨è¨»å†Š
- ä½¿ç”¨ Interface å®šç¾©å¥‘ç´„
- æ”¯æ´å¤šç¨®å¯¦ä½œåˆ‡æ›ï¼ˆä¾‹å¦‚ï¼šä¸åŒçš„ Embedding æœå‹™ï¼‰

**DI é…ç½®ç¯„ä¾‹**:
```csharp
// Program.cs - æœå‹™è¨»å†Š
builder.Services.AddScoped<IDocumentRepository, DocumentRepository>();
builder.Services.AddScoped<IDocumentParserFactory, DocumentParserFactory>();
builder.Services.AddScoped<IDocumentParser, PdfDocumentParser>();
builder.Services.AddScoped<IDocumentParser, DocxDocumentParser>();
builder.Services.AddScoped<IDocumentParser, TextDocumentParser>();
builder.Services.AddScoped<IDocumentChunker, FixedSizeChunker>();
builder.Services.AddScoped<IEmbeddingService, OpenAIEmbeddingService>();
builder.Services.AddScoped<IVectorStoreService, QdrantVectorStoreService>();
builder.Services.AddScoped<ISemanticSearchService, SemanticSearchService>();
builder.Services.AddScoped<IKeywordSearchService, KeywordSearchService>();
builder.Services.AddScoped<IHybridSearchService, HybridSearchService>();
builder.Services.AddScoped<IReranker, CrossEncoderReranker>();
builder.Services.AddScoped<IBlobStorageService, AzureBlobStorageService>();
builder.Services.AddScoped<ITokenCounter, TikTokenCounter>();

// Qdrant Client è¨»å†Š
builder.Services.AddSingleton<QdrantClient>(sp =>
{
    var config = sp.GetRequiredService<IConfiguration>();
    var endpoint = config["Qdrant:Endpoint"]!;
    return new QdrantClient(endpoint);
});

// Azure OpenAI Client è¨»å†Š
builder.Services.AddSingleton<OpenAIClient>(sp =>
{
    var config = sp.GetRequiredService<IConfiguration>();
    var endpoint = config["AzureOpenAI:Endpoint"]!;
    var apiKey = config["AzureOpenAI:ApiKey"]!;
    return new OpenAIClient(new Uri(endpoint), new AzureKeyCredential(apiKey));
});
```

---

### é–‹ç™¼é€±è¨ˆåŠƒ (Week by Week)

#### Week 1: US 4.1 - æ–‡æª”ä¸Šå‚³èˆ‡è™•ç† (Day 1-5)

**Day 1: Domain Layer + Infrastructure Setup**
- å»ºç«‹ Document Entity å’Œ DocumentChunk Entity
- å»ºç«‹ DocumentMetadata Value Object
- å»ºç«‹ DocumentStatus Enum
- EF Core Migration: Add Document and DocumentChunk tables
- **åƒè€ƒ**: [Domain Layer Design](../../../docs/architecture/layered-architecture/Domain-Layer.md)

**Day 2: æ–‡æª”ä¸Šå‚³ API**
- å¯¦ä½œ UploadDocumentCommand + Handler
- å¯¦ä½œ FluentValidation é©—è­‰å™¨
- å¯¦ä½œ DocumentsController (Upload endpoint)
- Azure Blob Storage æ•´åˆ
- **åƒè€ƒ**: [Document Upload API](../../../docs/api/knowledge-api-design.md#upload)

**Day 3: æ–‡æª”è§£æå¼•æ“**
- å¯¦ä½œ IDocumentParser ä»‹é¢
- å¯¦ä½œ PdfDocumentParser
- å¯¦ä½œ DocxDocumentParser
- å¯¦ä½œ TextDocumentParser
- å¯¦ä½œ DocumentParserFactory
- **åƒè€ƒ**: [Document Parsing Strategy](../../../docs/technical-implementation/01-backend-net9/10-document-parsing.md)

**Day 4: æ–‡æª”åˆ†å¡Šèˆ‡å‘é‡åŒ–**
- å¯¦ä½œ IDocumentChunker ä»‹é¢
- å¯¦ä½œ FixedSizeChunker
- å¯¦ä½œ TikTokenCounter
- å¯¦ä½œ IEmbeddingService ä»‹é¢
- å¯¦ä½œ OpenAIEmbeddingServiceï¼ˆå«å¿«å–ï¼‰
- **åƒè€ƒ**: [Chunking Strategies](../../../docs/technical-implementation/01-backend-net9/11-chunking-strategies.md)
- **åƒè€ƒ**: [Embedding Service](../../../docs/technical-implementation/01-backend-net9/12-embedding-service.md)

**Day 5: Qdrant æ•´åˆèˆ‡èƒŒæ™¯ä»»å‹™**
- å¯¦ä½œ IVectorStoreService ä»‹é¢
- å¯¦ä½œ QdrantVectorStoreService
- å¯¦ä½œ DocumentProcessingJobï¼ˆHangfireï¼‰
- Hangfire Dashboard é…ç½®
- **åƒè€ƒ**: [Qdrant Design](../../../docs/database/qdrant-design.md)
- **åƒè€ƒ**: [Background Jobs](../../../docs/technical-implementation/01-backend-net9/14-background-jobs.md)

---

#### Week 2: US 4.2 - RAG æª¢ç´¢ç­–ç•¥ (Day 6-10)

**Day 6: èªç¾©æœå°‹**
- å¯¦ä½œ ISemanticSearchService ä»‹é¢
- å¯¦ä½œ SemanticSearchService
- å¯¦ä½œ SemanticSearchOptions
- æ¸¬è©¦å‘é‡æœå°‹æµç¨‹
- **åƒè€ƒ**: [Semantic Search](../../../docs/technical-implementation/01-backend-net9/15-semantic-search.md)

**Day 7: é—œéµå­—æœå°‹**
- PostgreSQL FTS Migrationï¼ˆAdd tsvector columnï¼‰
- å¯¦ä½œ IKeywordSearchService ä»‹é¢
- å¯¦ä½œ KeywordSearchServiceï¼ˆBM25ï¼‰
- æ¸¬è©¦ FTS æŸ¥è©¢æ•ˆèƒ½
- **åƒè€ƒ**: [Keyword Search](../../../docs/technical-implementation/01-backend-net9/16-keyword-search.md)

**Day 8: æ··åˆæª¢ç´¢**
- å¯¦ä½œ IHybridSearchService ä»‹é¢
- å¯¦ä½œ HybridSearchServiceï¼ˆRRF èåˆï¼‰
- å¯¦ä½œ HybridSearchOptions
- æ¸¬è©¦ Hybrid Search æº–ç¢ºç‡
- **åƒè€ƒ**: [Hybrid Search](../../../docs/technical-implementation/01-backend-net9/17-hybrid-search.md)

**Day 9: Re-ranking**
- å¯¦ä½œ IReranker ä»‹é¢
- å¯¦ä½œ CrossEncoderRerankerï¼ˆHuggingFace APIï¼‰
- æ•´åˆ Re-ranking åˆ° HybridSearchService
- æ¸¬è©¦ Re-ranking æ•ˆæœ
- **åƒè€ƒ**: [Re-ranking Strategy](../../../docs/technical-implementation/01-backend-net9/18-reranking.md)

**Day 10: RAG API èˆ‡æ¸¬è©¦æ¡†æ¶**
- å¯¦ä½œ RAGControllerï¼ˆSearch endpointï¼‰
- å¯¦ä½œ RAGEvaluationService
- å»ºç«‹æ¨™æº–æ¸¬è©¦é›†ï¼ˆtest-dataset.jsonï¼‰
- åŸ·è¡Œæº–ç¢ºç‡è©•ä¼°ï¼ˆç›®æ¨™ â‰¥90%ï¼‰
- **åƒè€ƒ**: [RAG Accuracy Testing](../../../docs/testing/rag-accuracy-testing.md)

---

#### Week 3: æ•´åˆæ¸¬è©¦èˆ‡å„ªåŒ– (Day 11-15)

**Day 11: å–®å…ƒæ¸¬è©¦**
- DocumentTests, DocumentChunkTests
- UploadDocumentCommandHandlerTests
- PdfDocumentParserTests, DocxDocumentParserTests
- FixedSizeChunkerTests
- OpenAIEmbeddingServiceTests
- **åƒè€ƒ**: [Unit Testing Strategy](../../../docs/testing/unit-testing-strategy.md)

**Day 12: æ•´åˆæ¸¬è©¦**
- DocumentsControllerTestsï¼ˆAPI ç«¯é»æ¸¬è©¦ï¼‰
- RAGControllerTestsï¼ˆæœå°‹ API æ¸¬è©¦ï¼‰
- DocumentProcessingJobTestsï¼ˆèƒŒæ™¯ä»»å‹™æ¸¬è©¦ï¼‰
- QdrantVectorStoreServiceTestsï¼ˆQdrant æ•´åˆæ¸¬è©¦ï¼‰
- **åƒè€ƒ**: [Integration Testing](../../../docs/testing/integration-testing.md)

**Day 13: æ•ˆèƒ½å„ªåŒ–**
- Qdrant ç´¢å¼•å„ªåŒ–ï¼ˆHNSW åƒæ•¸èª¿æ•´ï¼‰
- Embedding å¿«å–ç­–ç•¥å„ªåŒ–
- PostgreSQL FTS ç´¢å¼•å„ªåŒ–
- Batch Embedding ä¸¦è¡Œè™•ç†
- **åƒè€ƒ**: [Performance Optimization](../../../docs/technical-implementation/01-backend-net9/19-performance-optimization.md)

**Day 14: æ–‡æª”èˆ‡ Swagger**
- API æ–‡æª”å®Œå–„ï¼ˆSwagger è¨»è§£ï¼‰
- README æ›´æ–°ï¼ˆRAG ä½¿ç”¨æŒ‡å—ï¼‰
- æŠ€è¡“æ–‡æª”å®Œå–„
- ç¨‹å¼ç¢¼è¨»è§£è£œå……
- **åƒè€ƒ**: [API Documentation Standards](../../../docs/api/api-documentation-standards.md)

**Day 15: æœ€çµ‚é©—æ”¶èˆ‡ç™¼å¸ƒ**
- åŸ·è¡Œå®Œæ•´æ¸¬è©¦å¥—ä»¶ï¼ˆUnit + Integrationï¼‰
- åŸ·è¡Œ RAG æº–ç¢ºç‡è©•ä¼°ï¼ˆç¢ºèª â‰¥90%ï¼‰
- Code Review
- Sprint 5 Retrospective
- æº–å‚™ Sprint 6 è¦åŠƒ

---

## ç¬¬ä¸‰éƒ¨åˆ†ï¼šç·¨ç¢¼è¦ç¯„

**å®Œæ•´ç·¨ç¢¼è¦ç¯„**: [C# Coding Standards](../../../docs/technical-implementation/01-backend-net9/02-coding-standards.md)

### 1. å‘½åè¦ç¯„

**é¡åˆ¥èˆ‡ä»‹é¢**:
```csharp
// âœ… æ­£ç¢º
public sealed class DocumentProcessingJob { }
public interface IDocumentParser { }

// âŒ éŒ¯èª¤
public class documentprocessingjob { }
public interface DocumentParser { }
```

**æ–¹æ³•èˆ‡å±¬æ€§**:
```csharp
// âœ… æ­£ç¢º
public async Task<Result> ProcessDocumentAsync(Guid documentId, CancellationToken cancellationToken)
{
    // ...
}

public string FileName { get; private set; }

// âŒ éŒ¯èª¤
public async Task<Result> process_document(Guid documentId) { }
public string filename;
```

**å€åŸŸè®Šæ•¸èˆ‡åƒæ•¸**:
```csharp
// âœ… æ­£ç¢º
var documentId = Guid.NewGuid();
var chunkingOptions = new ChunkingOptions();

// âŒ éŒ¯èª¤
var DocumentId = Guid.NewGuid();
var ChunkingOptions = new ChunkingOptions();
```

---

### 2. ç¨‹å¼ç¢¼çµ„ç¹”

**æª”æ¡ˆçµæ§‹**:
```csharp
// âœ… æ­£ç¢ºçš„æª”æ¡ˆçµæ§‹
namespace AIAgentPlatform.Application.Documents.Commands.UploadDocument;

// 1. Usings
using AIAgentPlatform.Domain.Entities;
using MediatR;

// 2. Namespace å®£å‘Š
namespace AIAgentPlatform.Application.Documents.Commands.UploadDocument;

// 3. é¡åˆ¥å®šç¾©
public sealed record UploadDocumentCommand : IRequest<Result<UploadDocumentResponse>>
{
    public required Guid AgentId { get; init; }
    public required IFormFileCollection Files { get; init; }
}

// 4. Handler å®šç¾©
public sealed class UploadDocumentCommandHandler
    : IRequestHandler<UploadDocumentCommand, Result<UploadDocumentResponse>>
{
    private readonly IDocumentRepository _documentRepo;
    private readonly IBlobStorageService _blobStorage;

    public UploadDocumentCommandHandler(
        IDocumentRepository documentRepo,
        IBlobStorageService blobStorage)
    {
        _documentRepo = documentRepo;
        _blobStorage = blobStorage;
    }

    public async Task<Result<UploadDocumentResponse>> Handle(
        UploadDocumentCommand request,
        CancellationToken cancellationToken)
    {
        // å¯¦ä½œ
    }
}
```

---

### 3. éŒ¯èª¤è™•ç†

**ä½¿ç”¨ Result Pattern**:
```csharp
// âœ… æ­£ç¢º
public async Task<Result<Document>> GetDocumentAsync(Guid id, CancellationToken cancellationToken)
{
    var document = await _documentRepo.GetByIdAsync(id, cancellationToken);

    if (document == null)
    {
        return Result<Document>.Failure($"Document not found: {id}");
    }

    return Result<Document>.Success(document);
}

// âŒ éŒ¯èª¤ï¼šç›´æ¥æ‹‹å‡ºä¾‹å¤–
public async Task<Document> GetDocumentAsync(Guid id)
{
    var document = await _documentRepo.GetByIdAsync(id);

    if (document == null)
    {
        throw new NotFoundException($"Document not found: {id}");
    }

    return document;
}
```

**è‡ªè¨‚ä¾‹å¤–**:
```csharp
// âœ… æ­£ç¢º
public sealed class DocumentParseException : Exception
{
    public DocumentParseException(string message, Exception? innerException = null)
        : base(message, innerException)
    {
    }
}

// ä½¿ç”¨
throw new DocumentParseException($"Failed to parse PDF: {fileName}", ex);
```

---

### 4. éåŒæ­¥ç¨‹å¼è¨­è¨ˆ

**Async/Await è¦ç¯„**:
```csharp
// âœ… æ­£ç¢º
public async Task<IReadOnlyList<SearchResult>> SearchAsync(
    string query,
    CancellationToken cancellationToken = default)
{
    var embedding = await _embeddingService.GenerateEmbeddingAsync(query, cancellationToken);
    var results = await _vectorStore.SearchAsync(embedding, cancellationToken);
    return results;
}

// âŒ éŒ¯èª¤ï¼šç¼ºå°‘ CancellationToken
public async Task<IReadOnlyList<SearchResult>> SearchAsync(string query)
{
    var embedding = await _embeddingService.GenerateEmbeddingAsync(query);
    var results = await _vectorStore.SearchAsync(embedding);
    return results;
}
```

**ConfigureAwait**:
```csharp
// âœ… æ­£ç¢ºï¼ˆLibrary ç¨‹å¼ç¢¼ï¼‰
public async Task<Document> GetDocumentAsync(Guid id, CancellationToken cancellationToken)
{
    return await _dbContext.Documents
        .FirstOrDefaultAsync(d => d.Id == id, cancellationToken)
        .ConfigureAwait(false);
}

// âœ… æ­£ç¢ºï¼ˆAPI Controller å¯çœç•¥ï¼‰
public async Task<IActionResult> GetDocument(Guid id, CancellationToken cancellationToken)
{
    var document = await _mediator.Send(new GetDocumentByIdQuery { Id = id }, cancellationToken);
    return Ok(document);
}
```

---

### 5. Dependency Injection

**å»ºæ§‹å­æ³¨å…¥**:
```csharp
// âœ… æ­£ç¢º
public sealed class DocumentProcessingJob : IDocumentProcessingJob
{
    private readonly IDocumentRepository _documentRepo;
    private readonly IBlobStorageService _blobStorage;
    private readonly IDocumentParserFactory _parserFactory;
    private readonly ILogger<DocumentProcessingJob> _logger;

    public DocumentProcessingJob(
        IDocumentRepository documentRepo,
        IBlobStorageService blobStorage,
        IDocumentParserFactory parserFactory,
        ILogger<DocumentProcessingJob> logger)
    {
        _documentRepo = documentRepo;
        _blobStorage = blobStorage;
        _parserFactory = parserFactory;
        _logger = logger;
    }
}

// âŒ éŒ¯èª¤ï¼šService Locator Anti-Pattern
public sealed class DocumentProcessingJob
{
    public DocumentProcessingJob(IServiceProvider serviceProvider)
    {
        _documentRepo = serviceProvider.GetRequiredService<IDocumentRepository>();
    }
}
```

---

### 6. Logging

**Structured Logging**:
```csharp
// âœ… æ­£ç¢º
_logger.LogInformation(
    "Document processing started for ID: {DocumentId}, Agent: {AgentId}",
    documentId, agentId);

_logger.LogWarning(
    "Document parsing took longer than expected: {Duration}ms for file {FileName}",
    duration, fileName);

_logger.LogError(ex,
    "Failed to generate embedding for document {DocumentId}",
    documentId);

// âŒ éŒ¯èª¤ï¼šå­—ä¸²æ’å€¼
_logger.LogInformation($"Document processing started for ID: {documentId}");
```

---

### 7. è¨»è§£èˆ‡æ–‡æª”

**XML æ–‡æª”è¨»è§£**:
```csharp
/// <summary>
/// è§£ææ–‡æª”ä¸¦æå–ç´”æ–‡å­—å…§å®¹å’Œå…ƒæ•¸æ“š
/// </summary>
/// <param name="fileStream">æ–‡æª”æª”æ¡ˆæµ</param>
/// <param name="fileName">æ–‡æª”æª”æ¡ˆåç¨±</param>
/// <param name="cancellationToken">å–æ¶ˆä»¤ç‰Œ</param>
/// <returns>åŒ…å«å…§å®¹å’Œå…ƒæ•¸æ“šçš„è§£æçµæœ</returns>
/// <exception cref="DocumentParseException">ç•¶æ–‡æª”è§£æå¤±æ•—æ™‚æ‹‹å‡º</exception>
Task<DocumentParseResult> ParseAsync(
    Stream fileStream,
    string fileName,
    CancellationToken cancellationToken = default);
```

---

## ç¬¬å››éƒ¨åˆ†ï¼šè³ªé‡ä¿è­‰

**å®Œæ•´æ¸¬è©¦ç­–ç•¥**: [Testing Strategy](../../../docs/testing/testing-strategy.md)

### 1. å–®å…ƒæ¸¬è©¦

**æ¸¬è©¦è¦†è“‹ç‡ç›®æ¨™**: â‰¥80% (Domain + Application å±¤)

**æ¸¬è©¦ç¯„ä¾‹**:
```csharp
// tests/AIAgentPlatform.UnitTests/Infrastructure/Services/FixedSizeChunkerTests.cs
public sealed class FixedSizeChunkerTests
{
    private readonly Mock<ITokenCounter> _tokenCounterMock;
    private readonly FixedSizeChunker _sut;

    public FixedSizeChunkerTests()
    {
        _tokenCounterMock = new Mock<ITokenCounter>();
        _sut = new FixedSizeChunker(_tokenCounterMock.Object, Mock.Of<ILogger<FixedSizeChunker>>());
    }

    [Fact]
    public async Task ChunkAsync_WithSmallContent_ShouldReturnSingleChunk()
    {
        // Arrange
        var content = "This is a short document.";
        var options = new ChunkingOptions { ChunkSize = 1024, OverlapSize = 0 };

        _tokenCounterMock
            .Setup(x => x.CountTokensAsync(It.IsAny<string>()))
            .ReturnsAsync(5);

        // Act
        var chunks = await _sut.ChunkAsync(content, options);

        // Assert
        chunks.Should().HaveCount(1);
        chunks[0].Content.Should().Contain(content);
    }

    [Fact]
    public async Task ChunkAsync_WithLargeContent_ShouldReturnMultipleChunks()
    {
        // Arrange
        var content = string.Join(" ", Enumerable.Repeat("word", 2000));
        var options = new ChunkingOptions { ChunkSize = 512, OverlapSize = 100 };

        _tokenCounterMock
            .Setup(x => x.CountTokensAsync(It.IsAny<string>()))
            .ReturnsAsync((string text) => text.Split(' ').Length);

        // Act
        var chunks = await _sut.ChunkAsync(content, options);

        // Assert
        chunks.Should().HaveCountGreaterThan(1);
        chunks.Should().OnlyContain(c => c.TokenCount <= 512);
    }

    [Fact]
    public async Task ChunkAsync_WithOverlap_ShouldHaveOverlappingContent()
    {
        // Arrange
        var content = string.Join(" ", Enumerable.Repeat("word", 1000));
        var options = new ChunkingOptions { ChunkSize = 300, OverlapSize = 50 };

        _tokenCounterMock
            .Setup(x => x.CountTokensAsync(It.IsAny<string>()))
            .ReturnsAsync((string text) => text.Split(' ').Length);

        // Act
        var chunks = await _sut.ChunkAsync(content, options);

        // Assert
        chunks.Should().HaveCountGreaterThan(1);

        // é©—è­‰ç›¸é„° chunks æœ‰é‡ç–Š
        for (int i = 0; i < chunks.Count - 1; i++)
        {
            var chunk1End = chunks[i].Content.Split(' ').TakeLast(50);
            var chunk2Start = chunks[i + 1].Content.Split(' ').Take(50);
            chunk1End.Intersect(chunk2Start).Should().NotBeEmpty();
        }
    }
}
```

---

### 2. æ•´åˆæ¸¬è©¦

**æ¸¬è©¦ç¯„ä¾‹**:
```csharp
// tests/AIAgentPlatform.IntegrationTests/RAG/RAGAccuracyTests.cs
public sealed class RAGAccuracyTests : IClassFixture<IntegrationTestWebAppFactory>
{
    private readonly IntegrationTestWebAppFactory _factory;
    private readonly IRAGEvaluationService _evaluationService;

    public RAGAccuracyTests(IntegrationTestWebAppFactory factory)
    {
        _factory = factory;
        _evaluationService = _factory.Services.GetRequiredService<IRAGEvaluationService>();
    }

    [Fact]
    public async Task HybridSearch_WithStandardTestSet_ShouldAchieve90PercentAccuracy()
    {
        // Arrange
        var testDatasetPath = "test-data/standard-qa-dataset.json";
        var agentId = await SetupTestAgentWithKnowledgeBase();

        var searchOptions = new HybridSearchOptions
        {
            TopK = 10,
            SemanticWeight = 0.7f,
            KeywordWeight = 0.3f,
            EnableReranking = true,
            SemanticScoreThreshold = 0.7f
        };

        // Act
        var report = await _evaluationService.EvaluateAsync(
            testDatasetPath,
            agentId,
            searchOptions,
            CancellationToken.None);

        // Assert
        report.Accuracy.Should().BeGreaterOrEqualTo(90.0);
        report.MeanReciprocalRank.Should().BeGreaterOrEqualTo(0.8);
        report.PassThreshold.Should().BeTrue();

        _factory.TestOutputHelper.WriteLine($"Accuracy: {report.Accuracy:F2}%");
        _factory.TestOutputHelper.WriteLine($"MRR: {report.MeanReciprocalRank:F3}");
        _factory.TestOutputHelper.WriteLine($"NDCG: {report.NormalizedDiscountedCumulativeGain:F3}");
    }

    private async Task<Guid> SetupTestAgentWithKnowledgeBase()
    {
        // å»ºç«‹æ¸¬è©¦ Agent
        // ä¸Šå‚³æ¸¬è©¦æ–‡æª”
        // ç­‰å¾…èƒŒæ™¯è™•ç†å®Œæˆ
        // è¿”å› Agent ID
        // å¯¦ä½œç•¥...
        return Guid.NewGuid();
    }
}
```

---

### 3. æ•ˆèƒ½æ¸¬è©¦

**æ•ˆèƒ½åŸºæº–**:
- æ–‡æª”ä¸Šå‚³: <2 ç§’ï¼ˆ10MB PDFï¼‰
- æ–‡æª”è§£æ: <5 ç§’ï¼ˆ100 é  PDFï¼‰
- æ–‡æª”åˆ†å¡Š: <1 ç§’ï¼ˆ1000 chunksï¼‰
- Embedding ç”Ÿæˆ: <3 ç§’ï¼ˆæ‰¹æ¬¡ 100 chunksï¼‰
- å‘é‡æœå°‹: <200msï¼ˆ100K å‘é‡ï¼‰
- Hybrid Search: <500msï¼ˆå« Re-rankingï¼‰

**æ•ˆèƒ½æ¸¬è©¦ç¯„ä¾‹**:
```csharp
[Fact]
public async Task QdrantVectorSearch_With100KVectors_ShouldComplete InLessThan200Ms()
{
    // Arrange
    var collectionName = "perf_test_collection";
    await SetupCollectionWith100KVectors(collectionName);

    var queryVector = await _embeddingService.GenerateEmbeddingAsync("test query");
    var stopwatch = Stopwatch.StartNew();

    // Act
    var results = await _vectorStore.SearchAsync(
        collectionName,
        queryVector,
        limit: 10,
        scoreThreshold: 0.7f);

    stopwatch.Stop();

    // Assert
    stopwatch.ElapsedMilliseconds.Should().BeLessThan(200);
    results.Should().HaveCountLessOrEqualTo(10);
}
```

---

### 4. ç¨‹å¼ç¢¼å¯©æŸ¥æª¢æŸ¥æ¸…å–®

**Domain Layer**:
- [ ] Entity åŒ…å«æ˜ç¢ºçš„æ¥­å‹™è¦å‰‡
- [ ] Value Object å¯¦ä½œ Equality æ¯”è¼ƒ
- [ ] Enum ä½¿ç”¨æœ‰æ„ç¾©çš„åç¨±
- [ ] æ²’æœ‰å¤–éƒ¨ä¾è³´ï¼ˆç´” Domain é‚è¼¯ï¼‰

**Application Layer**:
- [ ] Command/Query ä½¿ç”¨ MediatR
- [ ] Handler åŒ…å«å®Œæ•´çš„é©—è­‰é‚è¼¯
- [ ] FluentValidation è¦å‰‡å®Œæ•´
- [ ] DTO èˆ‡ Entity åˆ†é›¢

**Infrastructure Layer**:
- [ ] Repository å¯¦ä½œæ­£ç¢ºçš„ Interface
- [ ] EF Core Configuration ä½¿ç”¨ Fluent API
- [ ] å¤–éƒ¨æœå‹™ä½¿ç”¨ Interface æŠ½è±¡
- [ ] éŒ¯èª¤è™•ç†å®Œæ•´ï¼ˆtry-catch + loggingï¼‰

**API Layer**:
- [ ] Controller åƒ…è² è²¬è·¯ç”±å’Œå›æ‡‰
- [ ] ä½¿ç”¨ MediatR å‘¼å« Application Layer
- [ ] åŒ…å« Swagger è¨»è§£
- [ ] å›æ‡‰æ ¼å¼çµ±ä¸€ï¼ˆResult Patternï¼‰

---

## ç¬¬äº”éƒ¨åˆ†ï¼šåƒè€ƒæ–‡æª”

### æ ¸å¿ƒè¦åŠƒæ–‡æª”

**è¦åŠƒæ–‡æª”**:
- ğŸ“‹ [MVP Scope Definition](../../1-planning/MVP-SCOPE-DEFINITION.md) - Phase 1A ç¯„åœå®šç¾©
- ğŸ“Š [Sprint Allocation Analysis](../../1-planning/SPRINT-ALLOCATION-ANALYSIS.md#sprint-5) - Sprint 5 è©³ç´°åˆ†æ (US 4.1-4.2, 13 SP)
- ğŸ¯ [Development Strategy](../../1-planning/DEVELOPMENT-STRATEGY.md) - Knowledge RAG ç³»çµ±é–‹ç™¼ç­–ç•¥
- ğŸ“ [Architecture Design Document](../../../docs/architecture/Architecture-Design-Document.md) - Clean Architecture ç¸½è¦½

**Sprint 5 æ–‡æª”**:
- ğŸ“– [SPRINT-5-1-OVERVIEW.md](./SPRINT-5-1-OVERVIEW.md) - Sprint 5 æ¦‚è¦½
- ğŸ“– [SPRINT-5-3-CONTEXT.md](./SPRINT-5-3-CONTEXT.md) - Sprint 5 æŠ€è¡“èƒŒæ™¯
- ğŸ“– [SPRINT-5-4-CHECKLIST.md](./SPRINT-5-4-CHECKLIST.md) - Sprint 5 ä»»å‹™æ¸…å–®

---

### User Stories

**Knowledge Management (Module 05)**:
- ğŸ“– [US 5.1 - çŸ¥è­˜åº«æ–‡ä»¶ä¸Šå‚³èˆ‡è™•ç†](../../../docs/user-stories/modules/module-05-agent-memory.md#us-51)
- ğŸ“– [US 5.2 - ç²¾ç¢ºæª¢ç´¢ç­–ç•¥é…ç½®](../../../docs/user-stories/modules/module-05-agent-memory.md#us-52)

---

### æ¶æ§‹è¨­è¨ˆ ADR

**æ ¸å¿ƒæ¶æ§‹æ±ºç­–**:
- ğŸ—ï¸ [ADR-001: Clean Architecture](../../../docs/architecture/adr/ADR-001-clean-architecture.md) - æ¶æ§‹åˆ†å±¤æ±ºç­–
- ğŸ—ï¸ [ADR-002: CQRS Pattern](../../../docs/architecture/adr/ADR-002-cqrs-pattern.md) - CQRS æ¶æ§‹æ±ºç­–
- ğŸ—ï¸ [ADR-003: Repository Pattern](../../../docs/architecture/adr/ADR-003-repository-pattern.md) - Repository æ¶æ§‹æ±ºç­–
- ğŸ—ï¸ [ADR-006: Hybrid State Management](../../../docs/architecture/adr/ADR-006-hybrid-state-management.md) - Redis + PostgreSQL

---

### æŠ€è¡“å¯¦ä½œæŒ‡å—

**Backend æŠ€è¡“å¯¦ä½œ**:
- ğŸ“ [10-document-parsing.md](../../../docs/technical-implementation/01-backend-net9/10-document-parsing.md) - æ–‡æª”è§£æç­–ç•¥
- ğŸ“ [11-chunking-strategies.md](../../../docs/technical-implementation/01-backend-net9/11-chunking-strategies.md) - åˆ†å¡Šç­–ç•¥
- ğŸ“ [12-embedding-service.md](../../../docs/technical-implementation/01-backend-net9/12-embedding-service.md) - Embedding æœå‹™
- ğŸ“ [13-azure-storage.md](../../../docs/technical-implementation/01-backend-net9/13-azure-storage.md) - Azure Blob Storage
- ğŸ“ [14-background-jobs.md](../../../docs/technical-implementation/01-backend-net9/14-background-jobs.md) - èƒŒæ™¯ä»»å‹™ï¼ˆHangfireï¼‰
- ğŸ“ [15-semantic-search.md](../../../docs/technical-implementation/01-backend-net9/15-semantic-search.md) - èªç¾©æœå°‹
- ğŸ“ [16-keyword-search.md](../../../docs/technical-implementation/01-backend-net9/16-keyword-search.md) - é—œéµå­—æœå°‹ï¼ˆBM25ï¼‰
- ğŸ“ [17-hybrid-search.md](../../../docs/technical-implementation/01-backend-net9/17-hybrid-search.md) - æ··åˆæª¢ç´¢ï¼ˆRRFï¼‰
- ğŸ“ [18-reranking.md](../../../docs/technical-implementation/01-backend-net9/18-reranking.md) - Re-ranking ç­–ç•¥
- ğŸ“ [19-performance-optimization.md](../../../docs/technical-implementation/01-backend-net9/19-performance-optimization.md) - æ•ˆèƒ½å„ªåŒ–

---

### API è¨­è¨ˆ

**Knowledge API è¨­è¨ˆ**:
- ğŸ“˜ [knowledge-api-design.md](../../../docs/api/knowledge-api-design.md) - Document Upload, Search API
- ğŸ“˜ [api-documentation-standards.md](../../../docs/api/api-documentation-standards.md) - API æ–‡æª”æ¨™æº–

---

### è³‡æ–™åº«è¨­è¨ˆ

**è³‡æ–™åº« Schema**:
- ğŸ—„ï¸ [document-schema.md](../../../docs/database/document-schema.md) - Document & DocumentChunk Tables
- ğŸ—„ï¸ [qdrant-design.md](../../../docs/database/qdrant-design.md) - Qdrant Collection è¨­è¨ˆ

---

### æ¸¬è©¦æ–‡æª”

**æ¸¬è©¦ç­–ç•¥èˆ‡æŒ‡å—**:
- ğŸ§ª [testing-strategy.md](../../../docs/testing/testing-strategy.md) - å®Œæ•´æ¸¬è©¦ç­–ç•¥
- ğŸ§ª [unit-testing-strategy.md](../../../docs/testing/unit-testing-strategy.md) - å–®å…ƒæ¸¬è©¦æŒ‡å—
- ğŸ§ª [integration-testing.md](../../../docs/testing/integration-testing.md) - æ•´åˆæ¸¬è©¦æŒ‡å—
- ğŸ§ª [rag-accuracy-testing.md](../../../docs/testing/rag-accuracy-testing.md) - RAG æº–ç¢ºç‡æ¸¬è©¦

---

### ç›¸é—œ Sprint åƒè€ƒ

**Previous Sprints**:
- ğŸ“– [Sprint 1 - Agent CRUD](../sprint-1/SPRINT-1-2-PLAN.md) - Agent åŸºç¤æ¶æ§‹åƒè€ƒ
- ğŸ“– [Sprint 2 - Execution Engine](../sprint-2/SPRINT-2-2-PLAN.md) - Semantic Kernel æ•´åˆåƒè€ƒ
- ğŸ“– [Sprint 3 - Plugin System](../sprint-3/SPRINT-3-2-PLAN.md) - Plugin ç³»çµ±åƒè€ƒ
- ğŸ“– [Sprint 4 - Persona Framework](../sprint-4/SPRINT-4-2-PLAN.md) - Persona & Prompt åƒè€ƒ

---

### å¤–éƒ¨è³‡æº

**Azure æœå‹™**:
- ğŸ”— [Azure OpenAI Embeddings](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/embeddings)
- ğŸ”— [Azure Blob Storage](https://learn.microsoft.com/en-us/azure/storage/blobs/)

**å‘é‡è³‡æ–™åº«**:
- ğŸ”— [Qdrant Documentation](https://qdrant.tech/documentation/)
- ğŸ”— [HNSW Algorithm Paper](https://arxiv.org/abs/1603.09320)

**RAG æœ€ä½³å¯¦è¸**:
- ğŸ”— [Pinecone RAG Guide](https://www.pinecone.io/learn/retrieval-augmented-generation/)
- ğŸ”— [LangChain RAG Documentation](https://python.langchain.com/docs/use_cases/question_answering/)

**NuGet å¥—ä»¶**:
- ğŸ”— [PdfPig](https://github.com/UglyToad/PdfPig) - PDF è§£æ
- ğŸ”— [DocumentFormat.OpenXml](https://github.com/OfficeDev/Open-XML-SDK) - DOCX è§£æ
- ğŸ”— [TikToken](https://github.com/microsoft/Tokenizer) - Token è¨ˆæ•¸
- ğŸ”— [Hangfire](https://www.hangfire.io/) - èƒŒæ™¯ä»»å‹™

---

## ä½¿ç”¨æŒ‡å—

### å¦‚ä½•ä½¿ç”¨æ­¤æ–‡ä»¶

**é–‹ç™¼éšæ®µä½¿ç”¨**:
1. **é–‹ç™¼å‰**: é–±è®€ã€Œç¬¬ä¸€éƒ¨åˆ†ã€äº†è§£éœ€æ±‚ç¯„åœ
2. **é–‹ç™¼ä¸­**: åƒè€ƒã€Œç¬¬äºŒéƒ¨åˆ†ã€çš„æŠ€è¡“å¯¦ä½œæ–¹æ¡ˆå’Œç¨‹å¼ç¢¼ç¯„ä¾‹
3. **Coding æ™‚**: éµå¾ªã€Œç¬¬ä¸‰éƒ¨åˆ†ã€çš„ç·¨ç¢¼è¦ç¯„
4. **æ¸¬è©¦å‰**: æŸ¥é–±ã€Œç¬¬å››éƒ¨åˆ†ã€çš„è³ªé‡ä¿è­‰æ¨™æº–
5. **ç–‘å•æ™‚**: æŸ¥æ‰¾ã€Œç¬¬äº”éƒ¨åˆ†ã€çš„ç›¸é—œåƒè€ƒæ–‡æª”

**ä»»å‹™è¿½è¹¤ä½¿ç”¨**:
- é…åˆ [SPRINT-5-4-CHECKLIST.md](./SPRINT-5-4-CHECKLIST.md) è¿½è¹¤é€²åº¦
- é…åˆ [SPRINT-5-5-DEV-LOG.md](./SPRINT-5-5-DEV-LOG.md) è¨˜éŒ„é–‹ç™¼æ—¥èªŒ
- é…åˆ [SPRINT-5-6-ISSUES.md](./SPRINT-5-6-ISSUES.md) è¿½è¹¤å•é¡Œ

**Code Review ä½¿ç”¨**:
- ä½¿ç”¨ã€Œç¬¬å››éƒ¨åˆ†ã€çš„æª¢æŸ¥æ¸…å–®é€²è¡Œå¯©æŸ¥
- ç¢ºä¿ç¬¦åˆã€Œç¬¬ä¸‰éƒ¨åˆ†ã€çš„ç·¨ç¢¼è¦ç¯„
- é©—è­‰ã€Œç¬¬ä¸€éƒ¨åˆ†ã€çš„é©—æ”¶æ¨™æº–

---

## ç‰ˆæœ¬æ­·å²

**v2.1** (2025-11-13)
- åˆå§‹ç‰ˆæœ¬å»ºç«‹
- å®Œæ•´ US 4.1 (æ–‡æª”ä¸Šå‚³èˆ‡è™•ç†) æŠ€è¡“è¦æ ¼
- å®Œæ•´ US 4.2 (RAG æª¢ç´¢ç­–ç•¥) æŠ€è¡“è¦æ ¼
- é–‹ç™¼é€±è¨ˆåŠƒï¼ˆWeek 1-3ï¼‰
- ç·¨ç¢¼è¦ç¯„èˆ‡è³ªé‡ä¿è­‰æ¨™æº–
- å®Œæ•´åƒè€ƒæ–‡æª”ç´¢å¼•

---

**æ–‡ä»¶ç¶­è­·**:
- ğŸ“… ä¸‹æ¬¡å¯©æŸ¥æ—¥æœŸ: Sprint 5 é–‹å§‹å‰ (2026-01-06)
- ğŸ‘¤ è² è²¬äºº: Backend Tech Lead
- ğŸ”„ æ›´æ–°é »ç‡: Sprint é€²è¡Œä¸­æ¯é€±æ›´æ–°å¯¦ä½œç´°ç¯€

**ç›¸é—œæ–‡æª”**:
- â¬†ï¸ ä¸Šä¸€å±¤: [Sprint 5 Overview](./SPRINT-5-1-OVERVIEW.md)
- â¡ï¸ ä¸‹ä¸€æ­¥: [Sprint 5 Context](./SPRINT-5-3-CONTEXT.md)
- ğŸ“‹ ä»»å‹™æ¸…å–®: [Sprint 5 Checklist](./SPRINT-5-4-CHECKLIST.md)
